# Shurli

Access your home server from anywhere. Share services with friends. No cloud, no account, no SaaS dependency.

**Shurli** connects your devices through firewalls and CGNAT using encrypted P2P tunnels with SSH-style authentication. One binary, zero configuration servers, works behind any NAT.

## News

| Date | What's New |
|------|-----------|
| 2026-02-18 | **Private DHT** - Peer discovery now runs on `/shurli/kad/1.0.0`, fully isolated from the public IPFS network |
| 2026-02-23 | **Relay pairing** - `shurli relay pair` generates pairing codes, `shurli join` accepts them. SAS verification, reachability grades (A-F) |
| 2026-02-17 | **Daemon mode** - Background service with Unix socket API, cookie auth, and 15 REST endpoints |
| 2026-02-17 | **Network tools** - P2P ping, traceroute, and name resolution (standalone or via daemon) |
| 2026-02-16 | **Service management** - `shurli service add/remove/enable/disable` from the CLI |
| 2026-02-16 | **Config self-healing** - Archive, rollback, and commit-confirmed pattern for safe remote changes |
| 2026-02-16 | **AutoNAT v2** - Per-address reachability detection with nonce verification |
| 2026-02-16 | **Headless pairing** - `--non-interactive` flag for scripted invite/join workflows |
| 2026-02-15 | **Structured logging** - `log/slog` throughout, sentinel errors, build version embedding |

## What Can I Do With Shurli?

| Use Case | Command |
|----------|---------|
| SSH to your home machine behind CGNAT | `shurli proxy home ssh 2222` → `ssh -p 2222 localhost` |
| Remote desktop through NAT | `shurli proxy home xrdp 13389` → connect to `localhost:13389` |
| Share Jellyfin with a friend | `shurli invite` on your side, `shurli join <code>` on theirs |
| AI inference on a friend's GPU | `shurli proxy friend ollama 11434` → `curl localhost:11434` |
| Any TCP service, zero port forwarding | `shurli proxy <peer> <service> <local-port>` |
| Check connectivity | `shurli ping home` or `shurli traceroute home` |

Shurli works with **two machines and zero network effect** - useful from day one.

## Quick Start

### Path A: Joining someone's network

If someone shared an invite code with you:

```bash
# Install (or build from source: go build -o shurli ./cmd/shurli)
shurli join <invite-code> --name laptop
```

That's it. You're connected and mutually authorized.

### Path B: Setting up your own network

**1. Set up both machines:**
```bash
go build -o shurli ./cmd/shurli
shurli init
```

**2. Pair them (on the first machine):**
```bash
shurli invite --name home
# Shows invite code + QR code, waits for the other side...
```

**3. Join (on the second machine):**
```bash
shurli join <invite-code> --name laptop
```

**4. Use it:**
```bash
# On the server - start the daemon with services exposed
shurli daemon

# On the client - connect to a service
shurli proxy home ssh 2222
ssh -p 2222 user@localhost
```

### Path C: Relay-managed group setup

If a relay admin shared a pairing code:

```bash
shurli join <pairing-code> --name laptop
# Connects to relay, discovers other peers, auto-authorizes everyone
# Shows SAS verification fingerprints for each peer
```

The relay admin generates codes with `shurli relay pair --count 3` (for 3 peers). Each person joins with one command. Everyone in the group is mutually authorized and verified.

> **Relay server**: All machines connect through a relay for NAT traversal. See [relay-server/README.md](relay-server/README.md) for deploying your own. Run `shurli relay serve` to start a relay.

## Why Shurli exists

Shurli was created to solve one problem: reaching a service on a home server from outside the network without depending on anyone else's infrastructure.

Existing solutions require either a cloud account, a third-party VPN, or port forwarding - which CGNAT frequently makes impossible. They all share the same flaw: your connectivity depends on someone else's servers and their permission to keep it running.

Shurli uses a different model. Devices connect outbound to a lightweight relay for initial setup, then upgrade to direct peer-to-peer when possible. No accounts, no central identity server, no revocable subscriptions. Your keys stay on your machine, configuration lives in one YAML file, and you can run your own relay for zero external dependency.

## The Problem

Your devices are behind firewalls and NAT that block inbound connections. This affects:

- **Satellite ISPs** with Carrier-Grade NAT (CGNAT)
- **Mobile networks** (4G/5G), almost universally behind CGNAT
- **Many broadband providers** worldwide applying CGNAT to conserve IPv4 addresses
- **University and corporate networks** with strict firewalls
- **Double-NAT setups** - router behind router

Traditional solutions require either port forwarding (impossible with CGNAT), a VPN service (another dependency), or a cloud intermediary (defeats self-hosting). Shurli solves this with a lightweight relay that both sides connect to **outbound**, then upgrades to a direct connection when possible.

## Features

| Feature | Description |
|---------|-------------|
| **NAT Traversal** | Circuit relay v2 + DCUtR hole-punching. Works behind CGNAT, symmetric NAT, double-NAT |
| **SSH-Style Auth** | `authorized_keys` peer allowlist - only explicitly trusted peers can connect |
| **60-Second Pairing** | `shurli invite` + `shurli join` - exchanges keys, adds auth, maps names automatically |
| **TCP Service Proxy** | Forward any TCP port through P2P tunnels (SSH, XRDP, HTTP, databases, AI inference) |
| **Daemon Mode** | Background service with Unix socket API, cookie auth, hot-reload of auth keys |
| **Config Self-Healing** | Last-known-good archive, rollback, and commit-confirmed pattern for safe remote changes |
| **Private DHT** | Kademlia peer discovery on `/shurli/kad/1.0.0` - isolated from public networks |
| **Friendly Names** | Map names to peer IDs in config - `home`, `laptop`, `gpu-server` instead of raw peer IDs |
| **Reusable Library** | `pkg/p2pnet` - import into your own Go projects for P2P networking |
| **Single Binary** | One `shurli` binary with 16 subcommands. No runtime dependencies |
| **Cross-Platform** | Go cross-compiles to Linux, macOS, Windows, ARM, and more |
| **systemd + launchd** | Service files included for both Linux and macOS |

## How It Works

```
┌──────────┐         ┌──────────────┐         ┌──────────────┐
│  Client   │───────▶│ Relay Server │◀────────│    Server    │
│  (Phone)  │ outbound    (VPS)   outbound   │  (Linux/Mac) │
└──────────┘         └──────────────┘         └──────────────┘
                           │
                     Both connect OUTBOUND
                     Relay bridges the connection
                     DCUtR upgrades to direct P2P
```

1. **Server** runs `shurli daemon` behind CGNAT, connects outbound to a relay and reserves a slot
2. **Client** runs `shurli proxy`, connects outbound to the same relay and reaches the server through a circuit address
3. **DCUtR** (Direct Connection Upgrade through Relay) attempts hole-punching. If successful, traffic flows directly without the relay

Peer discovery uses a **private Kademlia DHT** - the relay server acts as bootstrap peer. Authentication is enforced at both the connection level (ConnectionGater) and the protocol level.

For the full architecture: [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)

## Commands

### Daemon

| Command | Description |
|---------|-------------|
| `shurli daemon` | Start the daemon (P2P host + Unix socket control API) |
| `shurli daemon status [--json]` | Query running daemon status |
| `shurli daemon stop` | Graceful shutdown |
| `shurli daemon ping <target> [-c N] [--json]` | Ping a peer via daemon |
| `shurli daemon services [--json]` | List exposed services via daemon |
| `shurli daemon peers [--all] [--json]` | List connected peers (shurli-only by default) |
| `shurli daemon connect --peer <p> --service <s> --listen <addr>` | Create a TCP proxy via daemon |
| `shurli daemon disconnect <id>` | Tear down a proxy |

### Network Tools (standalone, no daemon required)

| Command | Description |
|---------|-------------|
| `shurli ping <target> [-c N] [--interval 1s] [--json]` | P2P ping with stats |
| `shurli traceroute <target> [--json]` | P2P traceroute through relay hops |
| `shurli resolve <name> [--json]` | Resolve a name to peer ID and addresses |
| `shurli proxy <target> <service> <local-port>` | Forward a local TCP port to a remote service |

### Identity & Access

| Command | Description |
|---------|-------------|
| `shurli whoami` | Show your peer ID |
| `shurli auth add <peer-id> [--comment "..."]` | Authorize a peer |
| `shurli auth list` | List authorized peers |
| `shurli auth remove <peer-id>` | Revoke a peer |
| `shurli auth validate` | Validate authorized_keys format |

### Configuration & Setup

| Command | Description |
|---------|-------------|
| `shurli init` | Interactive setup wizard (config, keys, authorized_keys) |
| `shurli config validate` | Validate config file |
| `shurli config show` | Show resolved configuration |
| `shurli config rollback` | Restore last-known-good config |
| `shurli config apply <file> [--confirm-timeout 5m]` | Apply config with auto-revert safety net |
| `shurli config confirm` | Confirm applied config (cancels auto-revert) |
| `shurli relay add/list/remove` | Manage relay server addresses |
| `shurli service add/remove/enable/disable/list` | Manage exposed services |

### Pairing

| Command | Description |
|---------|-------------|
| `shurli invite [--name "home"] [--non-interactive]` | Generate invite code + QR, wait for join |
| `shurli join <code> [--name "laptop"] [--non-interactive]` | Accept invite or relay pairing code, auto-configure |
| `shurli relay pair [--count N] [--ttl 1h]` | Generate relay pairing codes (relay admin only) |
| `shurli verify <peer>` | Verify peer identity via SAS fingerprint (4-emoji + numeric) |
| `shurli status` | Show local config, identity, authorized peers (verified/unverified), services, names |
| `shurli version` | Show version, commit, build date, Go version |

The `<target>` in network commands accepts either a peer ID or a name from the `names:` section of your config. All commands support `--config <path>`.

## Daemon Mode

The daemon runs `shurli daemon` as a long-lived background process. It starts the full P2P host, exposes configured services, and opens a Unix socket API for management.

**Key features:**
- Unix socket at `~/.config/shurli/shurli.sock` (no TCP exposure)
- Cookie-based auth (`~/.config/shurli/.daemon-cookie`) - 32-byte random token, rotated per restart
- Hot-reload of authorized_keys via `daemon` auth endpoints
- 15 REST endpoints for status, peers, services, auth, proxies, ping, traceroute, resolve, paths

**Example:**
```bash
# Start the daemon
shurli daemon

# In another terminal - query status
shurli daemon status

# Create a proxy through the daemon
shurli daemon connect --peer home --service ssh --listen localhost:2222
```

For the full API reference: [docs/DAEMON-API.md](docs/DAEMON-API.md)

## Configuration

### Config Search Order

1. `--config <path>` flag (explicit)
2. `./shurli.yaml` (current directory)
3. `~/.config/shurli/config.yaml` (standard location, created by `shurli init`)
4. `/etc/shurli/config.yaml` (system-wide)

### Essential Config

```yaml
identity:
  key_file: "identity.key"

network:
  listen_addresses:
    - "/ip4/0.0.0.0/tcp/0"
    - "/ip4/0.0.0.0/udp/0/quic-v1"
  force_private_reachability: false  # true for servers behind CGNAT

relay:
  addresses:
    - "/ip4/YOUR_VPS_IP/tcp/7777/p2p/YOUR_RELAY_PEER_ID"

security:
  authorized_keys_file: "authorized_keys"
  enable_connection_gating: true

# services:       # Uncomment to expose services (server only)
#   ssh:
#     enabled: true
#     local_address: "localhost:22"

names: {}         # Map friendly names to peer IDs
#  home: "12D3KooW..."
```

Full sample configs: [configs/](configs/)

## Running as a Service

### Linux (systemd)

A service file is provided at [deploy/shurli-daemon.service](deploy/shurli-daemon.service):

```bash
sudo cp deploy/shurli-daemon.service /etc/systemd/system/shurli.service
# Edit ExecStart path and --config as needed
sudo systemctl daemon-reload
sudo systemctl enable --now shurli
```

Both `shurli daemon` and `shurli relay serve` send `sd_notify` signals (`READY=1`, `WATCHDOG=1`, `STOPPING=1`).

### macOS (launchd)

A plist is provided at [deploy/com.shurli.daemon.plist](deploy/com.shurli.daemon.plist).

### Relay Server

See [relay-server/README.md](relay-server/README.md) for the full VPS deployment guide (user creation, SSH hardening, firewall, systemd, health checks).

## Building

A Makefile is provided for common operations:

```bash
make build            # Build with version embedding and optimizations
make test             # Run all tests with race detection
make clean            # Remove build artifacts
make install          # Build, install to /usr/local/bin, and set up system service
make install-service  # Install and enable systemd (Linux) or launchd (macOS) service
make restart-service  # Restart the service after a rebuild
make uninstall        # Remove service and binary
make website          # Start Hugo development server for shurli.io
make help             # Show all available targets
```

**Local checks**: `make check` runs commands from a `.checks` file (gitignored, one command per line). `make push` runs checks before pushing. Create your own `.checks` with any validation commands you need:

```bash
# Example .checks file
echo "Running lint..."
go vet ./...
```

You can also build directly with Go:

```bash
# Build shurli
go build -o shurli ./cmd/shurli

# Build with version info
go build -ldflags "-X main.version=0.1.0 \
  -X main.commit=$(git rev-parse --short HEAD) \
  -X main.buildDate=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
  -o shurli ./cmd/shurli

# Cross-compile for Linux
GOOS=linux GOARCH=amd64 go build -o shurli ./cmd/shurli

# Run tests
go test -race -count=1 ./...
```

## Library (`pkg/p2pnet`)

The `pkg/p2pnet` package is an importable Go library for building P2P applications:

```go
import "github.com/shurlinet/shurli/pkg/p2pnet"

// Create a P2P network
net, _ := p2pnet.New(&p2pnet.Config{
    KeyFile:     "myapp.key",
    EnableRelay: true,
    RelayAddrs:  []string{"/ip4/.../tcp/7777/p2p/..."},
})

// Expose a local service
net.ExposeService("api", "localhost:8080", nil)

// Connect to a peer's service
conn, _ := net.ConnectToService(peerID, "api")

// Name resolution
net.LoadNames(map[string]string{"home": "12D3KooW..."})
peerID, _ := net.ResolveName("home")
```

## Project Structure

```
cmd/
├── shurli/                    # Single binary with subcommands
│   ├── main.go                # Command dispatch (16 subcommands)
│   ├── cmd_daemon.go          # Daemon mode (start, stop, status, ping, peers, ...)
│   ├── cmd_proxy.go           # TCP proxy client
│   ├── cmd_ping.go            # Standalone P2P ping
│   ├── cmd_traceroute.go      # P2P traceroute
│   ├── cmd_resolve.go         # Name resolution
│   ├── cmd_init.go            # Interactive setup wizard
│   ├── cmd_invite.go          # Generate invite code + QR + P2P handshake
│   ├── cmd_join.go            # Accept invite, auto-configure
│   ├── cmd_auth.go            # Auth add/list/remove/validate
│   ├── cmd_relay.go           # Relay add/list/remove (client config)
│   ├── cmd_relay_serve.go     # Relay server: serve/authorize/info/config
│   ├── cmd_config.go          # Config validate/show/rollback/apply/confirm
│   ├── cmd_service.go         # Service add/remove/enable/disable/list
│   ├── cmd_status.go          # Local status display
│   ├── cmd_whoami.go          # Show peer ID
│   ├── serve_common.go        # Shared P2P runtime (used by daemon + standalone tools)
│   ├── config_template.go     # Config YAML template
│   ├── flag_helpers.go        # CLI flag reordering for natural usage
│   └── relay_input.go         # Flexible relay address parsing
pkg/p2pnet/                    # Importable P2P networking library
├── network.go                 # Core: host setup, relay, DHT, name resolution
├── service.go                 # Service registry
├── proxy.go                   # Bidirectional TCP↔Stream proxy with half-close
├── naming.go                  # Local name resolution (name → peer ID)
├── identity.go                # Identity helpers
├── ping.go                    # PingPeer() with streaming results
├── traceroute.go              # P2P traceroute
└── errors.go                  # Sentinel errors
internal/
├── config/                    # YAML configuration + self-healing
│   ├── config.go              # Config structs
│   ├── loader.go              # Auto-discovery, path resolution, validation
│   ├── archive.go             # Last-known-good archive/rollback
│   ├── confirm.go             # Commit-confirmed pattern
│   └── errors.go
├── auth/                      # Connection gating + authorized_keys
│   ├── gater.go               # ConnectionGater (blocks unauthorized at network level)
│   ├── authorized_keys.go     # File parser
│   ├── manage.go              # AddPeer/RemovePeer/ListPeers
│   └── errors.go
├── daemon/                    # Daemon API server + client library
│   ├── server.go              # Unix socket HTTP server with cookie auth
│   ├── handlers.go            # 15 REST endpoint handlers
│   ├── client.go              # Go client (auto-reads cookie, Unix transport)
│   ├── types.go               # Request/response types
│   └── errors.go
├── identity/                  # Ed25519 identity management
├── invite/                    # Invite code encoding (binary → base32 + dash groups)
├── validate/                  # Input validation (service names, DNS-label format)
├── watchdog/                  # Health monitoring + systemd sd_notify (pure Go)
├── qr/                        # QR code generation (zero dependencies)
└── termcolor/                 # Terminal color output
relay-server/                  # Deployment artifacts
├── setup.sh                   # Full VPS setup (build, permissions, systemd, health)
├── relay-server.service       # systemd unit file
└── README.md                  # VPS deployment guide
deploy/                        # Client service files
├── shurli-daemon.service      # systemd unit for shurli daemon
└── com.shurli.daemon.plist    # launchd plist for macOS
configs/                       # Sample configuration files
├── shurli.sample.yaml
├── relay-server.sample.yaml
└── authorized_keys.sample
docs/                          # Documentation
├── ARCHITECTURE.md            # Full architecture deep dive
├── DAEMON-API.md              # Daemon REST API reference
├── FAQ.md                     # Frequently asked questions
├── NETWORK-TOOLS.md           # Ping, traceroute, resolve guide
├── ROADMAP.md                 # Multi-phase implementation plan
├── TESTING.md                 # Test strategy and coverage
└── ENGINEERING-JOURNAL.md     # Architecture decision records (ADRs)
```

## Security

**Two layers of defense:**

1. **ConnectionGater** (network level) - Blocks unauthorized peers during the connection handshake, before any data is exchanged
2. **Protocol handler** (application level) - Secondary authorization check before processing requests

**Fail-safe defaults:**
- Connection gating enabled + no authorized_keys file → **refuses to start**
- Empty authorized_keys → **warns loudly** (allows for initial setup)
- All outbound connections allowed (required for DHT and relay)
- All unauthorized inbound connections blocked

**File permissions:**
```
chmod 600 *.key              # Private keys: owner read/write only
chmod 600 authorized_keys    # Peer allowlist: owner read/write only
chmod 644 *.yaml             # Configs: readable
```

For security details, relay hardening, and threat model: [docs/FAQ.md](docs/FAQ.md)

## Troubleshooting

| Issue | Solution |
|-------|----------|
| `no config file found` | Run `shurli init` or use `--config <path>` |
| `Cannot resolve target` | Add name mapping to `names:` in config |
| `DENIED inbound connection` | Add peer ID to `authorized_keys`, restart daemon |
| `Invalid invite code` | Paste the full code as one argument (quote if spaces) |
| `Failed to connect to inviter` | Ensure `shurli invite` is still running |
| No `/p2p-circuit` addresses | Check `force_private_reachability: true` and relay address |
| `protocols not supported` | Relay server not running or unreachable |
| Bad config edit broke startup | `shurli config rollback` restores last-known-good |
| Remote config change went wrong | `shurli config apply new.yaml --confirm-timeout 5m`, then `config confirm` |
| `failed to sufficiently increase receive buffer size` | QUIC works but suboptimal - see UDP buffer tuning below |
| Daemon won't start (socket exists) | Stale socket from crash - daemon auto-detects and cleans up |

### UDP Buffer Tuning (QUIC)

QUIC works with default buffers but performs better with increased limits:

```bash
# Linux (persistent)
echo "net.core.rmem_max=7500000" | sudo tee -a /etc/sysctl.d/99-quic.conf
echo "net.core.wmem_max=7500000" | sudo tee -a /etc/sysctl.d/99-quic.conf
sudo sysctl --system
```

## Engineering Philosophy

This is not a weekend hobby project. Shurli is built as critical infrastructure, the kind where failure has real consequences for real people: financial, psychological, and potentially physical.

Think of it like a bubble in outer space. If it breaks, the people inside don't get a second chance. That standard guides everything here - from code quality to deployment to security decisions.

## Disclaimer

Shurli is experimental software under active development. It is built with significant AI assistance (Claude) and, despite thorough testing, **will contain bugs** that neither automated tests nor manual testing have caught.

**By using this software, you acknowledge:**

- This is provided "as is" with no warranty of any kind (see [LICENSE](LICENSE))
- The developers are not liable for any damages, losses, or consequences arising from its use
- Network tunnels may disconnect, services may become unreachable, and configurations may behave unexpectedly
- This is not a replacement for enterprise VPN, firewall, or security infrastructure
- You are responsible for evaluating whether Shurli is suitable for your use case

If you discover a bug, please [open an issue](https://github.com/shurlinet/shurli/issues). Every report makes the project more reliable for everyone.

## Development

### AI-Assisted Development

Shurli is developed with significant AI assistance (Claude). All AI-generated code is reviewed, tested, and committed by a human maintainer. The architecture, vision, and engineering decisions are human-directed.

### No Cryptocurrency / No Token

Shurli is a networking tool. It has no token, no coin, no blockchain dependency, and no plans to add one. If someone tells you otherwise, they're not affiliated with this project.

### Contributing

Issues and PRs are welcome.

**Testing checklist:**
- [ ] `go build ./...` succeeds
- [ ] `go vet ./...` passes
- [ ] `go test -race -count=1 ./...` passes
- [ ] Unauthorized peer is denied, authorized peer connects
- [ ] Service proxy works end-to-end

## Documentation

| Document | Description |
|----------|-------------|
| [ARCHITECTURE.md](docs/ARCHITECTURE.md) | Full architecture: relay circuit, DHT, proxy, auth system |
| [DAEMON-API.md](docs/DAEMON-API.md) | Daemon REST API reference (15 endpoints) |
| [FAQ.md](docs/FAQ.md) | Security FAQ, relay hardening, troubleshooting |
| [NETWORK-TOOLS.md](docs/NETWORK-TOOLS.md) | Ping, traceroute, resolve usage guide |
| [ROADMAP.md](docs/ROADMAP.md) | Multi-phase implementation plan |
| [TESTING.md](docs/TESTING.md) | Test strategy, coverage, integration tests |
| [ENGINEERING-JOURNAL.md](docs/ENGINEERING-JOURNAL.md) | Architecture decision records: why every design choice was made |

## Dependencies

- [go-libp2p](https://github.com/libp2p/go-libp2p) v0.47.0
- [go-libp2p-kad-dht](https://github.com/libp2p/go-libp2p-kad-dht) v0.28.1
- [go-multiaddr](https://github.com/multiformats/go-multiaddr)
- [gopkg.in/yaml.v3](https://gopkg.in/yaml.v3) v3.0.1

## License

MIT


---

# Network Diagnostic Tools

Shurli provides P2P network diagnostic commands that mirror familiar system utilities. These work both standalone (create a temporary P2P host) and through the daemon API (use the existing host for faster operation).

## Table of Contents

- [ping](#ping)
- [traceroute](#traceroute)
- [resolve](#resolve)
- [Standalone vs Daemon](#standalone-vs-daemon)

---

## ping

P2P `ping` - measures round-trip time and connection path to a peer.

### Usage

```bash
# Standalone (creates temp host, pings, exits)
shurli ping <peer> [-c N] [--interval 1s] [--json] [--config path]

# Via daemon (reuses existing host - faster)
shurli daemon ping <peer> [-c N] [--interval 1s] [--json]
```

### Behavior

| Mode | Behavior |
|------|----------|
| `-c N` (N > 0) | Send N pings, print per-ping line, summary at end |
| No `-c` flag | Continuous pinging until Ctrl+C, summary on exit |
| `--interval 1s` | Time between pings (default `1s`) |
| `--json` | JSON output (one line per ping result + final stats) |

### Output

**Plain text (default)**:

```
PING home-server (12D3KooWPrmh...):
seq=1 rtt=45.2ms path=[RELAYED]
seq=2 rtt=42.1ms path=[DIRECT]
seq=3 rtt=43.0ms path=[DIRECT]
^C
--- home-server ping statistics ---
3 sent, 3 received, 0% loss, rtt min/avg/max = 42.1/43.4/45.2 ms
```

**JSON (`--json`)**:

```json
{"seq":1,"peer_id":"12D3KooWPrmh...","rtt_ms":45.2,"path":"RELAYED"}
{"seq":2,"peer_id":"12D3KooWPrmh...","rtt_ms":42.1,"path":"DIRECT"}
{"seq":3,"peer_id":"12D3KooWPrmh...","rtt_ms":43.0,"path":"DIRECT"}
{"sent":3,"received":3,"lost":0,"loss_pct":0.0,"min_ms":42.1,"avg_ms":43.4,"max_ms":45.2}
```

### Connection Path

Each ping shows whether the connection went through the relay or directly:

| Path | Meaning |
|------|---------|
| `[DIRECT]` | Peer-to-peer (hole-punched or same LAN) |
| `[RELAYED]` | Via circuit relay server |

Direct connections have lower latency. If pings start as `RELAYED` and switch to `DIRECT`, DCUtR hole-punching succeeded.

### Ping Allow/Deny

Peers can disable ping responses in their config:

```yaml
protocols:
  ping:
    enabled: false   # silently drop ping requests
```

When disabled, the peer doesn't register the ping-pong stream handler. Pings will time out - same behavior as a firewall dropping ICMP.

### How It Works

1. Open a libp2p stream to the target peer using the ping-pong protocol (`/shurli/ping/1.0.0`)
2. Send `"ping\n"` on the stream
3. Wait for `"pong\n"` response
4. Measure round-trip time
5. Check connection type (direct vs relayed) from stream metadata

---

## traceroute

P2P `traceroute` - shows the network path to a peer with per-hop latency.

### Usage

```bash
# Standalone
shurli traceroute <peer> [--json] [--config path]

# Via daemon API
curl -X POST -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     -d '{"peer":"home-server"}' \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/traceroute
```

### Path Visualization

![Traceroute paths: relayed (2 hops via relay with latency) vs direct (1 hop, lower latency)](images/tools-traceroute-paths.svg)

### Output

**Relayed connection**:

```
traceroute to home-server (12D3KooWPrmh...), max 3 hops:
 1  12D3KooWK... (relay)  203.0.113.50:7777  23.0ms
 2  12D3KooWPrmh... (home-server)  via relay  45.0ms
--- path: [RELAYED via relay-server/0.1.0] ---
```

**Direct connection**:

```
traceroute to home-server (12D3KooWPrmh...), max 3 hops:
 1  12D3KooWPrmh... (home-server)  10.0.1.50:9000  2.0ms
--- path: [DIRECT] ---
```

### What It Shows

- Whether the connection is direct or through a relay
- Latency to each hop (relay, then peer)
- Relay server's software version (from peerstore AgentVersion)
- Peer addresses

### Implementation

This is not true multi-hop tracing (libp2p doesn't support TTL). Instead, it inspects connection metadata:

1. Check if the connection to the peer goes through a relay (multiaddr contains `/p2p-circuit`)
2. If relayed: measure RTT to relay separately, then RTT to peer through relay
3. If direct: single hop with measured RTT
4. Report the agent version of intermediate nodes from the peerstore

This gives the real diagnostic information needed: **where is my traffic going, and how fast is each segment**.

---

## resolve

P2P `nslookup` - resolves peer names to peer IDs.

### Usage

```bash
# Standalone (no network needed)
shurli resolve <name> [--json] [--config path]

# Via daemon API
curl -X POST -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     -d '{"name":"home-server"}' \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/resolve
```

### Output

**Plain text**:

```
home-server → 12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt (source: local_config)
```

**JSON (`--json`)**:

```json
{
  "name": "home-server",
  "peer_id": "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt",
  "source": "local_config"
}
```

### Resolution Sources

| Source | Meaning |
|--------|---------|
| `local_config` | Resolved from `names:` section in `config.yaml` |
| `peer_id` | Input was already a valid peer ID (direct parse) |

### No Network Required

`resolve` reads from local config only - it doesn't contact the network or start a P2P host. Resolution is instant.

Names are configured in `config.yaml`:

```yaml
names:
  home-server: "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt"
  laptop: "12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6"
```

---

## Standalone vs Daemon

All network tools work in two modes:

| Mode | Command | Speed | When to Use |
|------|---------|-------|-------------|
| Standalone | `shurli ping home-server` | Slower (creates temp host, bootstraps DHT) | One-off diagnostics, daemon not running |
| Daemon | `shurli daemon ping home-server` | Faster (reuses existing host + connections) | Repeated diagnostics, scripting |

### Standalone Startup

When running standalone, the tool:

1. Loads config to find relay addresses, bootstrap peers, and names
2. Creates a temporary libp2p host
3. Bootstraps into the DHT (client mode)
4. Connects to relay servers
5. Finds the target peer via DHT or relay fallback
6. Runs the diagnostic
7. Shuts down the host

This takes 5-15 seconds for initial setup. The daemon mode skips all of this.

### Shared Logic

Both modes use the same underlying functions:
- `p2pnet.PingPeer()` - streaming ping with configurable count and interval
- `p2pnet.ComputePingStats()` - min/avg/max/loss statistics
- `p2pnet.TracePeer()` - connection path analysis

---

## System Utility Comparison

| System Tool | Shurli Equivalent | What It Does |
|-------------|-------------------|--------------|
| `ping` | `shurli ping` | Measure RTT to a peer |
| `traceroute` | `shurli traceroute` | Show path to a peer (direct vs relay) |
| `nslookup` / `dig` | `shurli resolve` | Resolve name to peer ID |
| `ss` / `netstat` | `shurli daemon peers` | Show connected peers |
| `systemctl status` | `shurli daemon status` | Show daemon status |

---

**Last Updated**: 2026-02-22


---

# Monitoring with Prometheus and Grafana

This guide walks you through setting up Prometheus and Grafana to visualize Shurli metrics. By the end, you'll have a live dashboard showing proxy throughput, auth decisions, hole punch stats, API latency, and system health.

![Monitoring stack - Shurli nodes scraped by Prometheus, visualized in Grafana](images/monitoring-stack.svg)

## Prerequisites

- Shurli running with telemetry enabled
- Docker (recommended) or local Prometheus + Grafana installs

## Step 1: Enable telemetry in Shurli

Add this to your `shurli.yaml` (or relay config):

```yaml
telemetry:
  metrics:
    enabled: true
    listen_address: "127.0.0.1:9091"
  audit:
    enabled: true
```

Restart the daemon. Verify metrics are being served:

```bash
curl http://127.0.0.1:9091/metrics
```

You should see output starting with `# HELP` and `# TYPE` lines, followed by metric values. Both custom `shurli_*` metrics and libp2p built-in metrics (`libp2p_*`) will appear.

> **Relay server**: When both `health.enabled` and `telemetry.metrics.enabled` are set, the relay adds `/metrics` to its existing `/healthz` HTTP mux. No extra port needed.

## Step 2: Set up Prometheus

### Option A: Docker (recommended)

Create a `prometheus.yml` config file:

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'shurli'
    static_configs:
      - targets: ['host.docker.internal:9091']
        labels:
          instance: 'home-node'
```

> Replace `host.docker.internal` with the actual IP if not running on the same machine. Use `172.17.0.1` on Linux.

Run Prometheus:

```bash
docker run -d \
  --name prometheus \
  -p 9090:9090 \
  -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus
```

Verify at `http://localhost:9090/targets` - the shurli target should show as **UP**.

### Option B: Local install

Download from [prometheus.io/download](https://prometheus.io/download/), add the same `prometheus.yml` config, and run `./prometheus --config.file=prometheus.yml`.

### Scraping multiple nodes

Add each Shurli instance as a separate target:

```yaml
scrape_configs:
  - job_name: 'shurli'
    static_configs:
      - targets: ['10.0.1.50:9091']
        labels:
          instance: 'home-node'
      - targets: ['10.0.1.51:9091']
        labels:
          instance: 'client-node'
  - job_name: 'shurli-relay'
    static_configs:
      - targets: ['relay.example.com:9091']
        labels:
          instance: 'relay'
```

## Step 3: Set up Grafana

### Docker

```bash
docker run -d \
  --name grafana \
  -p 3000:3000 \
  grafana/grafana
```

Open `http://localhost:3000` (default login: `admin` / `admin`).

### Add Prometheus as a data source

1. Go to **Connections > Data sources > Add data source**
2. Select **Prometheus**
3. Set URL to `http://host.docker.internal:9090` (or `http://172.17.0.1:9090` on Linux)
4. Click **Save & test**

## Step 4: Import the Shurli dashboard

1. Go to **Dashboards > Import**
2. Click **Upload dashboard JSON file**
3. Select `grafana/shurli-dashboard.json` from the Shurli repository
4. Select your Prometheus data source when prompted
5. Click **Import**

You'll immediately see the dashboard with 23 visualization panels across 6 sections.

## Dashboard sections

![Observability data flow - from metric sources through Prometheus registry to /metrics endpoint](images/observability-flow.svg)

### Overview

Six stat panels showing the essentials at a glance:

- **Version** - which Shurli version is running
- **Uptime** - how long the daemon has been running
- **Active Proxy Connections** - current session count
- **Total Connections** - lifetime connection count
- **Total Bytes Transferred** - all proxy traffic
- **Auth Allow / Deny** - cumulative auth decisions

### Proxy Throughput

Track how your services are performing:

- **Throughput (bytes/sec)** - ingress and egress per service (ssh, xrdp, ollama, etc.)
- **Active Connections** - live gauge per service
- **Connection Rate** - new connections per second
- **Connection Duration** - p50, p95, p99 percentiles per service

### Security

Monitor access control:

- **Auth Decisions Rate** - allow/deny per second (deny spikes = something probing your node)
- **Auth Decisions (Cumulative)** - total allow vs deny count

### Hole Punch (NAT Traversal)

Understand your NAT traversal success:

- **Hole Punch Attempts** - success vs failure count over time
- **Success Rate** - gauge showing what percentage of hole punches succeed (green > 70%, orange > 50%, red < 50%)
- **Hole Punch Duration** - p50 and p95 timing (tells you how long direct connection setup takes)

### Daemon API

API health and latency:

- **Request Rate** - requests per second by endpoint path
- **Request Latency** - p50 and p95 response times by path
- **Requests by Status Code** - 200/404/500 breakdown (4xx in orange, 5xx in red)

### System

Go runtime health:

- **Memory Usage** - heap in-use, system allocated, RSS
- **Goroutines** - should be stable (a leak shows as continuous growth)
- **GC Rate** - garbage collection cycles per second
- **Open File Descriptors** - current vs max (hitting max = connection failures)
- **CPU Usage** - process CPU utilization

## Available metrics reference

### Custom shurli metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `shurli_proxy_bytes_total` | Counter | direction, service | Bytes transferred through proxy |
| `shurli_proxy_connections_total` | Counter | service | Connections established |
| `shurli_proxy_active_connections` | Gauge | service | Currently active connections |
| `shurli_proxy_duration_seconds` | Histogram | service | Connection session duration |
| `shurli_auth_decisions_total` | Counter | decision | Auth allow/deny counts |
| `shurli_holepunch_total` | Counter | result | Hole punch success/failure |
| `shurli_holepunch_duration_seconds` | Histogram | result | Hole punch attempt duration |
| `shurli_daemon_requests_total` | Counter | method, path, status | API request counts |
| `shurli_daemon_request_duration_seconds` | Histogram | method, path, status | API request latency |
| `shurli_info` | Gauge | version, go_version | Build information |

### libp2p built-in metrics (free, no extra code)

When metrics are enabled, all libp2p instrumentation appears automatically:

- `libp2p_swarm_connections_*` - connections by transport and direction
- `libp2p_holepunch_*` - hole punch protocol events
- `libp2p_autorelay_*` - relay reservation status
- `libp2p_autonat_*` - NAT reachability detection
- `libp2p_autonatv2_*` - per-address reachability (v2)
- `libp2p_identify_*` - peer identification events
- `libp2p_rcmgr_*` - resource manager limits and usage
- `libp2p_relaysvc_*` - relay service stats (relay server only)

### Go runtime metrics

Standard Go collectors are registered automatically:

- `go_goroutines` - current goroutine count
- `go_memstats_alloc_bytes` - heap allocation
- `go_gc_duration_seconds` - GC pause times
- `process_cpu_seconds_total` - CPU usage
- `process_open_fds` / `process_max_fds` - file descriptors
- `process_resident_memory_bytes` - RSS

## Audit logging

When `telemetry.audit.enabled` is set, security events are logged as structured JSON to stderr:

```json
{"time":"2026-02-21T10:30:00Z","level":"WARN","msg":"auth_decision","audit":{"peer":"12D3KooW...","direction":"inbound","result":"deny"}}
{"time":"2026-02-21T10:30:01Z","level":"INFO","msg":"daemon_api_access","audit":{"method":"GET","path":"/v1/status","status":200}}
```

### Event types

| Event | Level | Fields | When |
|-------|-------|--------|------|
| `auth_decision` | INFO/WARN | peer, direction, result | Every inbound connection (WARN for deny) |
| `service_acl_deny` | WARN | peer, service | Peer authorized but blocked by per-service ACL |
| `daemon_api_access` | INFO | method, path, status | Every daemon API request |
| `auth_change` | INFO | action, peer | Peer added or removed via API |

### Sending audit logs to a log aggregator

Since audit events go to stderr, you can pipe them to any log collector:

**systemd journal** (default when running as a service):
```bash
journalctl -u shurli-daemon -o json | jq 'select(.MESSAGE | contains("audit"))'
```

**File output**:
```bash
shurli daemon 2> /var/log/shurli-audit.json
```

**Loki / Promtail**: Point Promtail at the journal or log file, filter on `audit` field presence.

## Docker Compose (all-in-one)

For a quick local stack with Prometheus + Grafana + Shurli metrics:

```yaml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    extra_hosts:
      - "host.docker.internal:host-gateway"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  grafana-data:
```

Start with `docker compose up -d`, then import the dashboard at `http://localhost:3000`.

## Useful PromQL queries

**Proxy throughput over last hour:**
```promql
sum(rate(shurli_proxy_bytes_total[1h])) by (service, direction)
```

**Hole punch success rate (last 24h):**
```promql
sum(shurli_holepunch_total{result="success"}) / sum(shurli_holepunch_total)
```

**Top API endpoints by latency (p95):**
```promql
histogram_quantile(0.95, sum by (le, path) (rate(shurli_daemon_request_duration_seconds_bucket[5m])))
```

**Auth deny rate (should be near zero for a healthy node):**
```promql
rate(shurli_auth_decisions_total{decision="deny"}[5m])
```

**Memory trend:**
```promql
process_resident_memory_bytes
```

## Alerting examples

If you use Grafana alerting or Alertmanager, here are useful alert rules:

```yaml
# Too many auth denials (possible scanning/attack)
- alert: HighAuthDenyRate
  expr: rate(shurli_auth_decisions_total{decision="deny"}[5m]) > 1
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "High auth denial rate on {{ $labels.instance }}"

# Hole punch success rate dropping
- alert: LowHolePunchSuccess
  expr: sum(shurli_holepunch_total{result="success"}) / sum(shurli_holepunch_total) < 0.5
  for: 15m
  labels:
    severity: warning
  annotations:
    summary: "Hole punch success rate below 50% on {{ $labels.instance }}"

# File descriptor exhaustion approaching
- alert: FileDescriptorHigh
  expr: process_open_fds / process_max_fds > 0.8
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "File descriptors at {{ $value | humanizePercentage }} on {{ $labels.instance }}"
```


---

# Daemon API Reference

The Shurli daemon (`shurli daemon`) runs a long-lived P2P host with a Unix domain socket HTTP API for programmatic control.

## Table of Contents

- [Architecture](#architecture)
- [Authentication](#authentication)
- [Response Format](#response-format)
- [Endpoints](#endpoints)
  - [GET /v1/status](#get-v1status)
  - [GET /v1/services](#get-v1services)
  - [GET /v1/peers](#get-v1peers)
  - [GET /v1/auth](#get-v1auth)
  - [GET /v1/paths](#get-v1paths)
  - [POST /v1/auth](#post-v1auth)
  - [DELETE /v1/auth/{peer_id}](#delete-v1authpeer_id)
  - [POST /v1/ping](#post-v1ping)
  - [POST /v1/traceroute](#post-v1traceroute)
  - [POST /v1/resolve](#post-v1resolve)
  - [POST /v1/connect](#post-v1connect)
  - [DELETE /v1/connect/{id}](#delete-v1connectid)
  - [POST /v1/expose](#post-v1expose)
  - [DELETE /v1/expose/{name}](#delete-v1exposename)
  - [POST /v1/shutdown](#post-v1shutdown)
- [Error Codes](#error-codes)
- [CLI Usage](#cli-usage)
- [Integration Examples](#integration-examples)
- [Socket Lifecycle](#socket-lifecycle)

---

## Architecture

The daemon runs the full P2P lifecycle (relay connection, DHT bootstrap, service exposure, watchdog) plus an HTTP server on a Unix socket.

![Daemon architecture: P2P Runtime (relay, DHT, services, watchdog) connected bidirectionally to Unix Socket API (HTTP/1.1, cookie auth, 15 endpoints), with P2P Network below left and CLI/Scripts below right](images/daemon-api-architecture.svg)

**Default paths**:
- Socket: `~/.config/shurli/shurli.sock` (permissions `0600`)
- Cookie: `~/.config/shurli/.daemon-cookie` (permissions `0600`)

---

## Authentication

The daemon uses cookie-based authentication (same pattern as Bitcoin Core, Docker, containerd).

### How It Works

1. On startup, the daemon generates a 32-byte random hex token
2. Token is written to `~/.config/shurli/.daemon-cookie` with `0600` permissions
3. Every API request must include `Authorization: Bearer <token>` header
4. Token is validated on every request - `401 Unauthorized` if missing or wrong
5. Cookie file is deleted on clean shutdown
6. Token rotates on every daemon restart (limits exposure window)

### Why Cookie Over Config-Based Password

- No plaintext passwords in config files
- Token rotates every daemon restart
- Same-user access only (cookie file is `0600`)
- Proven pattern used by Bitcoin Core, Docker, containerd

### Example

```bash
curl -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/status
```

The CLI client (`shurli daemon status`, etc.) reads the cookie file automatically - no manual auth needed.

> **Tip**: All curl examples in this document use inline `$(cat ~/.config/shurli/.daemon-cookie)` so they work as-is when copy-pasted. For scripts that make multiple API calls, read the token once into a variable - see [Integration Examples](#integration-examples).

### Unauthorized Response

```json
{
  "error": "unauthorized: invalid or missing auth token"
}
```

HTTP status: `401 Unauthorized`

---

## Response Format

Every endpoint supports two output formats:

### JSON (Default)

Success responses are wrapped in a `data` envelope:

```json
{"data": { ... }}
```

Error responses use an `error` envelope:

```json
{"error": "description of what went wrong"}
```

### Plain Text

Request plain text via:
- Query parameter: `?format=text`
- Accept header: `Accept: text/plain`

Plain text responses are single-line or tabular, designed for `grep`/`awk`/`cut`.

### CLI Format Selection

```bash
shurli daemon status          # human-readable text
shurli daemon status --json   # raw JSON
```

---

## Endpoints

### GET /v1/status

Returns daemon status: peer ID, version, uptime, connected peers, addresses, services count, network capabilities, and reachability grade.

**Response (JSON)**:

```json
{
  "data": {
    "peer_id": "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt",
    "version": "0.1.0",
    "uptime_seconds": 3600,
    "connected_peers": 2,
    "listen_addresses": [
      "/ip4/10.0.1.50/tcp/9000",
      "/ip4/10.0.1.50/udp/9000/quic-v1"
    ],
    "relay_addresses": [
      "/ip4/203.0.113.50/tcp/7777/p2p/12D3KooWK.../p2p-circuit"
    ],
    "services_count": 2,
    "has_global_ipv6": true,
    "has_global_ipv4": false,
    "nat_type": "port-restricted",
    "stun_external_addrs": ["203.0.113.50:12345"],
    "is_relaying": false,
    "reachability": {
      "grade": "A",
      "label": "Excellent",
      "description": "Public IPv6 detected"
    }
  }
}
```

**Response (Text)**:

```
peer_id: 12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt
version: 0.1.0
uptime: 3600s
connected_peers: 2
services: 2
listen_addresses: 2
  /ip4/10.0.1.50/tcp/9000
  /ip4/10.0.1.50/udp/9000/quic-v1
relay_addresses: 1
  /ip4/203.0.113.50/tcp/7777/p2p/12D3KooWK.../p2p-circuit
```

**curl**:

```bash
curl -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/status
```

---

### GET /v1/services

Lists all registered services.

**Response (JSON)**:

```json
{
  "data": [
    {
      "name": "ssh",
      "protocol": "/shurli/ssh/1.0.0",
      "local_address": "localhost:22",
      "enabled": true
    },
    {
      "name": "ollama",
      "protocol": "/shurli/ollama/1.0.0",
      "local_address": "localhost:11434",
      "enabled": true
    }
  ]
}
```

**Response (Text)** (tab-separated):

```
ssh	localhost:22	/shurli/ssh/1.0.0	enabled
ollama	localhost:11434	/shurli/ollama/1.0.0	enabled
```

---

### GET /v1/peers

Lists connected peers with their addresses and software version.

**By default, only shurli and relay-server peers are shown.** Shurli uses a private Kademlia DHT (`/shurli/kad/1.0.0`), isolated from the public IPFS Amino network. Your node only communicates with other Shurli nodes for DHT peer discovery.

To see all connected peers (including DHT neighbors), add `?all=true`:

```
GET /v1/peers           → only shurli/relay-server peers
GET /v1/peers?all=true  → all connected peers (including DHT neighbors)
```

**CLI**:

```bash
shurli daemon peers          # only shurli peers
shurli daemon peers --all    # all peers including DHT neighbors
```

**Response (JSON)**:

```json
{
  "data": [
    {
      "id": "12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6",
      "addresses": [
        "/ip4/203.0.113.50/tcp/7777/p2p/12D3KooWK.../p2p-circuit/p2p/12D3KooWH..."
      ],
      "agent_version": "shurli/0.1.0"
    }
  ]
}
```

**Response (Text)**:

```
12D3KooWNq8c1fN...	shurli/0.1.0	3 addrs
```

---

### GET /v1/auth

Lists authorized peers from the `authorized_keys` file. Includes verification status and expiry if set.

**Response (JSON)**:

```json
{
  "data": [
    {
      "peer_id": "12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6",
      "comment": "laptop",
      "verified": "sha256:a1b2c3d4",
      "expires_at": ""
    },
    {
      "peer_id": "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt",
      "comment": "contractor-bob",
      "verified": "",
      "expires_at": "2026-03-15T00:00:00Z"
    }
  ]
}
```

| Field | Type | Description |
|-------|------|-------------|
| `peer_id` | string | The peer's libp2p ID |
| `comment` | string | Human-readable label (from `# comment` in authorized_keys) |
| `verified` | string | SAS verification fingerprint prefix, empty if unverified |
| `expires_at` | string | RFC3339 expiry timestamp, empty if never expires |

**Response (Text)**:

```
12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6	# laptop
```

---

### GET /v1/paths

Lists active connection paths to all connected peers, including path type, transport, IP version, and latency.

**Response (JSON)**:

```json
{
  "data": [
    {
      "peer_id": "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt",
      "path_type": "DIRECT",
      "address": "/ip6/2001:db8::1/udp/9000/quic-v1",
      "connected_at": "2026-02-23T10:30:00Z",
      "transport": "quic",
      "ip_version": "IPv6",
      "last_rtt_ms": 6.1
    }
  ]
}
```

| Field | Type | Description |
|-------|------|-------------|
| `peer_id` | string | The connected peer's ID |
| `path_type` | string | `DIRECT` or `RELAYED` |
| `address` | string | Multiaddr of the connection |
| `connected_at` | string | RFC3339 timestamp of connection |
| `transport` | string | `quic` or `tcp` |
| `ip_version` | string | `IPv4` or `IPv6` |
| `last_rtt_ms` | float | Last measured RTT in milliseconds (0 if unknown) |

**Response (Text)**:

```
12D3KooWPrmh16...	DIRECT	quic	IPv6	rtt=6.1ms
```

---

### POST /v1/auth

Adds a peer to `authorized_keys` and hot-reloads the connection gater. Takes effect immediately - no restart needed.

**Request Body**:

```json
{
  "peer_id": "12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6",
  "comment": "laptop"
}
```

**Response (JSON)**:

```json
{
  "data": {
    "status": "added"
  }
}
```

---

### DELETE /v1/auth/{peer_id}

Removes a peer from `authorized_keys` and hot-reloads the connection gater. Access revoked immediately.

**Response (JSON)**:

```json
{
  "data": {
    "status": "removed"
  }
}
```

**curl**:

```bash
curl -X DELETE \
     -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/auth/12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6
```

---

### POST /v1/ping

Pings a peer using the P2P ping-pong protocol. Returns per-ping results and summary statistics.

**Request Body**:

```json
{
  "peer": "home-server",
  "count": 4,
  "interval_ms": 1000
}
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `peer` | string | required | Peer name or ID |
| `count` | int | 4 | Number of pings (API defaults to 4) |
| `interval_ms` | int | 1000 | Milliseconds between pings |

**Response (JSON)**:

```json
{
  "data": {
    "results": [
      {"seq": 1, "peer_id": "12D3KooWPrmh...", "rtt_ms": 45.2, "path": "RELAYED"},
      {"seq": 2, "peer_id": "12D3KooWPrmh...", "rtt_ms": 42.1, "path": "DIRECT"},
      {"seq": 3, "peer_id": "12D3KooWPrmh...", "rtt_ms": 43.0, "path": "DIRECT"},
      {"seq": 4, "peer_id": "12D3KooWPrmh...", "rtt_ms": 41.8, "path": "DIRECT"}
    ],
    "stats": {
      "sent": 4,
      "received": 4,
      "lost": 0,
      "loss_pct": 0.0,
      "min_ms": 41.8,
      "avg_ms": 43.0,
      "max_ms": 45.2
    }
  }
}
```

**Response (Text)**:

```
PING home-server (12D3KooWPrmh163s...):
seq=1 rtt=45.2ms path=[RELAYED]
seq=2 rtt=42.1ms path=[DIRECT]
seq=3 rtt=43.0ms path=[DIRECT]
seq=4 rtt=41.8ms path=[DIRECT]
--- home-server ping statistics ---
4 sent, 4 received, 0% loss, rtt min/avg/max = 41.8/43.0/45.2 ms
```

---

### POST /v1/traceroute

Traces the network path to a peer. Shows whether the connection is direct or relayed, with per-hop latency.

**Request Body**:

```json
{
  "peer": "home-server"
}
```

**Response (JSON)**:

```json
{
  "data": {
    "target": "home-server",
    "target_peer_id": "12D3KooWPrmh...",
    "path": "RELAYED via relay-server/0.1.0",
    "hops": [
      {
        "hop": 1,
        "peer_id": "12D3KooWK...",
        "name": "relay",
        "address": "203.0.113.50:7777",
        "rtt_ms": 23.0
      },
      {
        "hop": 2,
        "peer_id": "12D3KooWPrmh...",
        "name": "home-server",
        "address": "via relay",
        "rtt_ms": 45.0
      }
    ]
  }
}
```

**Response (Text)**:

```
traceroute to home-server (12D3KooWPrmh163s...):
 1  12D3KooWK...  (relay)  203.0.113.50:7777  23.0ms
 2  12D3KooWPrmh...  (home-server)  via relay  45.0ms
--- path: [RELAYED via relay-server/0.1.0] ---
```

---

### POST /v1/resolve

Resolves a peer name to its peer ID. Shows the resolution source.

**Request Body**:

```json
{
  "name": "home-server"
}
```

**Response (JSON)**:

```json
{
  "data": {
    "name": "home-server",
    "peer_id": "12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt",
    "source": "local_config"
  }
}
```

| Source | Meaning |
|--------|---------|
| `local_config` | Resolved from `names:` section in config |
| `peer_id` | Input was already a valid peer ID |

**Response (Text)**:

```
home-server → 12D3KooWPrmh163sTHW3mYQm7YsLsSR2wr71fPp4g6yjuGv3sGQt (source: local_config)
```

---

### POST /v1/connect

Creates a dynamic TCP proxy to a peer's service. Returns a proxy ID and the local listen address.

**Request Body**:

```json
{
  "peer": "home-server",
  "service": "ssh",
  "listen": "127.0.0.1:2222"
}
```

| Field | Type | Description |
|-------|------|-------------|
| `peer` | string | Peer name or ID |
| `service` | string | Service name to connect to |
| `listen` | string | Local address:port to listen on |

**Response (JSON)**:

```json
{
  "data": {
    "id": "proxy-1",
    "listen_address": "127.0.0.1:2222"
  }
}
```

After this call, `ssh user@127.0.0.1 -p 2222` connects to the remote peer's SSH service through the P2P tunnel.

---

### DELETE /v1/connect/{id}

Tears down an active proxy by ID.

**Response (JSON)**:

```json
{
  "data": {
    "status": "disconnected"
  }
}
```

**curl**:

```bash
curl -X DELETE \
     -H "Authorization: Bearer $(cat ~/.config/shurli/.daemon-cookie)" \
     --unix-socket ~/.config/shurli/shurli.sock \
     http://localhost/v1/connect/proxy-1
```

---

### POST /v1/expose

Dynamically registers a service on the P2P host. Other peers can connect to it immediately.

**Request Body**:

```json
{
  "name": "jupyter",
  "local_address": "localhost:8888"
}
```

**Response (JSON)**:

```json
{
  "data": {
    "status": "exposed"
  }
}
```

---

### DELETE /v1/expose/{name}

Unregisters a service from the P2P host.

**Response (JSON)**:

```json
{
  "data": {
    "status": "unexposed"
  }
}
```

---

### POST /v1/shutdown

Requests a graceful shutdown of the daemon. The daemon closes all active proxies, shuts down the HTTP server, removes the socket and cookie files, then exits.

**Response (JSON)**:

```json
{
  "data": {
    "status": "shutting down"
  }
}
```

---

## Error Codes

| HTTP Status | Meaning |
|-------------|---------|
| `200` | Success |
| `400` | Bad request (missing/invalid fields) |
| `401` | Unauthorized (missing/wrong auth token) |
| `404` | Not found (unknown proxy ID, unresolvable name) |
| `500` | Internal error (file I/O failure, network error) |

All error responses use the envelope:

```json
{
  "error": "description of what went wrong"
}
```

### Sentinel Errors

| Error | Trigger |
|-------|---------|
| `daemon already running` | Socket is in use by another daemon instance |
| `daemon not running` | Socket file doesn't exist (client can't connect) |
| `proxy not found` | Disconnect called with unknown proxy ID |
| `unauthorized` | Missing or invalid auth token |

---

## CLI Usage

The CLI communicates with the daemon over the Unix socket. It reads the cookie file automatically.

### Starting the Daemon

```bash
shurli daemon              # Start daemon (foreground)
shurli daemon start        # Same as above
```

### Querying the Daemon

```bash
shurli daemon status               # Human-readable status
shurli daemon status --json        # JSON output
shurli daemon services             # List services
shurli daemon services --json
shurli daemon peers                # List connected peers
shurli daemon peers --json
```

### Network Diagnostics (via daemon)

```bash
shurli daemon ping home-server                 # 4 pings via daemon
shurli daemon ping home-server -c 10           # 10 pings
shurli daemon ping home-server --json          # JSON output
```

### Dynamic Proxy Management

```bash
# Create a proxy
shurli daemon connect --peer home-server --service ssh --listen 127.0.0.1:2222

# Use it
ssh user@127.0.0.1 -p 2222

# Tear it down
shurli daemon disconnect proxy-1
```

### Stopping the Daemon

```bash
shurli daemon stop          # Graceful shutdown via API
```

---

## Integration Examples

### Bash Script

```bash
#!/bin/bash
SOCKET=~/.config/shurli/shurli.sock
TOKEN=$(cat ~/.config/shurli/.daemon-cookie)

# Check if daemon is running
if [ ! -S "$SOCKET" ]; then
    echo "Daemon not running"
    exit 1
fi

# Get peer count
PEERS=$(curl -s -H "Authorization: Bearer $TOKEN" \
    --unix-socket "$SOCKET" \
    http://localhost/v1/status | jq '.data.connected_peers')

echo "Connected peers: $PEERS"

# Create SSH proxy to home server
PROXY=$(curl -s -X POST -H "Authorization: Bearer $TOKEN" \
    -d '{"peer":"home-server","service":"ssh","listen":"127.0.0.1:2222"}' \
    --unix-socket "$SOCKET" \
    http://localhost/v1/connect)

echo "Proxy: $(echo $PROXY | jq -r '.data.id')"
echo "Listen: $(echo $PROXY | jq -r '.data.listen_address')"
```

### Python (direct socket)

```python
import http.client
import json
import socket

SOCKET_PATH = os.path.expanduser("~/.config/shurli/shurli.sock")
COOKIE_PATH = os.path.expanduser("~/.config/shurli/.daemon-cookie")

# Read auth token
with open(COOKIE_PATH) as f:
    token = f.read().strip()

# Connect over Unix socket
conn = http.client.HTTPConnection("localhost")
conn.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
conn.sock.connect(SOCKET_PATH)

# Query status
conn.request("GET", "/v1/status", headers={
    "Authorization": f"Bearer {token}"
})
resp = conn.getresponse()
data = json.loads(resp.read())
print(f"Peer ID: {data['data']['peer_id']}")
print(f"Peers: {data['data']['connected_peers']}")
```

---

## Socket Lifecycle

### Startup

1. Generate 32-byte random hex token
2. Write token to `~/.config/shurli/.daemon-cookie` (`0600`)
3. Check for stale socket - dial the existing socket:
   - Connection succeeds → another daemon is alive → return `ErrDaemonAlreadyRunning`
   - Connection fails → stale socket → remove it and proceed
4. Create Unix socket at `~/.config/shurli/shurli.sock`
5. Set socket permissions to `0600`
6. Start HTTP server on the socket

### Stale Socket Detection

No PID files. The daemon dials the existing socket to determine if a daemon is alive:

- If the dial succeeds, another daemon is running - refuse to start.
- If the dial fails, the socket is stale (leftover from a crash) - remove it and start fresh.

This is more reliable than PID files, which can be stale themselves.

### Shutdown

1. HTTP server shutdown with 3s grace period
2. All active proxies cancelled and awaited
3. Socket file removed
4. Cookie file removed

---

**Last Updated**: 2026-02-23


---

# Shurli Architecture

This document describes the technical architecture of Shurli, from current implementation to future vision.

## Table of Contents

- [Current Architecture (Phase 5 Complete)](#current-architecture-phase-5-complete) - what's built and working
- [Target Architecture (Phase 8+)](#target-architecture-phase-8) - planned additions
- [Observability (Batch H)](#observability-batch-h) - Prometheus metrics, audit logging
- [Adaptive Path Selection (Batch I)](#adaptive-path-selection-batch-i) - interface discovery, dial racing, STUN, peer relay
- [Core Concepts](#core-concepts) - implemented patterns
- [Security Model](#security-model) - implemented + planned extensions
- [Naming System](#naming-system) - local names implemented, network-scoped and blockchain planned
- [Federation Model](#federation-model) - planned (Phase 12)
- [Mobile Architecture](#mobile-architecture) - planned (Phase 11)

---

## Current Architecture (Phase 5 Complete)

### Component Overview

```
Shurli/
├── cmd/
│   ├── shurli/              # Single binary with subcommands
│   │   ├── main.go          # Command dispatch (daemon, ping, traceroute, resolve,
│   │   │                    #   proxy, whoami, auth, relay, config, service,
│   │   │                    #   invite, join, verify, status, init, version)
│   │   ├── cmd_daemon.go    # Daemon mode + client subcommands (status, stop, ping, etc.)
│   │   ├── serve_common.go  # Shared P2P runtime (serveRuntime) - used by daemon
│   │   ├── cmd_init.go      # Interactive setup wizard
│   │   ├── cmd_proxy.go     # TCP proxy client
│   │   ├── cmd_ping.go      # Standalone P2P ping (continuous, stats)
│   │   ├── cmd_traceroute.go # Standalone P2P traceroute
│   │   ├── cmd_resolve.go   # Standalone name resolution
│   │   ├── cmd_whoami.go    # Show own peer ID
│   │   ├── cmd_auth.go      # Auth add/list/remove/validate subcommands
│   │   ├── cmd_relay.go     # Relay add/list/remove subcommands
│   │   ├── cmd_service.go   # Service add/list/remove subcommands
│   │   ├── cmd_config.go    # Config validate/show/rollback/apply/confirm
│   │   ├── cmd_invite.go    # Generate invite code + QR + P2P handshake (--non-interactive)
│   │   ├── cmd_join.go      # Decode invite, connect, auto-configure (--non-interactive, env var)
│   │   ├── cmd_status.go    # Local status: version, peer ID, config, services, peers
│   │   ├── cmd_verify.go    # SAS verification (4-emoji fingerprint)
│   │   ├── cmd_relay_serve.go # Relay server: serve/authorize/info/config
│   │   ├── cmd_relay_pair.go  # Relay pairing code generation
│   │   ├── cmd_relay_setup.go # Relay interactive setup wizard
│   │   ├── config_template.go # Shared node config YAML template (single source of truth)
│   │   ├── relay_input.go   # Flexible relay address parsing (IP, IP:PORT, multiaddr)
│   │   ├── flag_helpers.go  # Shared CLI flag parsing helpers
│   │   └── exit.go          # Testable os.Exit wrapper
│
├── pkg/p2pnet/              # Importable P2P library
│   ├── network.go           # Core network setup, relay helpers, name resolution
│   ├── service.go           # Service registry (register/unregister, expose/unexpose)
│   ├── proxy.go             # Bidirectional TCP↔Stream proxy with half-close + byte counting
│   ├── naming.go            # Local name resolution (name → peer ID)
│   ├── identity.go          # Identity helpers (delegates to internal/identity)
│   ├── ping.go              # Shared P2P ping logic (PingPeer, ComputePingStats)
│   ├── traceroute.go        # Shared P2P traceroute (TracePeer, hop analysis)
│   ├── verify.go            # SAS verification helpers (emoji fingerprints)
│   ├── reachability.go      # Reachability grade calculation (A-F scale)
│   ├── interfaces.go        # Interface discovery, IPv6/IPv4 classification
│   ├── pathdialer.go        # Parallel dial racing (direct + relay, first wins)
│   ├── pathtracker.go       # Per-peer path quality tracking (event-bus driven)
│   ├── netmonitor.go        # Network change monitoring (event-driven)
│   ├── stunprober.go        # RFC 5389 STUN client, NAT type classification
│   ├── peerrelay.go         # Every-peer-is-a-relay (auto-enable with public IP)
│   ├── mdns.go              # mDNS LAN discovery (dedup, concurrency limiting)
│   ├── peermanager.go       # Background reconnection with exponential backoff
│   ├── netintel.go          # Presence protocol (/shurli/presence/1.0.0, gossip forwarding)
│   ├── metrics.go           # Prometheus metrics (custom registry, all shurli collectors)
│   ├── audit.go             # Structured audit logger (nil-safe, slog-based)
│   └── errors.go            # Sentinel errors
│
├── internal/
│   ├── config/              # YAML configuration loading + self-healing
│   │   ├── config.go           # Config structs (HomeNode, Client, Relay, unified NodeConfig)
│   │   ├── loader.go           # Load, validate, resolve paths, find config
│   │   ├── archive.go          # Last-known-good archive/rollback (atomic writes)
│   │   ├── confirm.go          # Commit-confirmed pattern (apply/confirm/enforce)
│   │   ├── snapshot.go         # TimeMachine-style config snapshots
│   │   └── errors.go           # Sentinel errors (ErrConfigNotFound, ErrNoArchive, etc.)
│   ├── auth/                # SSH-style authentication
│   │   ├── authorized_keys.go  # Parser + ConnectionGater loader
│   │   ├── gater.go            # ConnectionGater implementation
│   │   ├── manage.go           # AddPeer/RemovePeer/ListPeers (shared by CLI commands)
│   │   └── errors.go           # Sentinel errors
│   ├── daemon/              # Daemon API server + client
│   │   ├── types.go            # JSON request/response types (StatusResponse, PingRequest, etc.)
│   │   ├── server.go           # Unix socket HTTP server, cookie auth, proxy tracking
│   │   ├── handlers.go         # HTTP handlers, format negotiation (JSON + text)
│   │   ├── middleware.go       # HTTP instrumentation (request timing, path sanitization)
│   │   ├── client.go           # Client library for CLI → daemon communication
│   │   ├── errors.go           # Sentinel errors (ErrDaemonAlreadyRunning, etc.)
│   │   └── daemon_test.go      # Tests (auth, handlers, lifecycle, integration)
│   ├── identity/            # Ed25519 identity management (shared by shurli + relay-server)
│   │   └── identity.go      # CheckKeyFilePermissions, LoadOrCreateIdentity, PeerIDFromKeyFile
│   ├── invite/              # Invite code encoding + PAKE handshake
│   │   ├── code.go          # Binary -> base32 with dash grouping
│   │   └── pake.go          # PAKE key exchange (X25519 DH + HKDF-SHA256 + XChaCha20-Poly1305)
│   ├── relay/               # Relay pairing, admin socket, peer introductions
│   │   ├── tokens.go        # Token store (v2 pairing codes, TTL, namespace)
│   │   ├── pairing.go       # Relay pairing protocol (/shurli/relay-pair/1.0.0)
│   │   ├── notify.go        # Reconnect notifier + peer introduction delivery (/shurli/peer-notify/1.0.0)
│   │   ├── admin.go         # Relay admin Unix socket server (cookie auth, /v1/pair)
│   │   └── admin_client.go  # HTTP client for relay admin socket (fire-and-forget)
│   ├── reputation/           # Peer interaction tracking
│   │   └── history.go       # Append-only interaction log per peer (foundation for PeerManager)
│   ├── qr/                  # QR Code encoder for terminal display (inlined from skip2/go-qrcode)
│   │   ├── qrcode.go        # Public API: New(), Bitmap(), ToSmallString()
│   │   ├── encoder.go       # Data encoding (numeric, alphanumeric, byte modes)
│   │   ├── symbol.go        # Module matrix, pattern placement, penalty scoring
│   │   ├── version.go       # All 40 QR versions × 4 recovery levels
│   │   ├── gf.go            # GF(2^8) arithmetic + Reed-Solomon encoding
│   │   └── bitset.go        # Append-only bit array operations
│   ├── termcolor/           # Minimal ANSI terminal colors (replaces fatih/color)
│   │   └── color.go         # Green, Red, Yellow, Faint - respects NO_COLOR
│   ├── validate/            # Input validation helpers
│   │   ├── service.go        # ServiceName() - DNS-label format for protocol IDs
│   │   ├── network.go        # Network address validation (multiaddr, IP, port)
│   │   └── errors.go         # Sentinel errors
│   └── watchdog/            # Health monitoring + systemd integration
│       └── watchdog.go      # Health check loop, sd_notify (Ready/Watchdog/Stopping)
│
├── relay-server/            # Deployment artifacts
│   ├── setup.sh             # Deploy/verify/uninstall (builds shurli, runs relay serve)
│   └── relay-server.service # systemd unit template (installed as shurli-relay.service)
│
├── deploy/                  # Service management files
│   ├── shurli-daemon.service   # systemd unit for daemon (Linux)
│   └── com.shurli.daemon.plist # launchd plist for daemon (macOS)
│
├── configs/                 # Sample configuration files
│   ├── shurli.sample.yaml
│   ├── relay-server.sample.yaml
│   └── authorized_keys.sample
│
├── docs/                    # Project documentation
│   ├── ARCHITECTURE.md      # This file
│   ├── DAEMON-API.md        # Daemon API reference
│   ├── ENGINEERING-JOURNAL.md # Phase-by-phase engineering decisions
│   ├── MONITORING.md        # Prometheus + Grafana monitoring guide
│   ├── NETWORK-TOOLS.md     # Network diagnostic tools guide
│   ├── ROADMAP.md
│   ├── TESTING.md
│   ├── engineering-journal/ # Detailed per-phase journal entries
│   └── faq/               # FAQ sub-pages (comparisons, security, relay, design, deep dives)
│
└── examples/                # Example implementations
    └── basic-service/
```

### Network Topology (Current)

![Network topology: Client and Home Node behind NAT, connected through Relay with optional direct path via DCUtR hole-punching](images/arch-network-topology.svg)

### Authentication Flow

![Authentication flow: Client → Noise handshake → ConnectionGater check → authorized or denied → protocol handler defense-in-depth](images/arch-auth-flow.svg)

### Peer Authorization Methods

There are three ways to authorize peers:

**1. CLI - `shurli auth`**
```bash
shurli auth add <peer-id> --comment "label"
shurli auth list
shurli auth remove <peer-id>
```

**2. Invite/Join flow - zero-touch mutual authorization**
```
Machine A: shurli invite --name home     # Generates invite code + QR
Machine B: shurli join <code> --name laptop  # Decodes, connects, auto-authorizes both sides
```
The invite protocol uses PAKE-secured key exchange: ephemeral X25519 DH + token-bound HKDF-SHA256 key derivation + XChaCha20-Poly1305 AEAD encryption. The relay sees only opaque encrypted bytes during pairing. Both peers add each other to `authorized_keys` and `names` config automatically. Version byte: 0x01 = PAKE-encrypted invite, 0x02 = relay pairing code. Legacy cleartext protocol was deleted (zero downgrade surface).

**3. Manual - edit `authorized_keys` file directly**
```bash
echo "12D3KooW... # home-server" >> ~/.config/shurli/authorized_keys
```

---

## Target Architecture (Phase 8+)

### Planned Additions

Building on the current structure, future phases will add:

```
Shurli/
├── cmd/
│   ├── shurli/              # ✅ Single binary (daemon, serve, ping, traceroute, resolve,
│   │                        #   proxy, whoami, auth, relay, config, service, invite, join,
│   │                        #   status, init, version)
│   └── gateway/             # 🆕 Phase 10: Multi-mode daemon (SOCKS, DNS, TUN)
│
├── pkg/p2pnet/              # ✅ Core library (importable)
│   ├── ...existing...
│   ├── interfaces.go        # 🆕 Phase 8: Plugin interfaces (note: pkg/p2pnet/interfaces.go already exists for Batch I interface discovery)
│   └── federation.go        # 🆕 Phase 12: Network peering
│
├── internal/
│   ├── config/              # ✅ Configuration + self-healing (archive, commit-confirmed)
│   ├── auth/                # ✅ Authentication
│   ├── identity/            # ✅ Shared identity management
│   ├── validate/            # ✅ Input validation (service names, etc.)
│   ├── watchdog/            # ✅ Health checks + sd_notify
│   ├── transfer/            # 🆕 Phase 8: File transfer plugin
│   └── tun/                 # 🆕 Phase 10: TUN/TAP interface
│
├── mobile/                  # 🆕 Phase 11: Mobile apps
│   ├── ios/
│   └── android/
│
└── ...existing (relay-server/, configs, docs, examples)
```

### Service Exposure Architecture

![Service exposure: 4-layer stack from Application (SSH/HTTP/SMB/Custom) through Service Registry and TCP-Stream Proxy to libp2p Network](images/arch-service-exposure.svg)

### Gateway Daemon Modes

> **Status: Planned (Phase 10)** - not yet implemented. See [Roadmap Phase 10](ROADMAP.md) for details.

![Gateway daemon modes: SOCKS Proxy (no root, app must be configured), DNS Server (resolve peer names to virtual IPs), and TUN/TAP (fully transparent, requires root)](images/arch-gateway-modes.svg)

---

## Daemon Architecture

![Daemon architecture: P2P Runtime (relay, DHT, services, watchdog) connected bidirectionally to Unix Socket API (HTTP/1.1, cookie auth, 15 endpoints), with P2P Network below left and CLI/Scripts below right](images/daemon-api-architecture.svg)

`shurli daemon` is the single command for running a P2P host. It starts the full P2P lifecycle plus a Unix domain socket API for programmatic control (zero overhead if unused - it's just a listener).

### Shared P2P Runtime

To avoid code duplication, the P2P lifecycle is extracted into `serve_common.go`:

```go
// serveRuntime holds the shared P2P lifecycle state.
type serveRuntime struct {
    network          *p2pnet.Network
    config           *config.HomeNodeConfig
    configFile       string
    gater            *auth.AuthorizedPeerGater // nil if gating disabled
    authKeys         string                    // path to authorized_keys
    ctx              context.Context
    cancel           context.CancelFunc
    version          string
    startTime        time.Time
    kdht             *dht.IpfsDHT             // peer discovery from daemon API
    ifSummary        *p2pnet.InterfaceSummary  // interface discovery (IPv4/IPv6)
    pathDialer       *p2pnet.PathDialer        // parallel dial racing
    pathTracker      *p2pnet.PathTracker       // per-peer path quality tracking
    stunProber       *p2pnet.STUNProber        // NAT type detection
    mdnsDiscovery    *p2pnet.MDNSDiscovery     // LAN discovery (nil when disabled)
    peerManager      *p2pnet.PeerManager       // background reconnection with backoff
    netIntel         *p2pnet.NetIntel          // presence protocol (nil when disabled)
    peerRelay        *p2pnet.PeerRelay         // auto-enabled with public IP
    metrics          *p2pnet.Metrics           // nil when telemetry disabled
    peerHistory      *reputation.PeerHistory   // per-peer interaction tracking
}
```

Methods: `newServeRuntime()`, `Bootstrap()`, `ExposeConfiguredServices()`, `SetupPingPong()`, `StartWatchdog()`, `StartStatusPrinter()`, `Shutdown()`.

### Daemon Server

The daemon server (`internal/daemon/`) is decoupled from the CLI via the `RuntimeInfo` interface:

```go
type RuntimeInfo interface {
    Network() *p2pnet.Network
    ConfigFile() string
    AuthKeysPath() string
    GaterForHotReload() GaterReloader            // nil if gating disabled
    Version() string
    StartTime() time.Time
    PingProtocolID() string
    ConnectToPeer(ctx context.Context, peerID peer.ID) error
    Interfaces() *p2pnet.InterfaceSummary        // nil before discovery
    PathTracker() *p2pnet.PathTracker             // nil before bootstrap
    STUNResult() *p2pnet.STUNResult               // nil before probe
    IsRelaying() bool                             // true if peer relay enabled
}
```

The `serveRuntime` struct implements this interface in `cmd_daemon.go`, keeping the daemon package importable without depending on CLI code.

### Cookie-Based Authentication

Every API request requires `Authorization: Bearer <token>`. The token is a 32-byte random hex string written to `~/.config/shurli/.daemon-cookie` with `0600` permissions. This follows the Bitcoin Core / Docker pattern - no plaintext passwords in config, token rotates on restart, same-user access only.

### Stale Socket Detection

No PID files. On startup, the daemon dials the existing socket:
- Connection succeeds → another daemon is alive → return error
- Connection fails → stale socket from a crash → remove and proceed

### Unix Socket API

15 HTTP endpoints over Unix domain socket. Every endpoint supports JSON (default) and plain text (`?format=text` or `Accept: text/plain`). Full API reference in [Daemon API](DAEMON-API.md).

### Dynamic Proxy Management

The daemon tracks active TCP proxies in memory. Scripts can create proxies via `POST /v1/connect` and tear them down via `DELETE /v1/connect/{id}`. All proxies are cleaned up on daemon shutdown.

### Auth Hot-Reload

`POST /v1/auth` and `DELETE /v1/auth/{peer_id}` modify the `authorized_keys` file and immediately reload the connection gater via the `GaterReloader` interface. Access grants and revocations take effect without restart.

---

## Concurrency Model

Background goroutines follow a consistent pattern for lifecycle management:

### Ticker + Select Pattern

All recurring background tasks (relay reservation, DHT advertising, status printing, stats logging) use `time.Ticker` with `select` on `ctx.Done()`:

```go
go func() {
    ticker := time.NewTicker(interval)
    defer ticker.Stop()
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            // do work
        }
    }
}()
```

This ensures goroutines exit cleanly when the parent context is cancelled (e.g., on Ctrl+C).

### Watchdog + sd_notify

Both `shurli daemon` and `shurli relay serve` run a watchdog goroutine (`internal/watchdog`) that performs health checks every 30 seconds:

- **shurli daemon**: Checks host has listen addresses, relay reservation is active, and Unix socket is responsive
- **shurli relay serve**: Checks host has listen addresses and protocols are registered

On success, sends `WATCHDOG=1` to systemd via the `NOTIFY_SOCKET` unix datagram socket (pure Go, no CGo). On non-systemd systems (macOS), all sd_notify calls are no-ops. `READY=1` is sent after startup completes; `STOPPING=1` on shutdown.

The systemd service uses `Type=notify` and `WatchdogSec=90` (3x the 30s check interval) so systemd will restart the process if health checks stop succeeding.

### Health Check HTTP Endpoint (`/healthz`)

The relay server optionally exposes a `/healthz` HTTP endpoint for external monitoring (Prometheus, UptimeKuma, etc.). Disabled by default in config:

```yaml
health:
  enabled: true
  listen_address: "127.0.0.1:9090"
```

The endpoint returns JSON with: `status`, `peer_id`, `version`, `uptime_seconds`, `connected_peers`, `protocols`. Bound to localhost by default - not exposed to the internet. The HTTP server starts after the relay service is up and shuts down gracefully on SIGTERM.

### Commit-Confirmed Enforcement

When a commit-confirmed is active (`shurli config apply --confirm-timeout`), `serve` starts an `EnforceCommitConfirmed` goroutine that waits for the deadline. If `shurli config confirm` is not run before the timer fires, the goroutine reverts the config and calls `os.Exit(1)`. Systemd then restarts the process with the restored config.

### Graceful Shutdown

Long-running commands (`daemon`, `proxy`, `relay serve`) handle `SIGINT`/`SIGTERM` by calling `cancel()` on their root context, which propagates to all background goroutines. The daemon also accepts shutdown requests via the API (`POST /v1/shutdown`). Deferred cleanup (`net.Close()`, `listener.Close()`, socket/cookie removal) runs after goroutines stop.

### Atomic Counters

Shared counters accessed by concurrent goroutines (e.g., bootstrap peer count) use `atomic.Int32` instead of bare `int` to prevent data races.

### Observability (Batch H)

> **Status: Implemented** - opt-in Prometheus metrics + structured audit logging.

![Observability data flow - from metric sources through Prometheus registry to /metrics endpoint](images/observability-flow.svg)

All observability features are disabled by default and opt-in via config:

```yaml
telemetry:
  metrics:
    enabled: true
    listen_address: "127.0.0.1:9091"
  audit:
    enabled: true
```

**Prometheus Metrics** (`pkg/p2pnet/metrics.go`): Uses an isolated `prometheus.Registry` (not the global default) for testability and collision-free operation. When enabled, `libp2p.PrometheusRegisterer(reg)` exposes all built-in libp2p metrics (swarm, holepunch, autonat, rcmgr, relay) alongside custom shurli metrics. When disabled, `libp2p.DisableMetrics()` is called for zero CPU overhead.

Custom shurli metrics:
- `shurli_proxy_bytes_total{direction, service}` - bytes transferred through proxy
- `shurli_proxy_connections_total{service}` - proxy connections established
- `shurli_proxy_active_connections{service}` - currently active proxy sessions
- `shurli_proxy_duration_seconds{service}` - proxy session duration
- `shurli_auth_decisions_total{decision}` - auth allow/deny counts
- `shurli_holepunch_total{result}` - hole punch success/failure
- `shurli_holepunch_duration_seconds{result}` - hole punch timing
- `shurli_daemon_requests_total{method, path, status}` - API request counts
- `shurli_daemon_request_duration_seconds{method, path, status}` - API latency
- `shurli_info{version, go_version}` - build information

**Audit Logger** (`pkg/p2pnet/audit.go`): Structured JSON events via `log/slog` with an `audit` group. All methods are nil-safe (no-op when audit is disabled). Events: auth decisions, service ACL denials, daemon API access, auth changes.

**Daemon Middleware** (`internal/daemon/middleware.go`): Wraps the HTTP handler chain (outside auth middleware) to capture request timing and status codes. Path parameters are sanitized (e.g., `/v1/auth/12D3KooW...` becomes `/v1/auth/:id`) to prevent high cardinality in metrics labels.

**Auth Decision Callback**: Uses a callback pattern (`auth.AuthDecisionFunc`) to decouple `internal/auth` from `pkg/p2pnet`, avoiding circular imports. The callback is wired in `serve_common.go` to feed both metrics counters and audit events.

**Relay Metrics**: When both health and metrics are enabled on the relay, `/metrics` is added to the existing `/healthz` HTTP mux. When only metrics is enabled, a dedicated HTTP server is started.

**Grafana Dashboard**: A pre-built dashboard (`grafana/shurli-dashboard.json`) ships with the project. Import it into any Grafana instance to visualize proxy throughput, auth decisions, hole punch success rates, API latency, and system metrics. 23 visualization panels across 6 sections: Overview, Proxy Throughput, Security, Hole Punch, Daemon API, and System.

**Reference**: `pkg/p2pnet/metrics.go`, `pkg/p2pnet/audit.go`, `internal/daemon/middleware.go`, `cmd/shurli/serve_common.go`, `grafana/shurli-dashboard.json`

### Adaptive Path Selection (Batch I)

> **Status: Implemented** - interface discovery, parallel dial racing, path tracking, network change monitoring, STUN probing, every-peer-is-a-relay.

![Adaptive Path Selection: 6 components (interface discovery, STUN probing, peer relay, parallel dial racing, path tracking, network monitoring) working together with path ranking from Direct IPv6 to VPS Relay](images/arch-adaptive-path.svg)

Six components work together to find and maintain the best connection path to each peer:

**Interface Discovery** (`pkg/p2pnet/interfaces.go`): `DiscoverInterfaces()` enumerates all network interfaces and classifies addresses as global IPv4, global IPv6, or loopback. Returns an `InterfaceSummary` with convenience flags (`HasGlobalIPv6`, `HasGlobalIPv4`). Called at startup and on every network change.

**Parallel Dial Racing** (`pkg/p2pnet/pathdialer.go`): `PathDialer.DialPeer()` replaces the old sequential connect (DHT 15s then relay 30s = 45s worst case) with parallel racing. If the peer is already connected, returns immediately. Otherwise fires DHT and relay strategies concurrently; first success wins, loser is cancelled. Classifies winning path as `DIRECT` or `RELAYED` based on multiaddr inspection.

![Dial Racing Flow: entry point checks if already connected (instant return), otherwise launches DHT discovery and relay circuit in parallel, first success wins with path classification](images/arch-dial-racing.svg)

**Path Quality Tracking** (`pkg/p2pnet/pathtracker.go`): `PathTracker` subscribes to libp2p's event bus (`EvtPeerConnectednessChanged`) for connect/disconnect events. Maintains per-peer path info: path type, transport (quic/tcp), IP version, connected time, last RTT. Exposed via `GET /v1/paths` daemon API. Prometheus labels: `path_type`, `transport`, `ip_version`.

**Network Change Monitoring** (`pkg/p2pnet/netmonitor.go`): `NetworkMonitor` watches for interface/address changes by polling `DiscoverInterfaces()` and diffing against the previous snapshot. On change, fires registered callbacks. Triggers: interface re-scan, STUN re-probe, peer relay auto-detect update.

**STUN NAT Detection** (`pkg/p2pnet/stunprober.go`): Zero-dependency RFC 5389 STUN client. Probes multiple STUN servers concurrently, collects external addresses, classifies NAT type (none, full-cone, address-restricted, port-restricted, symmetric). `HolePunchable()` indicates whether DCUtR hole-punching is likely to succeed. Runs in background at startup (non-blocking) and re-probes on network change.

**Every-Peer-Is-A-Relay** (`pkg/p2pnet/peerrelay.go`): Any peer with a detected global IP auto-enables circuit relay v2 with conservative resource limits (4 reservations, 16 circuits, 128KB/direction, 10min sessions). Uses the existing `ConnectionGater` for authorization (no new ACL needed). Auto-detects on startup and network changes. Disables when public IP is lost.

**Path Ranking**: direct IPv6 > direct IPv4 > STUN-punched > peer relay > VPS relay. If all paths fail, the system falls back to relay and tells the user honestly.

**Reference**: `pkg/p2pnet/interfaces.go`, `pkg/p2pnet/pathdialer.go`, `pkg/p2pnet/pathtracker.go`, `pkg/p2pnet/netmonitor.go`, `pkg/p2pnet/stunprober.go`, `pkg/p2pnet/peerrelay.go`, `cmd/shurli/serve_common.go`

---

## Core Concepts

### 1. Service Definition

Services are defined in configuration and registered at runtime:

```go
type Service struct {
    Name         string   // "ssh", "web", etc.
    Protocol     string   // "/shurli/ssh/1.0.0"
    LocalAddress string   // "localhost:22"
    Enabled      bool     // Enable/disable
}

type ServiceRegistry struct {
    services map[string]*Service
    host     host.Host
}

func (r *ServiceRegistry) RegisterService(svc *Service) error {
    // Set up stream handler for this service's protocol
    r.host.SetStreamHandler(svc.Protocol, func(s network.Stream) {
        // 1. Authorize peer
        if !r.isAuthorized(s.Conn().RemotePeer(), svc.Name) {
            s.Close()
            return
        }

        // 2. Dial local service
        localConn, err := net.Dial("tcp", svc.LocalAddress)
        if err != nil {
            s.Close()
            return
        }

        // 3. Bidirectional proxy
        go io.Copy(s, localConn)
        io.Copy(localConn, s)
    })
}
```

### 2. Bidirectional TCP↔Stream Proxy

```go
func ProxyStreamToTCP(stream network.Stream, tcpAddr string) error {
    // Connect to local TCP service
    tcpConn, err := net.Dial("tcp", tcpAddr)
    if err != nil {
        return err
    }
    defer tcpConn.Close()

    // Bidirectional copy
    errCh := make(chan error, 2)

    go func() {
        _, err := io.Copy(tcpConn, stream)
        errCh <- err
    }()

    go func() {
        _, err := io.Copy(stream, tcpConn)
        errCh <- err
    }()

    // Wait for either direction to finish
    return <-errCh
}
```

### 3. Name Resolution

**Currently implemented**: `LocalFileResolver` resolves friendly names (configured via `shurli invite`/`shurli join` or manual YAML) to peer IDs. Direct peer ID strings are always accepted as fallback.

```go
type LocalFileResolver struct {
    names map[string]peer.ID
}

func (r *LocalFileResolver) Resolve(name string) (peer.ID, error) {
    if id, ok := r.names[name]; ok {
        return id, nil
    }
    return "", ErrNotFound
}
```

> **Planned (Phase 8/13)**: The `NameResolver` interface, `DHTResolver`, multi-tier chaining, and blockchain naming are planned extensions. See [Naming System](#naming-system) below and [Roadmap Phase 13](ROADMAP.md).

---

## Security Model

### Authentication Layers

**Layer 1: Network Level (ConnectionGater)**
- Executed during connection handshake
- Blocks unauthorized peers before any data exchange
- Fastest rejection (minimal resource usage)

**Layer 2: Protocol Level (Stream Handler)**
- Defense-in-depth validation
- Per-service authorization (optional)
- Can override global authorized_keys

### Per-Service Authorization

> **Status: Implemented** (Pre-Batch H)

Each service can optionally restrict access to specific peer IDs via `allowed_peers`. When set, only listed peers can connect to that service. When omitted (nil), all globally authorized peers can access it.

```yaml
services:
  ssh:
    enabled: true
    local_address: "localhost:22"
    allowed_peers: ["12D3KooW..."]  # Only these peers can access SSH

  web:
    enabled: true
    local_address: "localhost:80"
    # No allowed_peers = all authorized peers can access
```

The ACL check runs in the stream handler before dialing the local TCP service, so rejected peers never trigger a connection to the backend.

### Federation Trust Model

> **Status: Planned (Phase 12)** - not yet implemented. See [Federation Model](#federation-model) and [Roadmap Phase 12](ROADMAP.md).

```yaml
# relay-server.yaml (planned config format)
federation:
  peers:
    - network_name: "alice"
      relay: "/ip4/.../p2p/..."
      trust_level: "full"      # Bidirectional routing

    - network_name: "bob"
      relay: "/ip4/.../p2p/..."
      trust_level: "one_way"   # Only alice → grewal, not grewal → alice
```

---

## Naming System

### Multi-Tier Resolution

> **What works today**: Tier 1 (Local Override) - friendly names configured via `shurli invite`/`join` or manual YAML - and the Direct Peer ID fallback. Tiers 2-3 (Network-Scoped, Blockchain) are planned for Phase 10/13.

![Name resolution waterfall: Local Override → Network-Scoped → Blockchain → Direct Peer ID, with fallthrough on each tier](images/arch-naming-system.svg)

### Network-Scoped Name Format

> **Status: Planned (Phase 10/13)** - not yet implemented. Currently only simple names work (e.g., `home`, `laptop` as configured in local YAML). The dotted network format below is a future design.

```
Format: <hostname>.<network>[.<tld>]

Examples (planned):
laptop.grewal           # Query grewal relay
desktop.alice           # Query alice relay
phone.bob.p2p           # Query bob relay (explicit .p2p TLD)
home.grewal.local       # mDNS compatible
```

---

## Federation Model

> **Status: Planned (Phase 12)** - not yet implemented. See [Roadmap Phase 12](ROADMAP.md).

### Relay Peering

![Federation model: three networks (A, B, C) with relay peering - cross-network connections routed through federated relays](images/arch-federation.svg)

---

## Mobile Architecture

> **Status: Planned (Phase 11)** - not yet implemented. See [Roadmap Phase 11](ROADMAP.md).

![Mobile architecture: iOS uses NEPacketTunnelProvider, Android uses VPNService - both embed libp2p-go via gomobile](images/arch-mobile.svg)

---

## Performance Considerations

### Transport Preference

Both `shurli daemon` and `shurli relay serve` register transports in this order:

1. **QUIC** (preferred) - 3 RTTs to establish, native multiplexing, better for hole-punching. libp2p's smart dialing (built into v0.47.0) ranks QUIC addresses higher than TCP.
2. **TCP** - 4 RTTs, universal fallback for networks that block UDP.
3. **WebSocket** - Anti-censorship transport that looks like HTTPS to deep packet inspection (DPI). Commented out by default in sample configs.

### AutoNAT v2

Enabled on all hosts. AutoNAT v2 performs per-address reachability testing with nonce-based dial verification. This means the node knows which specific addresses (IPv4, IPv6, QUIC, TCP) are publicly reachable, rather than a single "public or private" determination. Also prevents amplification attacks by requiring the probing peer to prove it controls the claimed address.

### Version in Identify Protocol

All hosts set `libp2p.UserAgent()` so peers can discover each other's software version via the Identify protocol:
- **shurli nodes**: `shurli/<version>` (e.g., `shurli/0.1.0` or `shurli/dev`)
- **relay server**: `relay-server/<version>`

The UserAgent is stored in each peer's peerstore under the `AgentVersion` key after the Identify handshake completes (automatically on connect).

### Connection Optimization

1. **Relay vs Direct** (implemented):
   - Always attempt DCUtR for direct connection
   - Fall back to relay if hole-punching fails

2. **Connection Pooling** (planned):
   - Reuse P2P streams for multiple requests
   - Multiplex services over single connection
   - Keep-alive mechanisms

3. **Bandwidth Management** (planned):
   - QoS for different service types
   - Rate limiting per service
   - Bandwidth monitoring and alerts

> Items marked "planned" are tracked in the [Roadmap](ROADMAP.md) under Phase 4C deferred items and Phase 14+.

---

## Security Hardening

### Relay Resource Limits

The relay server enforces resource limits via libp2p's circuit relay v2 `WithResources()` and `WithLimit()` options. All limits are configurable in `relay-server.yaml` under the `resources:` section. Defaults are tuned for a private relay serving 2-10 peers with SSH/XRDP workloads:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_reservations` | 128 | Total active relay slots |
| `max_circuits` | 16 | Open relay connections per peer |
| `max_reservations_per_ip` | 8 | Reservations per source IP |
| `max_reservations_per_asn` | 32 | Reservations per AS number |
| `reservation_ttl` | 1h | Reservation lifetime |
| `session_duration` | 10m | Max per-session duration |
| `session_data_limit` | 64MB | Max data per session per direction |

Session duration and data limits are raised from libp2p defaults (2min/128KB) to support real workloads (SSH, XRDP, file transfers). Zero-valued fields in config are filled with defaults at load time.

### Key File Permission Verification

Private key files are verified on load to ensure they are not readable by group or others. The shared `internal/identity` package provides `CheckKeyFilePermissions()` and `LoadOrCreateIdentity()`, used by both `shurli daemon` and `shurli relay serve`:

- **Expected**: `0600` (owner read/write only)
- **On violation**: Returns error with actionable fix: `chmod 600 <path>`
- **Windows**: Check is skipped (Windows uses ACLs, not POSIX permissions)

Keys are already created with `0600` permissions, but this check catches degradation from manual `chmod`, file copies across systems, or archive extraction.

### Config Self-Healing

The config system provides three layers of protection against bad configuration:

1. **Archive/Rollback** (`internal/config/archive.go`): On each successful `daemon` or `relay serve` startup, the validated config is archived as `.{name}.last-good.yaml` next to the original. If a future edit breaks the config, `shurli config rollback` restores it. Archive writes are atomic (write temp file + rename).

2. **Commit-Confirmed** (`internal/config/confirm.go`): For remote config changes, `shurli config apply` backs up the current config, applies the new one, and writes a pending marker with a deadline. If `shurli config confirm` is not run before the deadline, the serve process reverts the config and exits. Systemd restarts with the restored config.

3. **Validation CLI** (`shurli config validate`): Check config syntax and required fields without starting the node. Useful before restarting a remote service.

### Service Name Validation

Service names are validated before use in protocol IDs to prevent injection attacks. Names flow into `fmt.Sprintf("/shurli/%s/1.0.0", name)` - without validation, a name like `ssh/../../evil` or `foo\nbar` creates ambiguous or invalid protocol IDs.

The validation logic lives in `internal/validate/validate.go` (`validate.ServiceName()`), shared by all callers.

**Validation rules** (DNS-label format):
- 1-63 characters
- Lowercase alphanumeric and hyphens only
- Must start and end with alphanumeric character
- Regex: `^[a-z0-9]([a-z0-9-]{0,61}[a-z0-9])?$`

Validated at four points:
1. `shurli service add` - rejects bad names at CLI entry
2. `ValidateNodeConfig()` - rejects bad names in config before startup
3. `ExposeService()` - rejects bad names at service registration time
4. `ConnectToService()` - rejects bad names at connection time

---

## Security Considerations

### Threat Model

**Threats Addressed**:
- ✅ Unauthorized peer access (ConnectionGater)
- ✅ Man-in-the-middle (libp2p Noise encryption)
- ✅ Replay attacks (Noise protocol nonces)
- ✅ Relay bandwidth theft (relay authentication + resource limits)
- ✅ Relay resource exhaustion (configurable per-peer/per-IP/per-ASN limits)
- ✅ Protocol ID injection (service name validation)
- ✅ Key file permission degradation (0600 check on load)
- ✅ Newline injection in authorized_keys (sanitized comments)
- ✅ YAML injection via peer names (allowlisted characters)
- ✅ OOM via unbounded stream reads (512-byte buffer limits)
- ✅ Symlink attacks on temp files (os.CreateTemp with random suffix)
- ✅ Multiaddr injection in config (validated before writing)
- ✅ Per-service access control (AllowedPeers ACL on each service)
- ✅ Host resource exhaustion (libp2p ResourceManager with auto-scaled limits)
- ✅ SYN/UDP flood on relay (iptables rate limiting, SYN cookies, conntrack tuning)
- ✅ IP spoofing on relay (reverse path filtering via rp_filter)
- ✅ Runaway relay process (systemd cgroup limits: memory, CPU, tasks)

**Threats NOT Addressed** (out of scope):
- ❌ Relay compromise (relay can see metadata, not content)
- ❌ Peer key compromise (users must secure private keys)

### Best Practices

1. **Key Management**:
   - Private keys: 0600 permissions
   - authorized_keys: 0600 permissions
   - Never commit keys to git

2. **Network Segmentation**:
   - Use per-service authorized_keys when needed
   - Limit service exposure (disable unused services)
   - Audit authorized_keys regularly

3. **Relay Security**:
   - Enable relay authentication in production
   - Monitor relay bandwidth usage
   - Use non-standard ports

---

## Scalability

### Current Limitations

- **Relay bandwidth**: Limited by VPS plan (~1TB/month)
- **Connections per relay**: Limited by file descriptors (~1000-10000)
- **DHT lookups**: Slow for large networks (10-30 seconds)

### Future Improvements

- Multiple relay failover/load balancing
- Relay-to-relay mesh for redundancy
- Optimized peer routing (shortest path)
- Distributed hash table optimization
- Connection multiplexing

---

## Technology Stack

**Core**:
- Go 1.26+
- libp2p v0.47.0 (networking)
- Private Kademlia DHT (`/shurli/kad/1.0.0` - isolated from IPFS Amino). Optional namespace isolation: `discovery.network: "my-crew"` produces `/shurli/my-crew/kad/1.0.0`, creating protocol-level separation between peer groups
- Noise protocol (encryption)
- QUIC transport (preferred - 3 RTTs vs 4 for TCP)
- AutoNAT v2 (per-address reachability testing)

**Why libp2p**: Shurli's networking foundation is the same stack used by Ethereum's consensus layer (Beacon Chain), Filecoin, and Polkadot - networks collectively securing hundreds of billions in value. When Ethereum chose a P2P stack for their most critical infrastructure, they picked libp2p. Improvements driven by these ecosystems (transport optimizations, Noise hardening, gossipsub refinements) flow back to the shared codebase. See the [FAQ comparisons](faq/comparisons.md#how-do-p2p-networking-stacks-compare) for detailed comparisons.

**Optional**:
- Ethereum (blockchain naming)
- IPFS (distributed storage)
- gomobile (iOS/Android)

---

**Last Updated**: 2026-02-26
**Architecture Version**: 3.3 (Phase 5: mDNS, PeerManager, NetIntel presence + gossip forwarding)


---

# Testing Guide: SSH Access via P2P Network

This guide walks through testing the complete Shurli system with SSH service exposure.

## Goal

Connect to your home computer's SSH server from a client device (laptop/phone) through the P2P network, traversing CGNAT/NAT using a relay server.

```
[Client]  ──shurli proxy──▶  [Relay Server]  ◀──shurli daemon──  [Home Server]  ──TCP──▶  [SSH :22]
 (Laptop)                       (VPS)                         (Behind CGNAT)
```

## Prerequisites

### 1. Three Machines/Terminals

- **Relay Server**: VPS with public IP (Linode, DigitalOcean, AWS, etc.)
- **Home Server**: Your home computer behind CGNAT/NAT (runs `shurli daemon`)
- **Client**: Laptop or another device (runs `shurli proxy`)

### 2. SSH Server Running

On your home computer:
```bash
# Check if SSH server is running
sudo systemctl status sshd  # or ssh on macOS

# Start if not running (Linux)
sudo systemctl start sshd

# macOS - enable in System Preferences > Sharing > Remote Login
```

### 3. Build shurli

```bash
# Build shurli (single binary - handles both client and relay server)
go build -ldflags="-s -w" -trimpath -o shurli ./cmd/shurli
```

---

## Step 1: Deploy Relay Server

See [relay-server/README.md](../relay-server/README.md) for the full VPS setup guide.

Quick version:

```bash
cd relay-server
cp ../configs/relay-server.sample.yaml relay-server.yaml
# Edit relay-server.yaml if needed (defaults are fine)

# Build from project root
cd ..
go build -ldflags="-s -w" -trimpath -o relay-server/shurli ./cmd/shurli
cd relay-server && ./shurli relay serve
```

**Expected output:**
```
=== Relay Server (Circuit Relay v2) ===
🆔 Relay Peer ID: 12D3KooWABC...XYZ
📍 Listening on:
  /ip4/YOUR_VPS_IP/tcp/7777
  /ip4/YOUR_VPS_IP/udp/7777/quic-v1
✅ Relay server is running!
```

**Save these values:**
- Relay Peer ID: `12D3KooWABC...XYZ`
- VPS IP: `YOUR_VPS_IP`

---

## Step 2: Set Up Home Server

### Run the setup wizard

```bash
./shurli init
```

The wizard will:
1. Create `~/.config/shurli/` directory
2. Ask for your relay server address (accepts flexible formats):
   - Full multiaddr: `/ip4/1.2.3.4/tcp/7777/p2p/12D3KooW...`
   - IP and port: `1.2.3.4:7777` (then prompts for peer ID)
   - Bare IP: `1.2.3.4` (uses default port 7777, then prompts for peer ID)
   - IPv6: `[2600:3c00::1]:7777` or `[2600:3c00::1]`
3. Generate an Ed25519 identity key
4. Display your **Peer ID** as text + QR code (share with peers)
5. Write `config.yaml`, `identity.key`, and `authorized_keys`

**Tip**: Check your peer ID anytime with `./shurli whoami`

### Configure services

Add services via CLI (preferred) or by editing the config file:

```bash
# Add via CLI
./shurli service add ssh localhost:22
./shurli service add xrdp localhost:3389

# Or edit ~/.config/shurli/config.yaml directly
```

Ensure `force_private_reachability` is set for CGNAT:

```yaml
network:
  force_private_reachability: true  # CRITICAL for CGNAT (Starlink, etc.)
```

### Start the server

```bash
./shurli daemon
```

**Expected output:**
```
Loaded configuration from ~/.config/shurli/config.yaml
🏠 Peer ID: 12D3KooWHOME...ABC
✅ Connected to relay 12D3KooWABC...
✅ Relay address: /ip4/YOUR_VPS_IP/tcp/7777/p2p/12D3KooWABC.../p2p-circuit/p2p/12D3KooWHOME...ABC
✅ Registered service: ssh (protocol: /shurli/ssh/1.0.0, local: localhost:22)
```

**Save the Home Server Peer ID**: `12D3KooWHOME...ABC`

---

## Step 3: Set Up Client

### Run the setup wizard

```bash
./shurli init
```

### Authorize peers

**Option A: Invite/Join flow (recommended - handles both sides automatically)**

On the home server:
```bash
./shurli invite --name home
# Displays an invite code + QR code. Share the code with the client.
```

On the client:
```bash
./shurli join <invite-code> --name laptop
# Automatically: connects to inviter, exchanges peer IDs,
# adds each other to authorized_keys, adds name mapping.
```

**Option B: CLI commands**

On the client, add the home server's peer ID:
```bash
./shurli auth add 12D3KooWHOME...ABC --comment "home-server"
```

Do the same on the home server - add the client's peer ID:
```bash
./shurli auth add 12D3KooWCLIENT...XYZ --comment "laptop"
```

Verify with:
```bash
./shurli auth list
```

**Option C: Manual file edit**
```bash
# Edit ~/.config/shurli/authorized_keys
# Add the peer ID (one per line):
12D3KooWHOME...ABC  # home-server
```

### Add friendly name

If you used the invite/join flow, names are added automatically. Otherwise, edit `~/.config/shurli/config.yaml` on the client:

```yaml
# Map friendly names to peer IDs:
names:
  home: "12D3KooWHOME...ABC"  # From Step 2
```

---

## Step 4: Test SSH Connection via P2P

### Test connectivity first

```bash
./shurli ping home
```

You should see a successful ping/pong response.

### Start the SSH proxy

```bash
./shurli proxy home ssh 2222
```

This creates a local TCP listener on port 2222 that tunnels through the P2P network to the home server's SSH service.

### Connect via SSH

In another terminal:

```bash
ssh -p 2222 your_username@localhost
```

You should see your home computer's SSH prompt!

---

## Step 5: Test Other Services

### XRDP (Remote Desktop)

On the home server, enable XRDP in config:

```yaml
services:
  ssh:
    enabled: true
    local_address: "localhost:22"
  xrdp:
    enabled: true
    local_address: "localhost:3389"
```

Restart `shurli daemon`, then on the client:

```bash
./shurli proxy home xrdp 13389
# Then connect:
xfreerdp /v:localhost:13389 /u:your_username
```

### Any TCP Service

```yaml
services:
  web:
    enabled: true
    local_address: "localhost:8080"
```

```bash
./shurli proxy home web 8080
# Then: curl http://localhost:8080
```

---

## Managing Relay Addresses

After initial setup, you can add or remove relay servers:

```bash
# Add a relay (flexible formats)
./shurli relay add 1.2.3.4 --peer-id 12D3KooW...
./shurli relay add 1.2.3.4:7777 --peer-id 12D3KooW...
./shurli relay add /ip4/1.2.3.4/tcp/7777/p2p/12D3KooW...

# List configured relays
./shurli relay list

# Remove a relay
./shurli relay remove /ip4/1.2.3.4/tcp/7777/p2p/12D3KooW...
```

### Relay health check

On the VPS, verify the relay is healthy:
```bash
sudo ./setup.sh --check
```
This shows systemd status, peer ID, public IPs, full multiaddrs, and a QR code for easy sharing.

---

## Troubleshooting

### Relay Connection Failed

```
⚠️  Could not connect to relay
```

**Fix:**
- Verify VPS firewall allows TCP 7777 and UDP 7777
- Check relay server is actually running
- Verify relay peer ID is correct in config

### No Relay Address

```
⚠️  No relay addresses yet
```

**Fix:**
- Ensure `force_private_reachability: true` in home server config
- Wait 10-15 seconds for AutoRelay
- Check relay server logs for reservation requests

### SSH Service Not Found

```
Failed to connect to SSH service: protocol not supported
```

**Fix:**
- Verify `services.ssh.enabled: true` in home server config
- Check server logs for "Registered service: ssh"
- Ensure SSH protocol ID matches: `/shurli/ssh/1.0.0`

### Connection Refused on localhost:22

```
Failed to connect to local service localhost:22
```

**Fix:**
- Start SSH server on home computer
- Check: `sudo systemctl status sshd`
- Verify SSH is listening: `netstat -tlnp | grep :22`

### Cannot Resolve Target

```
Cannot resolve target "home"
```

**Fix:**
- Add name mapping to `names:` section in client config
- Or use the full peer ID directly: `shurli proxy 12D3KooW... ssh 2222`

### Discovery Not Working

```
📡 Searching for peers... (no results)
```

**Fix:**
- Verify both nodes use the same `rendezvous` string in config
- Check DHT is bootstrapped
- Wait 30-60 seconds for DHT propagation

---

## Success Criteria

- [ ] Relay server running and accessible
- [ ] Home server gets relay address with `/p2p-circuit`
- [ ] `shurli ping home` succeeds from client
- [ ] `shurli proxy home ssh 2222` creates local listener
- [ ] `ssh -p 2222 user@localhost` connects to home computer
- [ ] XRDP / other TCP services also work

---

## Unit Tests

Shurli has automated unit tests for core packages. These run in CI (GitHub Actions) on every push.

### Running Tests

All packages are in a single Go module. Run everything from the project root:

```bash
# Run all tests with race detection (same as CI)
go test -race -count=1 ./...

# Run tests for a specific package
go test -race ./internal/config/
go test -race ./internal/auth/
go test -race ./internal/invite/
go test -race ./cmd/shurli/

# Verbose output (see individual test names)
go test -race -v ./internal/auth/
```

### Test Coverage

| Package | Tests | What's covered |
|---------|-------|---------------|
| `internal/config` | `loader_test.go` | Config loading, YAML parsing, validation (all config types), path resolution, config version handling, FindConfigFile discovery |
| `internal/config` | `archive_test.go` | Archive path derivation, archive/rollback round-trip, permissions (0600), overwrite semantics, no temp file leaks, ErrNoArchive sentinel |
| `internal/config` | `confirm_test.go` | Begin/confirm lifecycle, duplicate prevention (ErrCommitConfirmedPending), ErrNoPending, ApplyCommitConfirmed file swap, EnforceCommitConfirmed timeout revert, context cancellation, expired deadline handling |
| `internal/watchdog` | `watchdog_test.go` | Health check loop execution, unhealthy check logging, context cancellation, default interval, sd_notify no-op without NOTIFY_SOCKET, sd_notify error on bad socket |
| `internal/auth` | `gater_test.go` | ConnectionGater: inbound/outbound filtering, peer authorization, hot-reload |
| `internal/auth` | `authorized_keys_test.go` | File loading, comment handling, invalid peer IDs, missing files |
| `internal/auth` | `manage_test.go` | AddPeer (with duplicate/sanitize), RemovePeer (atomic write, preserves comments), ListPeers |
| `internal/identity` | `identity_test.go` | Key creation, persistence, file permissions, PeerIDFromKeyFile |
| `internal/validate` | `service_test.go` | Service name validation (valid/invalid cases, max length) |
| `internal/invite` | `code_test.go` | Encode/decode round-trip, invalid codes, trailing junk rejection |
| `cmd/shurli` | `relay_input_test.go` | Relay address parsing (IPv4, IPv6, multiaddr detection, port validation) |
| `pkg/p2pnet` | `integration_test.go` | In-process libp2p host-to-host streaming, half-close semantics, P2P-to-TCP proxy, DialWithRetry retry/backoff, UserAgent exchange via Identify protocol |
| `pkg/p2pnet` | `interfaces_test.go` | Interface discovery, IPv6/IPv4 classification, global unicast detection |
| `pkg/p2pnet` | `pathdialer_test.go` | Parallel dial racing, already-connected fast path, path type classification |
| `pkg/p2pnet` | `pathtracker_test.go` | Path quality tracking, event-bus subscription, per-peer path info |
| `pkg/p2pnet` | `netmonitor_test.go` | Network change monitoring, interface diff detection, callback firing |
| `pkg/p2pnet` | `stunprober_test.go` | STUN probing, NAT type classification, multi-server concurrent probing |
| `pkg/p2pnet` | `peerrelay_test.go` | Peer relay auto-enable/disable, global IP detection, resource limits |

---

## Benchmarks

Performance benchmarks establish baselines for hot-path and cold-path functions.

### Running Benchmarks

```bash
# Run all benchmarks with memory stats
go test -bench=. -benchmem ./internal/auth/
go test -bench=. -benchmem ./internal/invite/
go test -bench=. -benchmem ./internal/config/
go test -bench=. -benchmem ./pkg/p2pnet/

# For statistical comparison (3+ runs recommended)
go test -bench=. -benchmem -count=3 ./internal/auth/

# Compare before/after with benchstat
go install golang.org/x/perf/cmd/benchstat@latest
go test -bench=. -benchmem -count=5 ./internal/auth/ > old.txt
# (make changes)
go test -bench=. -benchmem -count=5 ./internal/auth/ > new.txt
benchstat old.txt new.txt
```

### Benchmark Coverage

| File | Benchmarks | Path Type | What's Measured |
|------|-----------|-----------|-----------------|
| `internal/auth/gater_bench_test.go` | `InterceptSecuredAllowed`, `InterceptSecuredDenied`, `IsAuthorized` | Hot (per-connection) | RWMutex + map lookup latency |
| `internal/auth/authorized_keys_bench_test.go` | `LoadAuthorizedKeys5`, `LoadAuthorizedKeys50` | Cold (startup/reload) | File parse + peer ID decode |
| `internal/invite/code_bench_test.go` | `Encode`, `Decode` | Mixed (per-invite) | Base32 + multihash + multiaddr ops |
| `internal/config/loader_bench_test.go` | `LoadNodeConfig`, `ValidateNodeConfig` | Cold (startup) | YAML parse, validation |
| `pkg/p2pnet/naming_bench_test.go` | `ResolveByName`, `ResolveByPeerID` | Hot (per-proxy) | Map lookup vs peer.Decode fallback |

---

### Coverage-Instrumented Docker Tests

Docker integration tests exercise the actual compiled binary end-to-end (relay server, invite/join flow, ping through circuit relay). The binary is built with `go build -cover`, so coverage data is captured when processes exit.

```bash
# Run Docker tests with coverage collection
mkdir -p coverage/integration
SHURLI_COVDIR="$PWD/coverage/integration" \
  go test -tags integration -v -timeout 5m ./test/docker/

# Run unit tests with binary-format coverage (for merging)
mkdir -p coverage/unit
go test -cover ./... -args -test.gocoverdir="$PWD/coverage/unit"

# Merge unit + Docker coverage
mkdir -p coverage/merged
go tool covdata merge -i=coverage/unit,coverage/integration -o=coverage/merged

# Generate combined report
go tool covdata textfmt -i=coverage/merged -o=coverage/combined.out
go tool cover -func=coverage/combined.out | tail -1

# HTML visualization
go tool cover -html=coverage/combined.out -o=coverage/report.html
```

This captures code paths that unit tests cannot reach: `runRelayServe`, `runDaemon`, `runInvite`, `runJoin`, `runPing` through real P2P circuits.

### CI Pipeline

GitHub Actions runs on every push to `main` and `dev/next-iteration`. All commands run from the project root against the single Go module:

1. **Build** - all packages compile (`go build ./...`)
2. **Vet** - static analysis (`go vet ./...`)
3. **Test** - all tests with race detection (`go test -race -count=1 ./...`)
4. **Coverage** - unit + Docker integration coverage merged and reported

Config: [`.github/workflows/ci.yml`](../.github/workflows/ci.yml)

---

## Logging in Tests

Library code uses `log/slog` for structured logging. In tests, slog output goes to stderr by default, which `go test` captures and only shows on failure. No special test configuration is needed.

For benchmarks that previously used `log.New(io.Discard, ...)` to suppress logging, slog's default handler is used instead - the small overhead is part of the realistic benchmark measurement.

---

**Last Updated**: 2026-02-22


---

# Shurli FAQ

Frequently asked questions about Shurli, organized by topic. Each section covers a different aspect of the project - from design philosophy to technical internals.

> **Note on comparisons**: All technical comparisons in this document are based on publicly available documentation, specifications, and published benchmarks as of the date listed at the bottom. Software evolves - details may be outdated by the time you read this. If you spot an inaccuracy, corrections are welcome via [GitHub issues](https://github.com/shurlinet/shurli/issues) or pull requests.

## Sections

- **[Design Philosophy](design-philosophy.md)** - Why Shurli uses no accounts, no central servers, and no vendor dependencies. The reasoning behind self-sovereign identity and local-first design.
- **[Comparisons](comparisons.md)** - How different approaches to remote access compare: centralized VPNs, P2P mesh tools, relay architectures, and blockchain P2P stacks. Where Shurli's design sits in the landscape.
- **[Relay & NAT Traversal](relay-and-nat.md)** - How connections actually work: Circuit Relay v2, hole-punching, symmetric NAT, public vs self-hosted relays, and running your home node as a relay.
- **[Security & Features](security-and-features.md)** - How pairing, verification, reachability grading, encrypted invites, and private DHT networks work.
- **[Technical Deep Dives](technical-deep-dives.md)** - Under-the-hood details: libp2p improvements Shurli has adopted, emerging technologies to watch, and the Go vs Rust trade-off.

**Last Updated**: 2026-02-24


---

# FAQ - Design Philosophy

## Why no accounts or central servers?

Most remote access tools require you to create an account on someone else's server. That server becomes a single point of failure and a single point of control. If the company changes pricing, shuts down, gets acquired, or suffers a breach, your access breaks and your connection metadata is exposed.

Shurli eliminates that dependency entirely. Identity is an Ed25519 key pair generated locally on your machine. Authorization is a plain-text file listing which peer IDs are allowed to connect - the same model as SSH's `authorized_keys`. Configuration is one YAML file you can read, edit, back up, and version-control yourself.

There is no registration step, no authentication token issued by a third party, and no renewal that can lapse. Two machines can find and authenticate each other using only their local keys and a shared relay for the initial connection. If you want full independence, run your own relay on any VPS - the entire system operates with zero external accounts.

The result: your ability to reach your own machines depends on your infrastructure, not someone else's business decisions.

---

## Why Shurli avoids centralized identity

When a remote access tool requires you to sign in with an identity provider, that provider becomes a silent dependency in your infrastructure. If the provider has an outage, you cannot authenticate. If they change their terms, your access is conditional on compliance. If they are breached, your device graph, connection history, and metadata are exposed alongside every other user on the platform.

Shurli sidesteps this entirely. Identity is an Ed25519 key pair generated on your machine during `shurli init`. It never leaves your device, is never uploaded, and is never registered with any external service. Authentication happens directly between peers: each side checks the other's public key against a local `authorized_keys` file. No OAuth flow, no email verification, no session tokens issued by a third party.

The practical result: your identity cannot be suspended, revoked, or invalidated by anyone other than you. If every server on the internet went offline except your two machines and a relay, Shurli would still authenticate and connect them.

---

## Why Shurli rejects vendor lock-in

Remote access platforms typically control three things: the relay infrastructure your traffic passes through, the identity system that proves who you are, and the transport protocol that carries your data. When a single vendor controls all three, switching costs compound. Your device configurations point to their relays, your identity exists only in their database, and your client software speaks a proprietary protocol that nothing else understands. If the vendor raises prices, changes terms, or shuts down, you rebuild from scratch.

Shurli decouples all three layers. Relays are standard libp2p circuit relay v2 nodes - run your own on any VPS, or use someone else's, or run none if your peers have direct reachability. Identity is a local Ed25519 key pair with no external registration. Transport is QUIC and TCP through the libp2p stack, an open protocol with multiple independent implementations across languages. Nothing in the system is proprietary, vendor-specific, or non-replaceable.

The design consequence: every component can be swapped, self-hosted, or eliminated without losing access to your network. There is no migration path to plan because there is nothing to migrate away from.

---

**Last Updated**: 2026-02-24


---

# FAQ - Comparisons

> **Note on comparisons**: All technical comparisons in this document are based on publicly available documentation, specifications, and published benchmarks as of the date listed at the bottom. Software evolves - details may be outdated by the time you read this. If you spot an inaccuracy, corrections are welcome via [GitHub issues](https://github.com/shurlinet/shurli/issues) or pull requests.

## How does Shurli differ from centralized VPN tools?

The core architectural difference comes down to how coordination works.

Centralized VPN tools use a coordination server controlled by the vendor. Your devices register with that server, authenticate through it, and rely on it to broker connections. Your identity lives in their database, your device graph is visible to their infrastructure, and your ability to connect depends on their service being online and their terms remaining acceptable.

Shurli takes the opposite approach: fully decentralized coordination via a Kademlia DHT and local configuration files. Identity is an Ed25519 key pair generated on your machine. Authorization is a local `authorized_keys` file listing which peer IDs are allowed. There is no account, no sign-in, no external dependency for authentication or discovery.

### Architecture

| Aspect | **Shurli** | **Centralized VPN** |
|--------|------------|---------------------------------------|
| **Foundation** | libp2p (circuit relay v2, DHT, QUIC) | WireGuard (kernel-level crypto) |
| **Topology** | Client -> Relay -> Server (with DCUtR upgrade to direct) | Full mesh, point-to-point |
| **NAT Traversal** | Circuit relay + hole-punching (DCUtR) | DERP relay servers + STUN/hole-punching |
| **Encryption** | libp2p Noise protocol (Ed25519) | WireGuard (Curve25519) |
| **Control Plane** | None - fully decentralized (DHT + config files) | Centralized coordination server |

### Privacy & Sovereignty

| | **Shurli** | **Centralized VPN** |
|---|---|---|
| **Accounts** | None - no email, no OAuth | Required (Google, GitHub, etc.) |
| **Telemetry** | Zero - no data leaves your network | Coordination server sees device graph |
| **Control plane** | None - relay only forwards bytes | Centralized coordination server |
| **Key custody** | You generate, you store, you control | Keys managed via their control plane |
| **Source** | Fully open, self-hosted | Open source client, proprietary control plane |

### Features

| Feature | **Shurli** | **Centralized VPN** |
|---------|------------|---------------------------------------|
| **Service tunneling** | SSH, XRDP, generic TCP | Full IP-layer VPN (any protocol) |
| **Auth model** | SSH-style `authorized_keys` (peer ID allowlist) | SSO (Google, Okta, GitHub), ACLs |
| **DNS** | Friendly names in config + private DNS on relay (planned) | MagicDNS (auto device names) |
| **Platforms** | Linux, macOS (Go binary) | Linux, Windows, macOS, iOS, Android, containers |
| **Setup** | `shurli init` wizard | Download -> sign in -> done |
| **Admin UI** | CLI only | Web dashboard, admin console |
| **Exit nodes** | Not yet | Yes |
| **Subnet routing** | Not yet | Yes |
| **Multi-user/team** | Relay pairing codes + invite/join + `shurli verify` | Built-in team management, SSO |

### Strengths of the decentralized approach

- **No central authority** - No account, no coordination server, no vendor dependency
- **Importable library** - `pkg/p2pnet` can be embedded into any Go application
- **CGNAT/Starlink proven** - Relay-based architecture works through symmetric NAT
- **Self-hosted relay** - You run your own relay on a $5 VPS
- **GPU inference use case** - Purpose-built for exposing Ollama/vLLM through CGNAT

### Strengths of the centralized approach

- **IP-layer VPN** - Virtual network interface; any protocol works transparently
- **Mature ecosystem** - Mobile apps, web dashboard, ACLs, SSO, subnet routing, Funnel
- **Performance** - WireGuard is kernel-level and extremely fast
- **Scale** - Handles thousands of devices in an organization
- **Zero config** - "Install and sign in" onboarding
- **Platform coverage** - Runs everywhere including iOS, Android, containers

### Self-hosted control planes: a middle ground

Projects like [Headscale](https://github.com/juanfont/headscale) and [NetBird](https://github.com/netbirdio/netbird) offer self-hosted alternatives that eliminate the vendor dependency. You run the coordination server yourself, so there is no third-party account requirement and no external control over your network. However, the architecture still requires a coordination server - it is self-hosted rather than vendor-hosted, but not eliminated. The WireGuard transport layer remains the same, and you still manage a centralized piece of infrastructure. These sit in between: more sovereign than a vendor-hosted control plane, less decentralized than Shurli's DHT-based approach.

---

## How does Shurli differ from other P2P and mesh tools?

### Direct competitors

#### Hyprspace - Most similar in the libp2p ecosystem

- **Stack**: Go + libp2p + IPFS DHT (same as Shurli)
- **What it does**: Lightweight VPN that creates TUN interfaces, uses DHT for discovery, NAT hole-punching via libp2p
- **Key features**: Virtual IP addresses, IPv6 routing, Service Network (subdomain-based service addressing)
- **Difference**: Hyprspace operates at the IP layer (TUN/TAP VPN), not TCP service proxy. No invite/onboarding flow, no relay-first architecture.
- **Link**: https://github.com/hyprspace/hyprspace

#### connet - Similar concept, different stack

- **Stack**: Go + QUIC (not libp2p)
- **What it does**: P2P reverse proxy with NAT traversal, inspired by frp/ngrok/rathole
- **Key features**: Source + destination clients, QUIC protocol, NAT-PMP support, certificate-based auth
- **Difference**: Uses QUIC directly instead of libp2p. No DHT discovery, no friendly naming, no init wizard.
- **Link**: https://github.com/connet-dev/connet

#### SomajitDey/tunnel - Simpler alternative

- **Stack**: Bash scripts + HTTP relay (piping-server)
- **What it does**: P2P TCP/UDP port forwarding through an HTTP relay
- **Difference**: Much simpler (bash scripts), no libp2p, no DHT, no connection gating.
- **Link**: https://github.com/SomajitDey/tunnel

### Adjacent projects

#### Hyperswarm / Holepunch - DHT-assisted hole punching

- **Stack**: Node.js / C, HyperDHT, UTP + TCP
- **What it does**: P2P networking library powering [Keet](https://keet.io/) (encrypted P2P video/chat). DHT nodes actively assist with hole punching coordination.
- **Key features**: HyperDHT for discovery and relay-assisted hole punching, Noise protocol encryption, Hypercore for data replication
- **Difference**: Smaller ecosystem, fewer transports (no QUIC, no WebSocket), no anti-censorship story. Tightly coupled to the Hypercore/Dat ecosystem. Node.js-native (not Go). Hole punching may have higher success rates in some NAT scenarios because DHT nodes actively broker the handshake.
- **Link**: https://github.com/holepunchto/hyperswarm

#### Iroh - Library competitor to libp2p itself

- **Stack**: Rust, QUIC, custom relay protocol
- **What it does**: "Dial by public key" - P2P connectivity library with higher NAT traversal success rate than libp2p (~90%+ vs ~70%)
- **Difference**: A library, not an end-user tool. There's a `libp2p-iroh` transport adapter for using Iroh's NAT traversal within libp2p.
- **Link**: https://github.com/n0-computer/iroh

#### Nebula - Different stack, same goal

- **Stack**: Go, custom protocol (not WireGuard, not libp2p)
- **What it does**: P2P overlay network from Slack, full mesh with lighthouse nodes
- **Difference**: Certificate-authority model. **No relay fallback** - if hole-punching fails (e.g., CGNAT/Starlink), the connection simply doesn't work.
- **Link**: https://github.com/slackhq/nebula

#### Headscale / NetBird - Self-hosted coordination planes

- **Headscale**: Open source Tailscale control server - uses official Tailscale clients
- **NetBird**: Full self-hosted mesh with WireGuard, management service, signal server, relay
- **Difference**: Both are WireGuard-based, not libp2p. Different philosophy - they replicate Tailscale's architecture with self-hosted infrastructure, Shurli builds something structurally different.

### Comparison table

| Project | Stack | Layer | Relay fallback | CGNAT works | Onboarding | Self-sovereign |
|---------|-------|-------|---------------|-------------|------------|----------------|
| **Shurli** | Go + libp2p | TCP service proxy | Yes (circuit relay v2) | Yes | `init` wizard + invite/join | Yes |
| **Hyprspace** | Go + libp2p | IP layer (TUN) | Yes (circuit relay) | Yes | Manual config | Yes |
| **Hyperswarm** | Node.js + HyperDHT | Library | Yes (DHT-assisted) | Yes | API only | Yes |
| **connet** | Go + QUIC | TCP proxy | Yes (control server) | Partial | Manual config | Yes |
| **tunnel** | Bash + HTTP | TCP/UDP proxy | Yes (HTTP relay) | Yes | CLI flags | Yes |
| **Iroh** | Rust + QUIC | Library | Yes (home relay) | Yes | API only | No (uses Iroh's relays) |
| **Nebula** | Go + custom | IP layer (TUN) | No | No | Certificate CA | Yes |
| **Tailscale** | Go + WireGuard | IP layer (TUN) | Yes (DERP) | Yes | SSO sign-in | No |
| **Headscale** | Go + WireGuard | IP layer (TUN) | Yes (DERP) | Yes | SSO sign-in | Partial (self-hosted control) |
| **NetBird** | Go + WireGuard | IP layer (TUN) | Yes | Yes | Dashboard | Partial (self-hosted control) |

### Blockchain P2P networks as reference points

These are not competitors but useful reference points. Their P2P stacks solve different problems (block propagation, consensus) but share underlying technology with Shurli:

| Network | P2P Stack | Discovery | NAT Traversal | Encryption | Key Insight |
|---------|-----------|-----------|---------------|------------|-------------|
| **Bitcoin** | Custom (TCP only) | DNS seeds + addr gossip | None | BIP 324 (added 2023 - was plaintext for 14 years) | Simplicity is strength; 17 years of adversarial hardening |
| **Ethereum (execution)** | devp2p / RLPx | discv5 (UDP) | None (public IPs expected) | ECIES | Legacy layer, pre-Merge |
| **Ethereum (consensus)** | **libp2p** (same as Shurli) | discv5 (chose over Kademlia) | Minimal | Noise protocol | Validates libp2p for critical infrastructure |
| **Filecoin** | libp2p | Kademlia DHT | Circuit relay | Noise / TLS 1.3 | Largest libp2p deployment by data volume |
| **Polkadot** | libp2p (Rust) | Kademlia DHT | Circuit relay | Noise | Multi-chain P2P; validates rust-libp2p |

---

## How do relay architectures compare?

Three broad approaches to relay design exist in the P2P networking space: self-hosted relays where you control the infrastructure, vendor-operated relays where the service provider runs them, and hybrid approaches that blend elements of both.

### Hole-punching success (when no relay is needed)

| Protocol | NAT traversal success | Technique |
|----------|----------------------|-----------|
| **Circuit Relay v2 + DCUtR** (self-hosted) | ~70% | STUN-like, coordinate via relay, single punch attempt |
| **Iroh** (hybrid) | ~90%+ | Tailscale-inspired, aggressive probing, multiple strategies |
| **Tailscale DERP + STUN** (vendor-operated) | ~92-94% | Most mature, years of iteration, birthday attack techniques |
| **WireGuard alone** | ~0% behind CGNAT | No relay, no hole-punching |
| **Nebula** | ~60-70% | Lighthouse-based, no relay fallback |

**Important**: With Starlink CGNAT (symmetric NAT), hole-punching success is **0% for all of them**. Every single one falls back to relay. The hole-punch success rates only matter for regular NAT (home routers, etc.).

### Relay quality (when traffic stays on relay)

| | **Circuit Relay v2 (self-hosted)** | **Iroh relay (hybrid)** | **Tailscale DERP (vendor-operated)** |
|---|---|---|---|
| **Throughput** | Your VPS bandwidth | Iroh's servers | Tailscale's servers |
| **Latency** | Your VPS location | Nearest Iroh relay | Nearest DERP node |
| **Protocol overhead** | Minimal (libp2p framing) | Minimal (UDP-over-HTTP) | Minimal (DERP framing) |
| **Encryption** | Noise protocol (libp2p) | QUIC TLS | WireGuard (ChaCha20) |
| **You control limits** | Yes - unlimited duration/data | No | No |
| **Relay sees content** | No (end-to-end encrypted) | No (end-to-end encrypted) | No (end-to-end encrypted) |

All three are roughly equivalent in relay quality. The relay is a dumb pipe forwarding encrypted bytes. Performance depends on infrastructure, not protocol.

### Connection establishment speed

| Protocol | Time to first byte | Why |
|----------|-------------------|-----|
| **Circuit Relay v2** (self-hosted) | 5-15 seconds | Connect -> reserve -> DHT lookup -> peer connects -> DCUtR attempt |
| **Iroh** (hybrid) | 1-3 seconds | Persistent relay connection, peer dials by key, relay forwards immediately |
| **Tailscale DERP** (vendor-operated) | <1 second | Always-on DERP connection, peer dials by WireGuard key |

Circuit Relay v2 is slower because it involves a reservation step and DHT lookup. Iroh and Tailscale maintain persistent relay connections.

---

## How do P2P networking stacks compare?

These comparisons are reference points showing where Shurli's libp2p foundation sits in the broader P2P landscape. Bitcoin and Ethereum are not competitors - they solve different problems (block propagation, consensus) - but their P2P stacks share enough architectural overlap to be instructive.

### Bitcoin's P2P stack

Bitcoin's P2P protocol has **less overhead per message**, but it cannot do what Shurli needs.

#### The comparison

| | **Bitcoin P2P** | **libp2p (Shurli)** |
|---|---|---|
| **Transport** | Raw TCP only | TCP, QUIC, WebSocket, WebRTC |
| **Handshake** | 1.5-3 RTTs (~296 bytes) | 4+ RTTs (TCP) / 3 RTTs (QUIC) |
| **Per-message overhead** | 24 bytes (fixed header) | 12 bytes (Yamux) + encryption framing |
| **Encryption** | None | TLS 1.3 or Noise (mandatory) |
| **Multiplexing** | None (1 connection = 1 stream) | Yes (many streams per connection) |
| **NAT/CGNAT traversal** | No - requires port forwarding | Yes - relay, hole punching, AutoNAT |
| **Bulk data transfer** | Fast (minimal overhead) | Comparable once connected |

#### Why Bitcoin P2P is "faster"

It is simpler, not fundamentally faster. Bitcoin uses raw TCP with a 24-byte binary header and zero encryption. No protocol negotiation, no multiplexing, no security handshake. It is lean because it *trusts nothing* at the network layer - blocks are verified cryptographically after receipt anyway.

#### Why it does not matter for Shurli

**Bitcoin P2P cannot traverse NAT or CGNAT at all.** If both sides cannot directly reach each other, Bitcoin nodes simply cannot connect inbound. Users behind ISP CGNAT cannot run full Bitcoin nodes that accept inbound connections. Bitcoin originally had UPnP enabled by default but disabled it due to [miniupnpc vulnerabilities](https://bitcoin.org/en/alert/2015-10-12-upnp-vulnerability). It now uses PCP (Port Control Protocol), which ISP CGNAT equipment intentionally blocks.

NAT/CGNAT traversal is Shurli's entire reason for existing.

#### The key research finding

A 2021 study implemented Bitcoin's block exchange protocol on top of libp2p and found:

> *"Setting up communication channels is time-consuming, but data transfers are fast"*

Once the connection is established, **bulk throughput is comparable**. The overhead is in the handshake, not the data flow. For Shurli's use case (long-lived connections proxying SSH, Ollama, XRDP), connection setup latency is a one-time cost that becomes irrelevant.

**Source**: Barbara Guidi, Andrea Michienzi, Laura Ricci. *"A libP2P Implementation of the Bitcoin Block Exchange Protocol."* Proceedings of the 2nd International Workshop on Distributed Infrastructure for Common Good (DICG '21), ACM, 2021. DOI: [10.1145/3493426.3493822](https://dl.acm.org/doi/10.1145/3493426.3493822)

#### What Shurli does to close the gap

These optimizations shipped in Phase 4C:

1. **QUIC transport** (done) - saves 1 RTT on connection setup (3 RTTs vs 4 for TCP)
2. **DCUtR hole punching** (done) - bypass relay entirely for direct peer-to-peer
3. **Parallel dial racing** (done, Batch I) - race DHT and relay in parallel, first wins
4. **STUN probing** (done, Batch I) - classify NAT type, predict hole-punch success

Once hole punching succeeds, Shurli is essentially just encrypted TCP with 12 bytes of Yamux framing per frame - very close to Bitcoin's raw TCP speed but with encryption and NAT traversal. Connection warmup and stream pooling remain as future optimizations.

#### Bottom line

Bitcoin P2P is lean but primitive. It solved a different problem: broadcasting blocks to publicly-reachable nodes. Shurli needs relay + hole punching + encryption, and libp2p is the right tool for that. The performance gap narrows dramatically with QUIC + connection pooling + DCUtR direct connections.

### Ethereum's P2P stack

Ethereum is the most relevant comparison because **its consensus layer uses the same libp2p stack** that Shurli is built on. Ethereum actually runs two separate P2P networks.

#### Ethereum's two P2P layers

**Execution layer (devp2p/RLPx)** - the original Ethereum networking, predating The Merge:

| | **devp2p (Execution)** | **Shurli (libp2p)** |
|---|---|---|
| **Transport** | TCP only | QUIC + TCP + WebSocket |
| **Encryption** | ECIES (ECDH + AES) | Noise / TLS 1.3 |
| **Multiplexing** | Capability-based sub-protocols (eth, snap) | Yamux (any number of streams) |
| **Discovery** | discv5 (UDP-based DHT) | Kademlia DHT |
| **NAT traversal** | None - validators expected to have public IPs | AutoNAT v2 + circuit relay + DCUtR hole punching |
| **Identity** | ENR (Ethereum Node Records) | PeerID (Ed25519 multihash) |

**Consensus layer (libp2p)** - adopted for the Beacon Chain (post-Merge):

| | **Ethereum Consensus** | **Shurli** |
|---|---|---|
| **Stack** | libp2p (Go and Rust implementations) | libp2p (Go) |
| **libp2p version** | ~v0.30.x era | v0.47.0 (newer) |
| **Transports** | TCP primarily | QUIC -> TCP -> WebSocket |
| **Primary pattern** | gossipsub (topic-based pub/sub for blocks/attestations) | Point-to-point streams (service proxy) |
| **Discovery** | discv5 (custom, not libp2p Kademlia) | Kademlia DHT + relay bootstrap |
| **NAT traversal** | Minimal (validators run on servers) | Full: AutoNAT v2 + relay + hole punch |
| **Encryption** | Noise protocol | Noise / TLS 1.3 |

#### Why Ethereum chose libp2p for consensus

When Ethereum needed a P2P networking stack for the Beacon Chain - the system securing hundreds of billions of dollars - they evaluated their options and chose libp2p. The reasons:

1. **Modularity** - swap transports, security, multiplexers independently
2. **Multi-language support** - Go (Prysm), Rust (Lighthouse), Java (Teku), .NET (Nethermind) all have libp2p implementations
3. **Stream multiplexing** - essential for gossipsub topic subscriptions
4. **Noise protocol** - mutual authentication during handshake

#### Why Ethereum chose discv5 over libp2p's Kademlia for discovery

Ethereum's consensus layer uses libp2p for transport and encryption but **not** for peer discovery. They built discv5 instead:

| | **libp2p Kademlia DHT** | **Ethereum discv5** |
|---|---|---|
| **Protocol** | TCP-based | UDP-based |
| **Bandwidth** | Higher (DHT maintenance traffic) | Lower (lightweight probes) |
| **Topic advertisement** | Not built-in | Native topic-based discovery |
| **NAT handling** | Relies on relay/AutoNAT | Built-in PING/PONG with endpoint proof |
| **Purpose** | General content/peer routing | Pure peer discovery (minimal scope) |

The key reason: Kademlia DHT maintains routing tables and handles both content routing and peer discovery, which generates more background traffic than needed for pure discovery. discv5 does one thing - find peers - and does it with less bandwidth overhead.

**For Shurli**: Kademlia DHT is the right choice today because Shurli uses it for both peer discovery and rendezvous coordination, and the bandwidth overhead is negligible at current network sizes. The discv5 approach becomes interesting at larger scales where DHT maintenance traffic is measurable.

#### What this means for Shurli

Shurli's libp2p foundation is **validated by Ethereum's consensus layer** - the same networking stack secures one of the largest decentralized networks in existence. Shurli also benefits from improvements driven by Ethereum's scale: gossipsub optimizations, Noise protocol hardening, and transport upgrades all flow back to the shared libp2p codebase.

Where Shurli goes further than Ethereum's usage:
- **Full NAT traversal** (AutoNAT v2, circuit relay, DCUtR) - Ethereum validators do not need this
- **QUIC as preferred transport** - Ethereum consensus still primarily uses TCP
- **WebSocket for anti-censorship** - Ethereum has no DPI evasion story
- **Point-to-point service proxy** - different use pattern than gossipsub broadcast

---

## What does Shurli ship that others don't?

### Built-in observability

Most P2P tunnel tools ship with no metrics, no traces, and no structured audit logs. DevOps teams bolt on monitoring after the fact, poorly.

Shurli ships with Prometheus metrics (libp2p built-in + custom proxy/auth/holepunch counters), structured audit logging, and a pre-built Grafana dashboard with 23 visualization panels out of the box. No other P2P tunnel tool ships with this level of built-in observability.

**What's next**: Distributed tracing (deferred - 35% CPU overhead not justified yet). OTLP export via Prometheus bridge when users request it.

---

## What are the open problems in P2P networking?

These are genuine gaps in every P2P/VPN/tunnel tool available today, including Shurli:

### 1. Zero-RTT proxy connection resume

When your network flickers (WiFi to cellular, WiFi dropout), every existing tool drops connections and requires a full reconnection handshake. QUIC 0-RTT session tickets could make reconnection instant - send encrypted data before the server processes the handshake.

**Who has it**: Nobody in the P2P tunnel space.
**Difficulty**: Medium (requires QUIC transport + session ticket caching).

### 2. Hardware-backed peer identity

No P2P tool stores peer private keys in TPM 2.0 (Linux servers) or Secure Enclave (macOS/iOS). Keys sit on disk, stealable by anyone with filesystem access.

**Who has it**: Nobody.
**Difficulty**: Medium (platform-specific APIs: `go-tpm`, `Security.framework`).

### 3. Kernel-bypass relay forwarding

Every relay server processes packets through the kernel network stack (syscalls per packet). eBPF/XDP or DPDK could forward relayed packets at line rate - benchmarks show [DPDK achieves 51% better throughput](https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/) than kernel stack, VPP uses 1/9th the CPUs.

**Who has it**: Nobody (Cloudflare uses XDP for DDoS, not for relay forwarding).
**Difficulty**: High (Linux-only, requires privileged access).

### 4. Formally verified protocol state machine

No P2P tool has mathematically proven that its handshake / invite / key exchange protocol is correct. Bugs in state machines cause security vulnerabilities. Formal verification tools like [Kani](https://github.com/model-checking/kani) (Rust) and [TLA+](https://lamport.azurewebsites.net/tla/tla.html) can prove correctness.

**Who has it**: AWS s2n-quic (QUIC only, not application layer). [Bert13](https://dl.acm.org/doi/10.1145/3719027.3765213) (first formally-verified post-quantum TLS 1.3 in Rust).
**Difficulty**: High (requires Rust migration for Kani, or TLA+ model of invite protocol).

### 5. Cryptographic agility (post-quantum ready)

No P2P tool supports cipher suite negotiation or hybrid classical + post-quantum handshakes. When ML-KEM mandates arrive (2026-2028), every tool will need emergency patches.

**Who has it**: Nobody in P2P. AWS and Microsoft are preparing at the infrastructure layer.
**Difficulty**: Medium (design cipher negotiation now, implement when libp2p adopts PQC).

---

**Last Updated**: 2026-02-24


---

# FAQ - Relay & NAT Traversal

## What is Circuit Relay v2?

Circuit Relay v2 is libp2p's protocol for routing traffic through an intermediary relay node when peers can't connect directly (NAT, CGNAT, firewalls). It replaced v1 in 2021.

### How it works

![Circuit Relay v2 sequence: Peer A reserves slot on Relay, Peer B connects through Relay, streams bridged bidirectionally](../images/faq-circuit-relay-sequence.svg)

The protocol splits into two sub-protocols:
- **Hop** (`/libp2p/circuit/relay/0.2.0/hop`) - client <-> relay (reserve, connect)
- **Stop** (`/libp2p/circuit/relay/0.2.0/stop`) - relay <-> target peer (deliver connection)

### Why v1 was replaced

v1 had no resource reservation - relays got overloaded with no way to limit usage. v2 introduced explicit reservations with configurable limits (duration, data caps, bandwidth), making it cheap to run "an army of relays for extreme horizontal scaling." Relays can reject connections with status codes like `RESOURCE_LIMIT_EXCEEDED` or `RESERVATION_REFUSED`.

### Known limitations

| Limitation | Detail |
|-----------|--------|
| **Setup latency** | 5-15 seconds (reservation + handshake + DHT lookup) |
| **No persistent connections** | Connections have hard TTL; each dial requires new reservation |
| **Reservation overhead** | Every peer must explicitly reserve before receiving relayed connections |
| **Throughput asymmetry** | Limited by relay's aggregate bandwidth, not peer bandwidth |
| **Default public limits** | 128 KB data cap, 2-minute duration (configurable on self-hosted) |

### Is there a Circuit Relay v3?

**No.** No v3 exists or is planned. libp2p's strategy is to reduce *dependence* on relays through better hole punching ([DCUtR](https://github.com/libp2p/specs/blob/master/relay/DCUtR.md) improvements, [AutoNAT v2](https://github.com/libp2p/specs/blob/master/autonat/autonat-v2.md)), not to replace the relay protocol itself.

The improvements come from upgrading everything *around* the relay - see the next FAQ entry.

**Source**: [Circuit Relay v2 Specification](https://github.com/libp2p/specs/blob/master/relay/circuit-v2.md)

---

## Why Circuit Relay v2 is the right choice for Shurli

1. **Symmetric NAT** - Hole-punch success rates are irrelevant (all protocols fail against symmetric NAT, all fall back to relay)
2. **Self-hosted relay** - You control limits, so the 128KB/2min public relay caps don't apply
3. **No vendor dependency** - Matches the self-sovereign philosophy
4. **Native to libp2p** - No additional dependencies in the Go codebase
5. **Battle-tested** - Millions of IPFS nodes use it daily
6. **Configurable** - When you run your own relay, you set your own resource limits

The only area where alternatives genuinely outperform Circuit Relay v2:
- **Connection speed**: Iroh (1-3s) and Tailscale (<1s) are faster than Circuit Relay v2 (5-15s) due to persistent relay connections
- **Hole-punch success for regular NAT**: Iroh (~90%) and Tailscale (~92%) beat DCUtR (~70%) - but this doesn't matter for symmetric NAT

For Starlink CGNAT with a self-hosted relay, Circuit Relay v2 is **functionally equivalent** to Iroh and Tailscale in relay quality.

---

## Can I use public relay servers instead of my own?

Yes, public IPFS relays exist - thousands of them. Since Circuit Relay v2, every public IPFS node runs a relay by default. libp2p's AutoRelay can discover and use them automatically.

**But there's a catch.** Public relays have strict resource limits:

| Constraint | Public IPFS relay (v2 defaults) | Your self-hosted relay |
|-----------|-------------------------------|------------|
| **Duration** | 2 minutes per connection | Unlimited (you configure) |
| **Data cap** | 128 KB per relay session | Unlimited (you configure) |
| **Bandwidth** | ~1 Kbps (intentionally throttled) | Your VPS bandwidth |
| **Purpose** | Coordinate hole-punch, then disconnect | Full traffic relay |
| **Uptime** | Random node, could disappear | Your VPS, 99.9% uptime |
| **SSH session** | Drops after 2 min or 128 KB | Works indefinitely |

Public relays are designed as a **trampoline** - they help two peers find each other, attempt a hole-punch, and then drop off. They were never meant for sustained traffic like SSH sessions, XRDP, or LLM inference.

---

## How do public relays differ from self-hosted relays?

Conceptually yes - both are "someone else's relay you use for free." But the implementation differs significantly:

| | **IPFS public relays** | **Iroh's relays** |
|---|---|---|
| **Operator** | Thousands of random IPFS peers | n0 team (Iroh's company) |
| **Architecture** | Decentralized - any public node can be a relay | Centralized - Iroh runs them |
| **Data limit** | 128 KB per session | No hard cap |
| **Time limit** | 2 minutes | Persistent connection |
| **Purpose** | Trampoline for hole-punch coordination | Actual traffic fallback (like Tailscale's DERP) |
| **Reliability** | Random node could vanish anytime | Operated infrastructure |
| **Protocol** | libp2p Circuit Relay v2 | Custom protocol (UDP-over-HTTP) |

Iroh's relays are essentially **Tailscale's DERP servers for the Iroh ecosystem** - meant to carry real traffic when hole-punching fails. IPFS's public relays are just for the initial handshake.

---

## Why does Shurli use its own relay?

For Starlink/CGNAT (symmetric NAT) users, hole-punching **always fails**. Traffic must stay on the relay for the entire session. This means:

1. **Public IPFS relays** - Connection drops after 2 minutes or 128 KB. Unusable.
2. **Iroh's relays** - Would work, but you depend on Iroh's infrastructure and lose sovereignty.
3. **Tailscale's DERP** - Would work, but requires a Tailscale account and their control plane.
4. **Your own relay** - Works indefinitely, unlimited data, you control everything.

Shurli's self-hosted relay ($5/month VPS) is the only option that provides **both** unlimited traffic **and** full sovereignty.

---

## Why does symmetric NAT break hole-punching?

Nebula uses **lighthouse nodes** (like STUN servers) to help peers discover each other's public IP:port. Then it attempts direct hole-punching.

With symmetric NAT (CGNAT), the mapped port **changes for every destination**:

![Why symmetric NAT breaks hole-punching: different mapped port per destination causes port mismatch](../images/faq-symmetric-nat.svg)

The port the lighthouse tells Peer B to use was allocated for the lighthouse connection, not Peer B. The hole-punch fails.

**Nebula has no relay fallback.** If hole-punching fails, the connection simply doesn't work. Tailscale falls back to DERP. Shurli falls back to circuit relay. Nebula has nothing.

---

## Is it safe for my home node to act as a relay?

This is the most important security question for Shurli's future. The short answer: **yes, with Circuit Relay v2's built-in protections, a home node can safely relay traffic for authorized peers without increasing its attack surface.**

Here's the full breakdown - because "trust us" is not a security argument.

![Home relay security layers: unauthenticated peers blocked at ConnectionGater (Layer 1), authorized peers pass through resource limits (Layer 2) and E2E encryption (Layer 3)](../images/faq-relay-security-layers.svg)

### What "acting as a relay" actually means

When your home node enables relay service, it does one thing: accept a reservation from a peer (identified by their peer ID), then forward encrypted bytes between that peer and whoever connects to them through you. Your node never sees the content - it's end-to-end encrypted with Noise protocol. Your node never authenticates the remote peer's connections - that's the target peer's job.

Think of it as holding two tin cans connected by a string. You're the string. You can feel vibrations but not hear words.

### Resource limits are enforced by the protocol, not by trust

Circuit Relay v2 was specifically redesigned (from v1) because v1 relays had no resource controls and got overwhelmed. v2 has per-reservation resource budgets baked into the protocol:

| Protection | What it prevents | Default |
|-----------|-----------------|---------|
| **Max reservations** | Total peers using your relay | Configurable (128 default) |
| **Max circuits per peer** | One peer consuming all relay capacity | 16 |
| **Max reservations per IP** | IP address hoarding reservations | 8 |
| **Max reservations per ASN** | One ISP's network flooding your relay | 32 |
| **Reservation TTL** | Stale reservations consuming resources | 1 hour |
| **Session duration** | Indefinite relay connections | Configurable (10 min default) |
| **Session data limit** | Bandwidth theft | Configurable (64 MB default) |

When any limit is hit, the relay returns `RESOURCE_LIMIT_EXCEEDED` and the connection is refused. No crash, no OOM, no degradation. The peer simply can't connect.

### Only relay for people you chose

Shurli's relay service is **restricted to peers in your `authorized_keys` file** by default. The ConnectionGater blocks unauthenticated connections before any relay protocol runs - an anonymous internet scanner hitting your relay's port gets rejected at the connection layer, and the reservation request never reaches the relay service logic.

Batch I-f shipped "every-peer-is-a-relay" - any peer with a detected global IP auto-enables circuit relay v2 with conservative resource limits (4 reservations, 16 circuits, 128KB per direction, 10-minute sessions). The ConnectionGater ensures only authorized peers can make reservations or create circuits through this relay.

Relay management uses the relay admin socket (Unix domain socket with cookie auth). Admins generate pairing codes via `shurli relay pair`, which talks to the running relay process directly. No SSH needed for peer onboarding, no manual peer ID exchange.

The attack surface increase from enabling relay is **zero** for unauthenticated peers - they are rejected at the same layer they would be rejected at today.

### "But my IP address becomes visible"

This concern has two parts:

**Part 1: Visible to peers you explicitly authorized.** Yes - when a peer connects directly (via hole punch or IPv6), they see your IP. But you already authorized them in `authorized_keys`. They already know where you are conceptually. And your IP is visible in any direct TCP/QUIC connection regardless of whether relay service is enabled.

**Part 2: Visible on the public DHT.** No - Shurli uses a **private Kademlia DHT** (`/shurli/kad/1.0.0`), completely isolated from the public IPFS Amino DHT. Your node only talks to other Shurli nodes for discovery, not the broader IPFS network. Your addresses are only discoverable by peers running Shurli software. Additional mitigations:
- **Relay-only advertising**: Advertise only your relay VPS address; your home IP only visible after authentication
- **IPv6 privacy extensions**: Use temporary IPv6 addresses that rotate

The relay VPS model today already exposes its public IP. A home relay with the ConnectionGater is no more exposed than the VPS - and arguably less, since only authorized peers can connect.

### What your relay CANNOT be used for

| Attack | Why it fails |
|--------|-------------|
| **Traffic sniffing** | End-to-end Noise encryption - relay sees ciphertext only |
| **Connection injection** | Both peers authenticate each other via peer ID (Ed25519) |
| **DDoS amplification** | QUIC source address verification; per-IP reservation limits |
| **Resource exhaustion** | Hard limits on reservations, circuits, duration, data |
| **Open relay abuse** | ConnectionGater allowlist - only authorized peers can reserve |
| **Pivot to your LAN** | Relay forwards bytes, doesn't parse them. No routing to your network. |

### Why this is different from running an open service

Self-hosters know the pattern: expose a service on a home server, and within hours the port scanners find it. Firewall logs light up with probes from around the world. This happens because the service accepts connections from anyone - every participant on the network can reach it.

A Shurli relay with its ConnectionGater allowlist is fundamentally different:

| | **Typical exposed service** | **Shurli relay** |
|---|---|---|
| **Who can connect** | Anyone who finds the port | Only peers in your `authorized_keys` |
| **What they can do** | Full protocol interaction | Forward encrypted bytes (nothing else) |
| **Discovery** | Port scanning, Shodan, service-specific gossip | Private - only authorized peers know about it |
| **Attack surface** | Full protocol parser (HTTP, SSH, etc.) | ConnectionGater rejection (zero protocol parsing for unauthorized) |

An open service is an open door with a bouncer inside. A Shurli relay is a door that only opens with the right key - and even then, it only passes sealed envelopes.

### Comparison with other relay architectures

| Feature | **Shurli** | **Tailscale DERP** | **IPFS public relay** |
|---------|------------|-------------------|---------------------|
| **Who can use it** | Explicit allowlist (authorized_keys) | Any Tailscale account holder | Anyone |
| **Authentication** | ConnectionGater (peer ID) | WireGuard keys | None |
| **Resource limits** | Per-peer, per-IP, per-ASN | Not published | 128 KB / 2 min |
| **E2E encryption** | Noise (Ed25519) | WireGuard (Curve25519) | Noise |
| **Relay sees content** | No | No | No |
| **Self-hosted** | Yes | No (Tailscale operates) | N/A (public nodes) |

### The path forward

![Decentralization path: from single VPS relay to peer relays to DHT discovery to fully distributed where every public peer relays for authorized peers](../images/faq-decentralization-path.svg)

The ConnectionGater-protected relay model is how Shurli eliminates dependency on the central relay VPS:

1. **Today**: One relay VPS + every-peer-is-a-relay (Batch I-f) for peers with public IPs
2. **Near-term**: Home nodes with public IPv6 or port-forwarding serve as additional relays -> multiple relays across the network
3. **Medium-term**: Peers discover authorized relays via DHT (not a central endpoint) -> no single point of failure
4. **End state**: Every publicly-reachable Shurli node relays for its authorized peers -> relay VPS becomes **obsolete** (not just optional)

This follows the same decentralization path as Bitcoin: hardcoded seeds -> DNS seeds -> peer exchange -> fully self-sustaining network.

**Sources**:
- [Circuit Relay v2 Specification](https://github.com/libp2p/specs/blob/master/relay/circuit-v2.md)
- [IPFS Hole Punching Blog - Relay as Last Resort](https://blog.ipfs.tech/2022-01-20-libp2p-hole-punching/)
- [libp2p Resource Manager](https://github.com/libp2p/go-libp2p/tree/master/p2p/host/resource-manager)

---

## Can NAT traversal improve without changes to Shurli?

Yes. NAT traversal success depends heavily on what the NAT device does, and router/OS vendors are starting to make NATs friendlier.

### FreeBSD PF: Endpoint-Independent Mapping (Sep 2024)

FreeBSD's packet filter (PF) now has an `endpoint-independent` NAT option for UDP. This makes the NAT behave as "full cone" - the mapped port stays the same regardless of destination. Full-cone NATs have near-100% hole-punch success because both peers can predict each other's mapped ports.

**Why this matters**: OPNsense (a popular firewall/router OS) is FreeBSD-based. If OPNsense adopts this option, a significant number of home and SMB routers get friendlier NAT behavior - and Shurli's DCUtR success rate improves automatically without any code changes.

**What to watch**: OPNsense releases, pfSense updates, and any Linux `nftables` equivalent. If this pattern spreads to consumer routers, the percentage of "hard NAT" cases (endpoint-dependent mapping) shrinks organically.

**Source**: [FreeBSD Status Report - Endpoint-Independent Mapping NAT](https://www.freebsd.org/status/report-2024-07-2024-09/eim-nat/)

### IPv6 eliminates NAT entirely

Many ISPs now provide globally routable public IPv6 addresses. IPv6 has no NAT - every device gets a public address. When two peers both have IPv6, they connect directly with zero NAT traversal, zero hole punching, zero relay dependency.

Shurli already supports IPv6 through libp2p's transport layer. AutoNAT v2 tests IPv4 and IPv6 reachability independently, so a node behind IPv4 CGNAT but with public IPv6 will correctly identify that its IPv6 addresses are directly reachable.

**Expected impact**: As IPv6 adoption grows (currently ~45% globally, higher on mobile networks), the percentage of connections requiring relay will decrease. For networks where both peers have IPv6, relay is already unnecessary today.

---

## How does CGNAT detection work?

CGNAT (Carrier-Grade NAT) is a second layer of NAT applied by mobile carriers and some ISPs. It makes hole-punching unreliable because the outer NAT drops unsolicited inbound packets regardless of inner NAT type.

Shurli detects CGNAT by checking local interfaces for RFC 6598 addresses (`100.64.0.0/10`). When found, the reachability grade is capped at D regardless of STUN results. This prevents false optimism: STUN sees only the inner NAT and might report "hole-punchable" when the outer CGNAT will actually block it.

| Detection method | What it catches | What it misses |
|-----------------|-----------------|----------------|
| RFC 6598 (`100.64.0.0/10`) on local interface | Standard CGNAT deployments | Mobile carriers using RFC 1918 (`172.x.x.x`) for CGNAT |
| STUN external IP differs from local IP | Any NAT layer | Can't distinguish CGNAT from regular NAT |

The limitation: mobile carriers that assign RFC 1918 addresses (like `172.20.x.x`) for CGNAT look identical to regular home networks. There's no clean way to detect this without active probing against a known endpoint. In testing, a 5G carrier using `172.20.10.x` was graded as "hole-punchable" even though most hole-punch attempts failed.

The grade is honest about what it knows and what it doesn't. When CGNAT is detected, the grade says so. When it can't be detected, the grade reflects STUN results and the user sees the outcome in their actual connection path (direct vs relayed).

---

**Last Updated**: 2026-02-25


---

# FAQ - Security & Features

## How does the encrypted invite handshake work?

The invite/join flow uses a PAKE-inspired encrypted handshake (shipped in Pre-Batch I-b). The invite code itself is a shared secret that never travels over the wire.

![PAKE handshake sequence: both sides generate X25519 keys, exchange public keys, derive AEAD key from DH secret + invite code, exchange encrypted identities, confirm with MACs](../images/faq-pake-handshake.svg)

1. Both sides generate ephemeral X25519 key pairs
2. They exchange public keys over the relay-mediated stream
3. Each side derives a shared AEAD key using HKDF-SHA256 with the DH shared secret and invite code combined as input keying material (info: "shurli-invite-v1")
4. All subsequent messages (peer names, peer IDs) are encrypted with XChaCha20-Poly1305
5. Key confirmation MACs verify both sides derived the same key

The relay sees only ephemeral public keys and encrypted bytes. It cannot learn the invite code, peer names, or any protocol details. If the invite codes don't match, AEAD decryption fails silently with no information leaked.

The original v1 cleartext invite protocol has been deleted. There is zero downgrade surface.

---

## How does relay pairing work?

Relay pairing is the streamlined way to onboard multiple peers to a relay. Instead of SSH-ing into the relay and manually exchanging peer IDs, the relay admin generates pairing codes and shares them.

![Relay pairing flow: admin generates codes, shares out-of-band, peers join relay with codes, relay validates tokens and distributes peer lists for mutual authorization](../images/faq-relay-pairing.svg)

**Relay admin:**
```bash
shurli relay pair --count 3 --ttl 1h
# Generates 3 pairing codes, each valid for 1 hour
```

**Each person joining:**
```bash
shurli join <pairing-code> --name laptop
# Connects to relay, discovers other peers, mutually authorizes everyone
```

The relay-side flow covers: token validation, enrollment mode (probationary peer admission), HMAC group commitment, peer authorization, group attribute annotation, peer discovery, and response encoding with the joined peer list.

Security properties:
- Pairing codes are hashed (SHA-256) on the relay. The relay stores the hash, not the code.
- Max 3 failed attempts per code group before all codes in the group burn.
- Probationary peers (max 10, 15s timeout) are evicted if pairing doesn't complete.
- All failure modes return a uniform "pairing failed" error (no oracle attacks).

**Comparison to invite/join**: Invite/join (`shurli invite` + `shurli join`) pairs exactly 2 peers directly with PAKE-encrypted key exchange. Relay pairing (`shurli relay pair`) onboards multiple peers to a relay-managed group. Use invite/join for peer-to-peer, relay pairing for groups.

---

## How does peer verification work?

After pairing, peers show an `[UNVERIFIED]` badge in ping, traceroute, and status output. This means the cryptographic identity hasn't been verified out-of-band.

**To verify a peer:**
```bash
shurli verify home
# Shows a 4-emoji + 6-digit numeric fingerprint
# Both sides must see the same fingerprint
# Confirm via a separate channel (phone call, in person, messaging app)
```

The fingerprint is computed from a sorted hash of both peer IDs (OMEMO-style). Once confirmed, the peer's `authorized_keys` entry gets a `verified=sha256:<prefix>` attribute and the badge changes to `[VERIFIED]`.

This is the same trust model as Signal's safety numbers or WhatsApp's security code. The emoji format makes it easy to compare verbally ("moon rocket house cat" is faster and less error-prone than reading hex digits).

**When verification matters**: If you paired via a relay code and want to confirm no MITM occurred during the relay-mediated exchange. If you paired in person or via a trusted channel, the pairing itself provides the verification.

---

## What are reachability grades?

The daemon status API (`shurli daemon status`) shows a reachability grade from A to F:

| Grade | Meaning | What it means for connections |
|-------|---------|------------------------------|
| **A** | Public IPv6 | Direct connections to anyone, no relay needed |
| **B** | Public IPv4 or hole-punchable NAT | Direct connections likely (full-cone or address-restricted NAT) |
| **C** | Port-restricted NAT | Hole-punching possible but less reliable |
| **D** | Symmetric NAT / CGNAT | Relay required for most connections |
| **F** | Offline or no connectivity | Cannot connect |

The grade is computed from interface discovery (which IPs are available) and STUN probe results (what type of NAT is in front of each interface). It updates automatically when the network changes (WiFi to cellular, cable plugged in, etc.).

The grade helps you understand *why* a connection goes through the relay instead of direct. A peer with grade D behind CGNAT connecting to a peer with grade A on public IPv6 will likely go direct via the grade-A peer's address. Two grade-D peers will relay.

**CGNAT grade capping**: STUN can report "hole-punchable" when a port-restricted NAT sits behind CGNAT, because STUN only sees the inner NAT. The outer CGNAT will still drop unsolicited inbound packets. The grade computation overrides STUN's false optimism by capping at grade D whenever CGNAT is detected on a local interface (RFC 6598 `100.64.0.0/10`). Mobile carriers using RFC 1918 addresses for CGNAT cannot be detected this way.

---

## How does peer introduction delivery work?

After relay pairing, the first peer's daemon doesn't know about later joiners. The relay solves this with `/shurli/peer-notify/1.0.0` - a push-based introduction protocol.

![Peer introduction delivery: relay validates pairing tokens, then pushes introductions with HMAC proofs to each peer's daemon for mutual authorization](../images/faq-peer-notify-flow.svg)

The wire format: version byte + group ID (32 bytes) + group size (byte) + per-peer entries. Each entry contains the peer ID, name, and a 32-byte HMAC proof.

Two triggers fire introductions:
1. **Post-pairing** - immediately after a new peer completes pairing
2. **Reconnect** - when a peer reconnects after being offline during a previous pairing event (fires on `EvtPeerIdentificationCompleted`)

The receiving daemon validates group membership, enforces group size limits, verifies HMAC proofs, and adds authorized peers via hot-reload. No restart required.

**Key principle**: The relay is the delivery medium, not the trust authority. HMAC proofs provide cryptographic verification that each introduced peer actually held a valid pairing token.

---

## What is the HMAC group commitment?

When a peer completes relay pairing, the daemon computes `HMAC-SHA256(token, groupID)` while the raw token is still in memory. This 32-byte proof is stored alongside the peer's entry in the relay's token store.

When peer-notify delivers introductions, each peer entry includes its HMAC proof. The receiving daemon can verify it against the group's token.

Why this matters: a compromised relay cannot forge valid HMAC proofs without access to the original pairing tokens (which are hashed immediately after use with SHA-256). Only SHA-256 hashes are stored on the relay. The HMAC proof is the only derivative that persists.

Wire cost: 32 bytes per peer in the introduction message. Negligible.

---

## How does the relay admin socket work?

The relay daemon exposes a Unix domain socket for management commands. This solves the problem of `relay pair` needing to create tokens in the running relay process's token store.

Architecture: same pattern as the daemon API socket.

| Property | Value |
|----------|-------|
| Transport | Unix domain socket (`<config-dir>/relay-admin.sock`) |
| Auth | 32-byte random hex cookie, `0600` permissions, rotated per restart |
| Endpoints | `POST /v1/pair`, `GET /v1/pair`, `DELETE /v1/pair/{id}` |
| Cookie comparison | `subtle.ConstantTimeCompare` (timing-safe) |
| Body limits | `MaxBytesReader` (4096 bytes) |
| Pairing count cap | Max 100 per request |

`relay pair` is a fire-and-forget HTTP client: it sends a request to the admin socket and prints the generated pairing codes. No shared file state, no race conditions.

---

## How do private DHT networks work?

By default, all Shurli nodes share one DHT with protocol prefix `/shurli/kad/1.0.0`. With private DHT networks, you set a namespace and your nodes form a completely separate DHT:

![Private DHT networks: two isolated DHT bubbles with different protocol prefixes cannot discover or communicate with each other](../images/faq-private-dht.svg)

```yaml
# config.yaml
discovery:
  network: "my-crew"
```

This changes the DHT prefix to `/shurli/my-crew/kad/1.0.0`. Nodes with different namespaces literally speak different protocols and cannot discover each other. It's not a firewall or ACL - it's protocol-level isolation.

Use cases: gaming groups, family networks, organization-internal deployments. Each private network needs its own relay (or at least one bootstrap peer).

When using invite codes (v2 format), the inviter's namespace is encoded in the code. The joiner automatically inherits the same namespace.

---

**Last Updated**: 2026-02-25


---

# FAQ - Technical Deep Dives

## What libp2p improvements has Shurli adopted?

Shurli uses go-libp2p v0.47.0. Several improvements have shipped since then that would meaningfully improve performance, security, and reliability.

### AutoNAT v2 (go-libp2p v0.41.1+)

The old AutoNAT tested "is my node reachable?" as a binary yes/no. v2 tests **individual addresses**:

| | **AutoNAT v1** | **AutoNAT v2** |
|---|---|---|
| **Tests** | Whole node reachability | Each address independently |
| **Verification** | Trust the dialer's claim | Nonce-based proof (dial-back) |
| **Amplification risk** | Yes (could be spoofed) | No (client must transfer 30-100KB first) |
| **IPv4/IPv6** | Can't distinguish | Tests each separately |

A Shurli node could know "IPv4 is behind NAT but IPv6 is public" and make smarter connection decisions.

**Source**: [AutoNAT v2 Specification](https://github.com/libp2p/specs/blob/master/autonat/autonat-v2.md)

### Smart Dialing (go-libp2p v0.28.0+)

Old behavior: dial all peer addresses in parallel, abort on first success. Wasteful and creates network churn.

New behavior: ranks addresses intelligently, prioritizes QUIC over TCP, dials sequentially with fast failover. When a peer has both relay and direct addresses, smart dialing tries the direct path first.

### Resource Manager

DAG-based resource constraints at system, protocol, and per-peer levels. This is the proper replacement for Shurli's `WithInfiniteLimits()`:

- Per-peer connection and stream limits
- Per-peer bandwidth caps
- Memory and file descriptor budgets
- Rate limiting (1 connection per 5s per IP, 16-burst default)
- Prevents one peer from exhausting all relay resources

### QUIC Source Address Verification

Validates that the peer's source IP isn't spoofed. Prevents relay from being used as a DDoS reflector. Built into go-libp2p's QUIC transport since quic-go v0.54.0.

### DCUtR Hole Punching Improvements

No v2 of DCUtR, but continuous refinement:
- RTT measurement retries on each attempt (prevents one bad measurement from ruining all retries)
- TCP hole punching now achieves "statistically indistinguishable success rates" from UDP
- Measured success: **70% ± 7.1%** across 4.4M attempts from 85K+ networks in 167 countries

**Source**: [Large Scale NAT Traversal Measurement Study](https://arxiv.org/html/2510.27500v1), [libp2p Hole Punching blog](https://blog.ipfs.tech/2022-01-20-libp2p-hole-punching/)

### What Shurli has done (Phase 4C - shipped)

| Optimization | Status |
|-------------|--------|
| **Upgraded go-libp2p** to v0.47.0 | Done |
| **Replaced `WithInfiniteLimits()`** with Resource Manager (auto-scaled limits) | Done |
| **Enabled DCUtR** in proxy command | Done (+ parallel dial racing in Batch I) |
| **Persistent relay reservation** | Done (periodic refresh in background goroutine) |
| **QUIC as default transport** | Done (3 RTTs vs 4 for TCP) |
| **Adaptive path selection** | Done (Batch I: interface discovery, STUN probing, every-peer-is-a-relay) |
| **Relay pairing codes** | Done (Post-I-1: relay admin generates codes, joiners connect in one command) |
| **SAS verification** | Done (Post-I-1: OMEMO-style 4-emoji fingerprint, persistent [UNVERIFIED] badge) |
| **Reachability grades** | Done (Post-I-1: A-F scale from interface discovery + STUN results) |
| **PAKE-secured invite** | Done (Pre-I-b: encrypted handshake, v1 cleartext deleted) |
| **Private DHT namespaces** | Done (Pre-I-c: `discovery.network` for isolated peer groups) |
| **Daemon-first commands** | Done (Post-I-1: ping/traceroute try daemon API first, fall back to standalone) |
| **Peer introduction delivery** | Done (Post-I-2: `/shurli/peer-notify/1.0.0`, relay pushes introductions with HMAC proofs) |
| **HMAC group commitment** | Done (Post-I-2: `HMAC-SHA256(token, groupID)` proves token possession) |
| **Relay admin socket** | Done (Post-I-2: Unix socket + cookie auth, `relay pair` is HTTP client) |
| **Sovereign interaction history** | Done (Post-I-2: per-peer `peer_history.json`, Welford's running average) |
| **Startup race fix** | Done (Pre-Phase 5: handlers registered before DHT bootstrap) |
| **Stale address detection** | Done (Pre-Phase 5: `[stale?]` labels after network change) |
| **systemd/launchd services** | Done (Pre-Phase 5: `shurli service install/start/stop/status`) |

These changes brought connection setup closer to 3-10 seconds via parallel dial racing, while keeping the self-sovereign architecture. Connection warmup and stream pooling remain as future optimizations.

---

## What emerging technologies could benefit Shurli?

### Protocols to watch

| Protocol | What it gives Shurli | Status (2026) | Phase |
|----------|----------------------|---------------|-------|
| **MASQUE** ([RFC 9298](https://www.ietf.org/rfc/rfc9298.html)) | HTTP/3 relay that looks like HTTPS to deep packet inspection. 0-RTT session resumption for instant reconnection after network switch. | Production (Cloudflare deploys across 330+ datacenters) | Future |
| **Post-quantum Noise** (ML-KEM / FIPS 203) | Quantum-resistant handshakes. Regulatory mandates expected 2026-2028. | AWS KMS, Windows 11 shipping ML-KEM. libp2p not yet adopted. | Future |
| **QUIC v2** ([RFC 9369](https://datatracker.ietf.org/doc/rfc9369/)) | Anti-ossification - randomized version field prevents middleboxes from special-casing QUIC v1. | Finalized | 4C |
| **WebTransport** | Browser-native QUIC transport (replaces WebSocket for anti-censorship). Lower overhead, native datagrams. | Chrome/Firefox production, Safari flag-only | Future |
| **W3C DID v1.1** | Decentralized Identifiers - peer IDs in a standard, interoperable format (`did:key`, `did:peer`). | [First Public Draft 2025](https://www.w3.org/TR/did-1.1/) | Future |
| **eBPF / XDP** | Kernel-bypass packet filtering at millions of packets/sec. DDoS mitigation without userspace overhead. | Production (Cloudflare, Meta, Netflix) | 4C/Future |

### MASQUE: The next-generation relay transport

[MASQUE](https://www.ietf.org/rfc/rfc9298.html) (Multiplexed Application Substrate over QUIC Encryption) is an HTTP/3 proxying protocol with properties that directly address Circuit Relay v2's weaknesses:

| | **Circuit Relay v2** | **MASQUE** |
|---|---|---|
| **Looks like** | Custom libp2p protocol | Standard HTTPS traffic |
| **DPI evasion** | Requires WebSocket wrapping | Native - it IS HTTP/3 |
| **Session resume** | New reservation per connection | 0-RTT resume (TLS 1.3 tickets) |
| **Multiplexing** | Via Yamux (12-byte frames) | Native QUIC streams |
| **Infrastructure** | Self-hosted relay | Self-hosted or Cloudflare's global network |
| **Browser support** | No (requires native client) | Yes (WebTransport API) |

Shurli could offer MASQUE as an alternative relay transport alongside Circuit Relay v2 - giving users the choice between libp2p-native P2P and HTTP/3-based relay for environments where traffic must look like standard HTTPS.

### Post-quantum cryptography: The coming mandate

Shurli currently uses Noise protocol with Ed25519 (classical cryptography). Quantum computers could eventually break this. The industry is preparing:

- **NIST finalized** ML-KEM (FIPS 203) and ML-DSA (FIPS 204) as post-quantum standards
- **AWS** KMS, ACM, and Secrets Manager support ML-KEM (Nov 2025)
- **Windows 11/Server 2025** ship with built-in ML-KEM and ML-DSA
- **CRYSTALS-Kyber** being phased out in favor of ML-KEM (transition by 2026)
- **Hybrid approach**: Run classical + post-quantum in parallel during transition

For Shurli, the path is:
1. **Watch** libp2p's adoption of post-quantum Noise variants
2. **Design** cipher suite selection into the architecture (cryptographic agility)
3. **Implement** hybrid Noise + ML-KEM when libp2p support lands

**Sources**: [NIST PQC Standards](https://www.nist.gov/pqcrypto), [AWS ML-KEM Support](https://aws.amazon.com/blogs/security/ml-kem-post-quantum-tls-now-supported-in-aws-kms-acm-and-secrets-manager/)

### eBPF: Relay-server hardening at kernel speed

[eBPF](https://ebpf.io/) (extended Berkeley Packet Filter) allows running sandboxed programs in the Linux kernel without modifying kernel source. For Shurli's relay server:

- **XDP (eXpress Data Path)**: Process packets before they reach the network stack - millions of packets/sec DDoS mitigation
- **Rate limiting**: Per-IP connection throttling at kernel level (faster than iptables)
- **Runtime monitoring**: Detect exploitation attempts on the relay via syscall tracing (Falco, Tetragon)
- **Profiling**: Trace packet processing bottlenecks without instrumentation overhead

This complements the userspace hardening (Resource Manager, per-peer limits) with kernel-level defense. Requires Linux kernel >= 5.8.

### Zero-RTT proxy connection resume

**The problem**: When a laptop switches from WiFi to cellular (or WiFi flickers), all TCP connections through the proxy drop. The user must wait for reconnection (5-15 seconds with Circuit Relay v2).

**The solution**: QUIC 0-RTT session resumption. The client caches a session ticket from the previous connection. On reconnect, it sends encrypted data in the very first packet - before the server even processes the handshake.

**Who has this**: Cloudflare's MASQUE relays, QUIC-native applications.
**Who doesn't**: WireGuard (stateless, reconnects fast but not 0-RTT), all current P2P tunnel tools.

This is a future optimization for Shurli's QUIC transport - particularly valuable for mobile clients (Phase 9).

---

## Why does Shurli use Go instead of Rust?

### The trade-off

| Factor | **Go** | **Rust** |
|--------|--------|----------|
| Development speed | Fast - the reason Shurli exists today | 2-3x slower initial development |
| GC pauses at scale | 10s pauses observed at 600K connections | None - no garbage collector |
| Memory per connection | ~28KB (GC overhead, interface boxing) | ~4-8KB (zero-cost abstractions) |
| libp2p ecosystem | Mature (go-libp2p, most examples) | Growing (rust-libp2p, Iroh) |
| Formal verification | Limited | Strong (s2n-quic has 300+ Kani harnesses) |
| Binary size | ~25-28MB | ~5-10MB |
| Cross-compilation | Trivial (`GOOS=linux GOARCH=arm64`) | Requires target toolchain setup |
| Concurrency model | Goroutines (simple, GC-managed) | async/await (no runtime overhead) |

### Why Go is right for now

Go's simplicity enabled rapid iteration through 7 phases of development. The libp2p Go ecosystem is the most mature, with the most examples and documentation. For a project with 1-100 concurrent connections (typical home use), Go's performance is more than adequate.

### When Rust becomes worth it

At scale - when a relay server handles thousands of concurrent circuits, or when the proxy loop becomes CPU-bound. The hot paths (packet forwarding in the relay, bidirectional proxy loop, SOCKS5 gateway) are candidates for selective Rust rewrite via FFI, not a full project rewrite.

### Rust libraries to watch

| Library | What it does | Why it matters |
|---------|-------------|----------------|
| **[Iroh](https://github.com/n0-computer/iroh)** | Rust P2P library, QUIC-native | ~90% NAT traversal success, QUIC multipath, approaching 1.0 |
| **[Quinn](https://github.com/quinn-rs/quinn)** | Pure Rust QUIC implementation | Used by Iroh, high performance, no C FFI |
| **[s2n-quic](https://github.com/aws/s2n-quic)** | AWS's Rust QUIC | Formal verification with Kani, production-tested in AWS |
| **[tokio](https://github.com/tokio-rs/tokio)** | Async runtime | LTS until Sept 2026, powers hyper (HTTP/2 + HTTP/3) |

### The hybrid strategy

Shurli's planned approach:
1. **Now through Phase 7**: Ship in Go. Fix goroutine lifecycle, tune GC, add observability.
2. **Phase 8+**: Profile hot paths under load. Selectively rewrite proxy loop / relay forwarding in Rust via FFI if performance demands it.
3. **Long-term**: Re-evaluate full Rust migration only if market demands 100x throughput and there's engineering capacity for it.

**Sources**: [Rust vs Go (Bitfield)](https://bitfieldconsulting.com/posts/rust-vs-go), [Go GC Guide](https://tip.golang.org/doc/gc-guide), [Iroh roadmap](https://www.iroh.computer/roadmap)

---

## How does reachability grade computation work in detail?

The reachability grade combines two data sources: interface discovery and STUN probe results.

**Interface discovery** scans all network interfaces and classifies each address:
- Global unicast IPv6 -> public
- Public IPv4 (not RFC 1918 / RFC 6598) -> public
- RFC 6598 (`100.64.0.0/10`) -> CGNAT flag set
- Everything else -> private/local

**STUN probing** uses Google's public STUN servers to determine NAT behavior. It reports the external IP, port allocation strategy, and filtering behavior.

**Grade computation logic**:

```
if no connectivity:           Grade F
if CGNAT detected:            Grade D (cap, overrides STUN)
if public IPv6:               Grade A
if public IPv4:               Grade B
if full-cone or addr-restricted: Grade B
if port-restricted:           Grade C
if symmetric:                 Grade D
```

The CGNAT cap at grade D is the critical design choice. STUN probes the inner NAT and can report "hole-punchable" when the outer CGNAT will drop the punched packets. The grade overrides this false optimism.

Grades update automatically on network change events (WiFi switch, cable plug/unplug, VPN up/down). The grade is exposed via `shurli daemon status` and the REST API.

---

## What is sovereign peer interaction history?

Each daemon maintains a local `peer_history.json` file tracking interaction data with every known peer. This data never leaves the machine - it's the foundation for future trust algorithms.

**What's tracked per peer**:

| Field | Purpose |
|-------|---------|
| `first_seen` | When this peer was first encountered |
| `last_seen` | Most recent connection |
| `connection_count` | Total successful connections |
| `avg_latency_ms` | Running average (Welford's online algorithm) |
| `path_types` | Map of `"direct": N, "relay": M` |
| `introduced_by` | Which relay or peer introduced this one |
| `intro_method` | `"relay-pairing"`, `"invite"`, or `"manual"` |

**Implementation details**:
- Thread-safe with `sync.RWMutex`
- Atomic file writes (temp file + rename) for crash safety
- Best-effort load on startup (missing file is not an error)
- Storage bounded by peer count (per-peer aggregates, not per-connection logs)

**Why collect now**: Future trust algorithms (EigenTrust, reputation scoring) need interaction data as input. Starting collection now means months of history will be ready when those algorithms ship. Waiting until algorithm implementation to start collecting means zero history to bootstrap from.

**Sovereignty**: Each peer controls its own history. No central reputation server. No gossip-based sharing. The data stays local until explicit trust algorithms decide how (and whether) to use it.

---

**Last Updated**: 2026-02-25


---

# Engineering Journal

This document captures the **why** behind every significant architecture decision in Shurli. Each entry follows a lightweight ADR (Architecture Decision Record) format: what problem we faced, what options we considered, what we chose, and what trade-offs we accepted.

New developers, contributors, and future-us should be able to read this and understand not just what the code does, but why it's shaped the way it is.

## Reading Guide

- **ADR-0XX**: Core architecture decisions made before the batch system
- **ADR-X0Y**: Batch-specific decisions (A=reliability, B=code quality, etc.)
- Each ADR is self-contained - read any entry independently
- Entries link to source files and commits where relevant

## Sections

| Section | ADRs | Focus |
|---------|------|-------|
| [Core Architecture](core-architecture.md) | ADR-001 to ADR-008 | Foundational technology choices |
| [Batch A: Reliability](batch-a-reliability.md) | ADR-A01 to ADR-A04 | Timeouts, retries, DHT, integration tests |
| [Batch B: Code Quality](batch-b-code-quality.md) | ADR-B01 to ADR-B04 | Dedup, logging, errors, versioning |
| [Batch C: Self-Healing](batch-c-self-healing.md) | ADR-C01 to ADR-C03 | Config backup, commit-confirmed, watchdog |
| [Batch D: libp2p Features](batch-d-libp2p-features.md) | ADR-D01 to ADR-D04 | AutoNAT, QUIC, Identify, smart dialing |
| [Batch E: New Capabilities](batch-e-new-capabilities.md) | ADR-E01 to ADR-E02 | Relay health, headless invite/join |
| [Batch F: Daemon Mode](batch-f-daemon-mode.md) | ADR-F01 to ADR-F04 | Unix socket, cookie auth, hot-reload |
| [Batch G: Test Coverage](batch-g-test-coverage.md) | ADR-G01 to ADR-G04 | Docker tests, relay binary, injectable exit, audit protocol |
| [Batch H: Observability](batch-h-observability.md) | ADR-H01 to ADR-H03 | Prometheus, nil-safe pattern, auth callback |
| [Pre-Batch I](pre-batch-i.md) | ADR-Ia01 to ADR-Ib02 | Makefile, PAKE invite, DHT namespaces |
| [Batch I: Adaptive Path Selection](batch-i-adaptive-path.md) | ADR-I01 to ADR-I06 | Interface discovery, dial racing, path tracking, network monitoring, STUN, peer relay |
| [Post-I-2: Trust & Delivery](post-i-2-trust-and-delivery.md) | ADR-J01 to ADR-J06 | Peer notify, HMAC, relay admin, SAS verification, reachability, history |
| [Pre-Phase 5 Hardening](pre-phase5-hardening.md) | ADR-K01 to ADR-K03 | Startup race, CGNAT detection, stale addresses, service deployment |
| [Dev Tooling](dev-tooling.md) | ADR-DT01 to ADR-DT02 | Go doc sync pipeline, relay setup subcommand |


---

# Core Architecture Decisions

Foundational technology choices made before the batch system.

---

### ADR-001: Why Go

**Context**: Shurli needs to compile to a single static binary, run on Linux/macOS/Windows, and interface with libp2p (which has mature Go and Rust implementations).

**Alternatives considered**:
- **Rust** - Better memory safety guarantees, smaller binaries. Rejected because rust-libp2p has less mature circuit relay v2 support, and compile times would slow iteration during early development.
- **Python/Node.js** - Faster prototyping. Rejected because distribution requires runtime dependencies, violating the "single binary, zero dependencies" principle.

**Decision**: Go. Single binary compilation, excellent cross-platform support, mature libp2p ecosystem, and fast compilation for rapid iteration.

**Consequences**: Larger binary size (~28MB stripped) compared to Rust. Accepted because distribution simplicity outweighs binary size for a CLI tool. Binary size is actively monitored and optimized (see `binary-optimization` practices).

**Reference**: `go.mod`, `cmd/shurli/main.go`

---

### ADR-002: Why libp2p (Not Raw QUIC, Not WireGuard)

**Context**: Shurli needs NAT traversal, encrypted transport, peer discovery, and circuit relay. Building these from scratch would take years.

**Alternatives considered**:
- **Raw QUIC + custom protocol** - Full control, smaller dependency tree. Rejected because we'd need to implement hole punching, relay, DHT, and peer routing from scratch.
- **WireGuard** - Excellent performance, kernel-level. Rejected because it requires root/admin privileges, doesn't solve discovery, and doesn't provide circuit relay for CGNAT.
- **Noise protocol + custom transport** - Lighter than libp2p. Rejected because discovery and relay still need to be built.

**Decision**: libp2p v0.47.0. Provides QUIC+TCP+WebSocket transports, circuit relay v2, hole punching (DCUtR), Kademlia DHT, peer identity (Ed25519), and connection gating - all battle-tested.

**Consequences**: Large dependency tree (100+ transitive deps). The binary includes WebRTC and other transports we don't directly use. Accepted because reliability > binary size, and we actively track CVEs in dependencies.

**Reference**: `go.mod`, `pkg/p2pnet/network.go`

---

### ADR-003: Why Private DHT `/shurli/kad/1.0.0`

**Context**: Initially used the public IPFS Amino DHT (`/ipfs/kad/1.0.0`). This worked but mixed shurli peers into the global IPFS routing table, leaking peer discovery to the public network.

**Alternatives considered**:
- **Keep IPFS Amino DHT** - Zero config, large bootstrap. Rejected because (a) privacy: shurli peers are discoverable by anyone on IPFS, (b) pollution: shurli's rendezvous strings pollute the global DHT, (c) reliability: depends on IPFS bootstrap nodes staying healthy.
- **No DHT, relay-only** - Simpler. Rejected because DHT enables peer discovery without centralized infrastructure.
- **mDNS only** - Local network discovery. Rejected because it doesn't work across networks.

**Decision**: Private Kademlia DHT with protocol prefix `/shurli/kad/1.0.0` (constant `p2pnet.DHTProtocolPrefix`). Shurli peers only discover and route to other shurli peers.

**Consequences**: Smaller routing table (only shurli peers), no IPFS bootstrap dependency, but requires at least one known peer (relay) to bootstrap into the DHT.

**Reference**: `pkg/p2pnet/network.go:27` (`DHTProtocolPrefix` constant), commit `d1d4336`

---

### ADR-004: Why Circuit Relay v2

**Context**: Users behind CGNAT (5G, carrier-grade NAT, double NAT) cannot receive inbound connections. This is the core problem Shurli solves.

**Alternatives considered**:
- **UPnP/NAT-PMP only** - Works for simple NAT, fails on CGNAT. Rejected as sole strategy.
- **TURN server** - WebRTC-style relay. Rejected because it's a separate protocol ecosystem; libp2p's circuit relay v2 integrates naturally with the existing transport stack.
- **Circuit relay v1** - Deprecated by libp2p. Rejected.

**Decision**: Circuit relay v2 via `libp2p.EnableAutoRelayWithStaticRelays()`. The relay server makes reservations for peers, enabling them to be reached through the relay.

**Consequences**: All traffic flows through the relay when direct connection fails. Relay becomes a critical infrastructure component - must be hardened, monitored, and eventually made redundant. Batch I shipped every-peer-is-a-relay (I-f), beginning the path to relay VPS elimination.

**Reference**: `pkg/p2pnet/network.go:140`, `cmd/relay-server/`

---

### ADR-005: Why Connection Gating via `authorized_keys`

**Context**: Shurli networks are private. Only explicitly authorized peers should connect. Needed an SSH-like trust model.

**Alternatives considered**:
- **Certificate authority** - More scalable, supports expiration. Rejected because it requires PKI infrastructure (CA key management, certificate issuance), which contradicts "no central authority."
- **Pre-shared keys** - Simpler than CA. Rejected because it doesn't provide per-peer identity.
- **No gating, encryption only** - Let any peer connect but encrypt traffic. Rejected because authorization is a core security requirement, not optional.

**Decision**: `authorized_keys` file containing one peer ID per line, checked by `auth.AuthorizedPeerGater` in `InterceptSecured()`. Only inbound connections are gated; outbound (to relay, DHT) are always allowed.

**Consequences**: Simple file-based auth that users already understand from SSH. Hot-reloadable at runtime. Scales to hundreds of peers. Does not support per-peer permissions (all-or-nothing access) - acceptable for current scope.

**Reference**: `internal/auth/gater.go`, `internal/auth/keys.go`

---

### ADR-006: Why Single Binary with Subcommands

**Context**: Shurli has many functions: daemon, ping, proxy, config management, relay server (separate binary). Needed a clean CLI structure.

**Alternatives considered**:
- **Separate binaries per function** - `shurli-daemon`, `shurli-ping`, etc. Rejected because it complicates distribution and PATH management.
- **cobra/urfave CLI framework** - Feature-rich. Rejected because they add dependency weight and complexity for what's essentially a dispatch table. Standard library `flag` + manual dispatch is lighter and fully sufficient.

**Decision**: Single `shurli` binary using `os.Args[1]` dispatch (`cmd/shurli/main.go`) with standard library `flag` for each subcommand. Relay server is a separate binary (`cmd/relay-server/`) because it has different deployment concerns (VPS vs local machine).

**Consequences**: The binary includes all functionality, so it's slightly larger than specialized binaries would be. Accepted because single-binary deployment is a core principle - `curl install | sh` drops one file.

**Reference**: `cmd/shurli/main.go`

---

### ADR-007: Why YAML Config

**Context**: Shurli needs configuration for identity, network, relay, discovery, security, services, and names. Needed a human-readable, editable format.

**Alternatives considered**:
- **TOML** - Good for flat config. Rejected because nested structures (services map, relay addresses) are more natural in YAML.
- **JSON** - Universal. Rejected because no comments, poor human editability for config files users need to hand-edit.
- **HCL** - HashiCorp's format. Rejected because it adds a dependency and is unfamiliar to most users.
- **Flags/env vars only** - Simpler. Rejected because the configuration is too complex for command-line flags alone.

**Decision**: YAML via `gopkg.in/yaml.v3`. Single config file with versioning (`version: 1`), duration strings (`10m`, `1h`), and relative path resolution.

**Consequences**: YAML is sensitive to indentation, which can confuse users. Mitigated by: (a) `shurli init` generates valid config automatically, (b) `shurli config validate` catches syntax errors, (c) config templates in `config_template.go` ensure consistency.

**Reference**: `internal/config/types.go`, `internal/config/loader.go`, `cmd/shurli/config_template.go`

---

### ADR-008: Why No External Dependencies Beyond libp2p

**Context**: Every dependency is an attack surface, a binary size cost, and a maintenance burden. Shurli is infrastructure software.

**Alternatives considered**: N/A - this is a constraint, not a choice between options.

**Decision**: The only direct dependencies are `go-libp2p`, `go-libp2p-kad-dht`, `go-multiaddr`, and `gopkg.in/yaml.v3`. Everything else (logging, config, auth, watchdog, QR codes) is implemented with Go standard library.

**Consequences**: More code to maintain (e.g., pure-Go sd_notify instead of using a systemd library), but complete control over behavior, smaller binary, and zero supply chain risk beyond the libp2p ecosystem.

**Reference**: `go.mod` (4 direct dependencies)


---

# Batch A: Reliability

Timeouts, retries, DHT in the proxy path, and in-process integration tests.

---

### ADR-A01: TCP Timeout Strategy

**Context**: TCP proxy connections through circuit relay need appropriate timeouts. Too short = drops active SSH sessions. Too long = leaked connections consume relay resources.

**Alternatives considered**:
- **No explicit timeouts** (rely on libp2p defaults) - Rejected because libp2p's default stream timeouts are too short for interactive SSH sessions.
- **Configurable per-service timeouts** - Considered for future, but adds complexity for a problem that has reasonable defaults.

**Decision**: 10-second dial timeout for initial TCP connection (`net.DialTimeout("tcp", addr, 10*time.Second)`), 30-second context timeout for service connections. No idle timeout - SSH sessions can be long-lived; the half-close proxy (`BidirectionalProxy`) cleanly handles EOF propagation.

**Consequences**: Long-lived connections are supported, but a peer that disappears without closing the stream will hold resources until the relay's session duration limit (default 10 minutes) kicks in.

**Reference**: `pkg/p2pnet/proxy.go:66`

---

### ADR-A02: Retry with Exponential Backoff

**Context**: P2P connections through relays are inherently unreliable. A single dial failure shouldn't kill a proxy session.

**Alternatives considered**:
- **No retry** - Fail immediately. Rejected because relay connections often fail transiently.
- **Fixed delay retry** - Simpler but can cause thundering herd and doesn't adapt to load.

**Decision**: `DialWithRetry()` wraps any dial function with exponential backoff: 1s, 2s, 4s, ..., capped at 60s. Default 3 retries for daemon-created proxies.

**Consequences**: A failing connection takes up to ~7 seconds before giving up (1+2+4), which is acceptable for interactive use. The cap at 60s prevents runaway delays.

**Reference**: `pkg/p2pnet/proxy.go:130-155`

---

### ADR-A03: DHT in Proxy Path

**Context**: When the daemon receives a proxy connect request, the target peer might not be directly connected. Need to find and reach them first.

**Alternatives considered**:
- **Require pre-existing connection** - Simpler but fragile. Rejected because peers reconnect through DHT discovery, and the user shouldn't need to manually reconnect before proxying.
- **DNS-based discovery** - Rejected because it requires external infrastructure.

**Decision**: `ConnectToPeer()` in the daemon runtime performs DHT lookup + relay address injection before establishing the service stream. Every proxy and ping operation calls this first.

**Consequences**: First connection to a peer may be slow (DHT walk + relay reservation). Subsequent connections reuse the existing link. This is the correct behavior - find the peer, then talk to them.

**Reference**: `cmd/shurli/serve_common.go` (`ConnectToPeer` method), `internal/daemon/handlers.go:338`

---

### ADR-A04: In-Process Integration Tests

**Context**: Need integration tests that verify multi-peer P2P scenarios without requiring Docker, LAN access, or actual network infrastructure.

**Alternatives considered**:
- **Docker-only tests** - Realistic but slow and requires Docker installed. Added later as a complement (Batch G), not a replacement.
- **Mock libp2p hosts** - Too much mocking makes tests unreliable.

**Decision**: Create real libp2p hosts in the same process, connecting through an in-process relay. Tests in `pkg/p2pnet/` create 2-3 hosts that communicate through circuit relay within a single test binary.

**Consequences**: Tests are fast (~2s) and run anywhere (`go test ./...`). They don't test actual network conditions (latency, packet loss), which is why Docker integration tests were added later as a complement.

**Reference**: `pkg/p2pnet/network_test.go`, `pkg/p2pnet/service_test.go`


---

# Batch B: Code Quality

Relay address deduplication, structured logging, sentinel errors, and build version embedding.

---

### ADR-B01: Proxy Deduplication

**Context**: `ParseRelayAddrs()` could receive duplicate relay addresses (same peer, different multiaddrs). Without dedup, libp2p would make redundant connections.

**Alternatives considered**:
- **Let libp2p handle it** - libp2p does some dedup, but passing duplicates to `EnableAutoRelayWithStaticRelays` wastes resources.

**Decision**: `ParseRelayAddrs()` deduplicates by peer ID and merges addresses for the same relay peer. If the same relay appears twice with different addresses, all addresses are collected under one `peer.AddrInfo`.

**Consequences**: Clean relay configuration. Users can list multiple addresses for the same relay (e.g., IPv4 and IPv6) without issues.

**Reference**: `pkg/p2pnet/network.go:280-309`

---

### ADR-B02: `log/slog` over zerolog/zap

**Context**: Needed structured logging throughout the project. Many Go projects use zerolog or zap for performance.

**Alternatives considered**:
- **zerolog** - Zero-allocation, fast. Rejected because it's another dependency, and Shurli doesn't produce enough log volume to need zero-allocation logging.
- **zap** - Uber's logger, excellent performance. Rejected for the same reason - adds dependency weight for no measurable benefit.
- **log/slog** - Go 1.21+ standard library structured logging. Built-in, no dependency, sufficient performance.

**Decision**: `log/slog` everywhere. `slog.Info`, `slog.Warn`, `slog.Error` with structured key-value pairs. Default handler writes to stderr.

**Consequences**: No external logging dependency. Standard library compatibility means any future handler (JSON, OpenTelemetry) can be swapped in without changing call sites. Slightly more verbose than zerolog's fluent API, but consistency with stdlib is worth it.

**Reference**: `cmd/shurli/main.go:20-22` (handler setup), used throughout all packages

---

### ADR-B03: Sentinel Errors

**Context**: Error handling was using `fmt.Errorf("service not found")` strings that callers couldn't programmatically check.

**Alternatives considered**:
- **String matching** - `strings.Contains(err.Error(), "not found")`. Rejected because it's fragile and breaks on message changes.
- **Custom error types** - `type NotFoundError struct { Name string }`. Considered for complex errors, but sentinel variables are simpler for the common case.

**Decision**: Package-level sentinel errors using `errors.New()`: `ErrServiceNotFound`, `ErrNameNotFound`, `ErrConfigNotFound`, `ErrNoArchive`, `ErrCommitConfirmedPending`, `ErrNoPending`, `ErrDaemonAlreadyRunning`, `ErrProxyNotFound`. Callers use `errors.Is()` to check.

**Consequences**: Clean error checking, wrappable with `fmt.Errorf("%w: ...", ErrFoo)`. Error messages in two packages: `pkg/p2pnet/errors.go` and `internal/config/errors.go`.

**Reference**: `pkg/p2pnet/errors.go`, `internal/config/errors.go`, `internal/daemon/errors.go`

---

### ADR-B04: Build Version Embedding

**Context**: Need to know exactly which version and commit is running, especially when debugging relay issues remotely.

**Alternatives considered**:
- **Version file** - Read from embedded file. Rejected because it's another artifact to maintain.
- **Git describe at runtime** - Call `git describe` at startup. Rejected because the binary might not be in a git repo.

**Decision**: `ldflags` injection at build time: `-X main.version=... -X main.commit=... -X main.buildDate=...`. Defaults to `dev` and `unknown` for development builds. Also sent as libp2p Identify UserAgent (`shurli/0.1.0`).

**Consequences**: Every binary is self-identifying. `shurli version` shows exact build info. The UserAgent appears in `shurli daemon peers --all`, making it easy to verify what version each peer runs.

**Reference**: `cmd/shurli/main.go:10-17`, `pkg/p2pnet/network.go:121-123`


---

# Batch C: Self-Healing

Config archive/rollback, commit-confirmed pattern, and watchdog with pure-Go sd_notify.

---

### ADR-C01: Config Archive/Rollback (Juniper-Inspired)

**Context**: A bad config change on a remote node (e.g., wrong relay address) could make it permanently unreachable. Need a recovery mechanism.

**Alternatives considered**:
- **Git-based config history** - Track config in a git repo. Rejected because it requires git installed and adds complexity.
- **Numbered backups** (config.1, config.2, ...) - More history but harder to manage cleanup.

**Decision**: Juniper-style last-known-good: `Archive()` copies current config to `.config.last-good.yaml` with atomic write (temp file + rename). `Rollback()` restores it. Single backup slot - simple, sufficient.

**Consequences**: Only one rollback level (no multi-step undo). Accepted because the common case is "my last change broke it, undo that one change." The archive is created before daemon start and before config apply.

**Reference**: `internal/config/archive.go`

---

### ADR-C02: Commit-Confirmed Pattern

**Context**: Changing config on a remote node is dangerous - if the new config prevents connectivity, you're locked out. Network engineers solve this with "commit confirmed" - apply the change, and if you don't confirm within N minutes, it auto-reverts.

**Alternatives considered**:
- **Manual rollback only** - User must SSH in (if they can) and run `shurli config rollback`. Rejected because if the config broke SSH access, there's no way in.
- **Two-phase commit** - More complex, requires coordination. Rejected as over-engineering for a single-node config change.

**Decision**: `shurli config apply <new> --confirm-timeout 5m` backs up current config, applies new config, starts a timer. If `shurli config confirm` isn't run within the timeout, the daemon reverts to the backup and restarts via `exitFunc(1)` (systemd restarts it with the restored config).

**Consequences**: Requires systemd (or equivalent) to restart on exit. The `exitFunc` is injectable for testing (`EnforceCommitConfirmed` takes `func(int)` instead of calling `os.Exit` directly).

**Reference**: `internal/config/confirm.go`, `cmd/shurli/cmd_config.go`

---

### ADR-C03: Watchdog + sd_notify (Pure Go)

**Context**: The daemon needs to report health to systemd and restart on failure. Most Go projects use `coreos/go-systemd` for sd_notify.

**Alternatives considered**:
- **`coreos/go-systemd`** - Mature library. Rejected because it's another dependency for 30 lines of socket code. Also pulls in dbus bindings we don't need.
- **No watchdog** - Let systemd's simple restart handle failures. Rejected because watchdog provides proactive health checking, not just crash recovery.

**Decision**: Pure Go sd_notify implementation in `internal/watchdog/watchdog.go`. Three functions: `Ready()` (READY=1), `Watchdog()` (WATCHDOG=1), `Stopping()` (STOPPING=1). All send datagrams to `$NOTIFY_SOCKET`. No-op when not running under systemd (macOS, manual launch).

The watchdog loop runs configurable health checks (default 30s interval) and only sends WATCHDOG=1 when all checks pass. The daemon adds a socket health check to verify the API is still accepting connections.

**Consequences**: Zero dependency for systemd integration. Works on both Linux (systemd) and macOS (launchd, where sd_notify is a no-op). The health check framework is extensible - Batch H will add libp2p connection health.

**Reference**: `internal/watchdog/watchdog.go`, `cmd/shurli/cmd_daemon.go:158-166`


---

# Batch D: libp2p Features

AutoNAT v2, QUIC transport ordering, Identify UserAgent, and smart dialing.

---

### ADR-D01: AutoNAT v2

**Context**: Peers need to know if they're behind NAT to decide whether to use relay. libp2p's AutoNAT v1 had accuracy issues.

**Alternatives considered**:
- **Manual reachability flag only** (`force_private_reachability: true`) - Works but requires users to know their NAT situation.
- **AutoNAT v1** - Older protocol, less accurate with CGNAT.

**Decision**: Enable AutoNAT v2 via `libp2p.EnableAutoNATv2()` alongside the manual flag. AutoNAT v2 uses a more reliable probing mechanism to determine reachability.

**Consequences**: Slightly more network chatter (AutoNAT probes), but more accurate reachability detection. The manual `force_private_reachability` flag remains as an override for cases where AutoNAT can't determine the correct state.

**Reference**: `pkg/p2pnet/network.go:118`

---

### ADR-D02: QUIC Preferred Transport Ordering

**Context**: libp2p supports multiple transports. The order they're specified affects which is tried first during connection establishment.

**Alternatives considered**:
- **TCP first** - Most compatible, works through all middleboxes. But slower connection establishment (4 RTTs for TCP+TLS+mux vs 3 for QUIC).
- **WebSocket first** - Anti-censorship benefit. But highest overhead.

**Decision**: Transport order is QUIC first, TCP second, WebSocket third. QUIC has native multiplexing (no yamux needed), faster handshake (1-RTT after initial), and better hole-punching characteristics. TCP is the universal fallback. WebSocket is for DPI/censorship evasion.

**Consequences**: Environments that block UDP (some corporate networks) will fall back to TCP automatically. The ordering is declarative in `New()` - first transport to succeed wins.

**Reference**: `pkg/p2pnet/network.go:113-117`

---

### ADR-D03: Identify UserAgent

**Context**: When multiple peers are connected, it's hard to tell which are shurli peers vs DHT neighbors, relay servers, or random libp2p nodes.

**Alternatives considered**:
- **Custom protocol handshake** - Send version info in a custom protocol. Rejected because libp2p's Identify protocol already does this.

**Decision**: Set `libp2p.UserAgent("shurli/" + version)` on every host. The daemon's peer list filters by UserAgent prefix (`shurli/` or `relay-server/`) by default, showing only network members. `--all` flag shows everything.

**Consequences**: Version info is visible to any connected peer (including non-shurli peers). Accepted because version strings are not sensitive - they aid debugging and interoperability.

**Reference**: `pkg/p2pnet/network.go:121-123`, `internal/daemon/handlers.go:78-80`

---

### ADR-D04: Smart Dialing

**Context**: libp2p tries all known addresses for a peer simultaneously. With relay addresses in the peerstore, it might waste time on direct addresses that will fail for CGNAT peers.

**Alternatives considered**:
- **Relay-only dialing** - Only use relay. Rejected because direct connections should be preferred when available.

**Decision**: Let libp2p's default smart dialing handle address selection, but ensure relay circuit addresses are always in the peerstore via `AddRelayAddressesForPeer()`. This gives the dialer both direct and relay options, and it picks the fastest.

**Consequences**: Relies on libp2p's dialing heuristics, which generally prefer direct connections. Batch I added explicit address ranking via `PathDialer` (direct IPv6 > direct IPv4 > STUN-punched > peer relay > VPS relay) and parallel dial racing.

**Reference**: `pkg/p2pnet/network.go:260-270`


---

# Batch E: New Capabilities

Relay health endpoint and headless invite/join for scripting.

---

### ADR-E01: `/healthz` on Relay

**Context**: The relay server is a critical public-facing service. Monitoring systems need a health endpoint.

**Alternatives considered**:
- **TCP port check only** - Just verify the port is open. Rejected because it doesn't verify the relay is actually functional.
- **Full metrics endpoint** - Prometheus-style. Planned for Batch H, but `/healthz` needed now for basic monitoring.

**Decision**: HTTP `/healthz` endpoint on configurable address (default `127.0.0.1:9090`). Returns JSON with `status`, `uptime_seconds`, and `connected_peers`. Restricted to loopback by default - reverse proxy or SSH tunnel for remote access.

**Consequences**: Minimal information exposure (no peer IDs, no version, no protocol list in the health response - hardened in the post-phase audit). Loopback-only prevents information disclosure to the public internet.

**Reference**: `cmd/shurli/cmd_relay_serve.go`

---

### ADR-E02: Headless Invite/Join

**Context**: Docker containers, CI/CD pipelines, and scripts need to create/accept invites without interactive prompts or QR codes.

**Alternatives considered**:
- **Separate CLI for scripting** - A `shurli-cli` tool. Rejected because it fragments the tool.
- **Environment variables only** - `SHURLI_INVITE_CODE=xxx shurli join`. Supported alongside the flag.

**Decision**: `--non-interactive` flag on both `invite` and `join`. In non-interactive mode: invite prints bare code to stdout (progress to stderr), join reads code from positional arg or `SHURLI_INVITE_CODE` env var. No QR code, no prompts, no color.

**Consequences**: Docker integration tests can create and exchange invite codes programmatically. The flag reuses the same code paths as interactive mode - just different I/O routing.

**Reference**: `cmd/shurli/cmd_invite.go:34`, `cmd/shurli/cmd_join.go`


---

# Batch F: Daemon Mode

Unix socket IPC, cookie authentication, RuntimeInfo interface, and hot-reload authorized_keys.

---

### ADR-F01: Unix Socket (Not TCP)

**Context**: The daemon needs a control API for CLI subcommands (`shurli daemon status`, `shurli daemon ping`, etc.). Need an IPC mechanism.

**Alternatives considered**:
- **TCP on localhost** - Universal, works on all platforms. Rejected because (a) any local process can connect (no filesystem permissions), (b) port conflicts with other services, (c) potentially exposed if firewall misconfigured.
- **Named pipes** - Windows-friendly. Rejected because they don't support HTTP natively and complicate the implementation.
- **gRPC** - Type-safe, bi-directional streaming. Rejected because it adds protobuf dependency, code generation, and binary size. HTTP+JSON is simpler and sufficient.

**Decision**: Unix domain socket at `~/.config/shurli/shurli.sock` with HTTP/1.1 over it. Socket created with `umask(0077)` to ensure `0700` permissions atomically (no TOCTOU race between `Listen()` and `Chmod()`). Stale socket detection: try connecting first, only remove if connection fails.

**Consequences**: Unix-only (no Windows support for now). Accepted because Shurli's target users are Linux/macOS. Socket permissions enforce that only the owning user can connect. The HTTP layer means standard tools (`curl --unix-socket`) work for debugging.

**Reference**: `internal/daemon/server.go:86-138`

---

### ADR-F02: Cookie Auth (Not mTLS)

**Context**: Even with socket permissions, the API needs authentication to prevent attacks via symlink races or debugger attachment.

**Alternatives considered**:
- **mTLS** - Strong mutual authentication. Rejected because it requires certificate management, key generation, and trust store configuration - too complex for a local IPC mechanism.
- **Token in socket filename** - Embed the token in the path. Rejected because path-based auth is fragile and leaks the token in `ps` output and logs.
- **No auth** (rely on socket permissions) - Rejected because defense-in-depth requires authentication even when filesystem permissions are correct.

**Decision**: 32-byte random hex cookie written to `~/.config/shurli/.daemon-cookie` with `0600` permissions. CLI reads the cookie and sends it as `Authorization: Bearer <token>`. Cookie is rotated every daemon restart. Written AFTER socket is secured (ordering prevents clients from reading cookie before socket is ready).

**Consequences**: Simple, fast, no crypto libraries needed. The cookie file is the single secret - protect it like an SSH private key. If compromised, restart the daemon to rotate.

**Reference**: `internal/daemon/server.go:88-116`, `internal/daemon/client.go`

---

### ADR-F03: RuntimeInfo Interface

**Context**: The daemon server needs access to the P2P network, config paths, version info, and connection methods. But the daemon package shouldn't import `cmd/shurli`.

**Alternatives considered**:
- **Pass individual fields** - `NewServer(network, configPath, authKeys, version, ...)`. Rejected because the parameter list would grow with every new feature.
- **Share a struct directly** - Import the runtime struct from cmd. Rejected because it creates a circular dependency between `internal/daemon` and `cmd/shurli`.

**Decision**: `daemon.RuntimeInfo` interface with methods: `Network()`, `ConfigFile()`, `AuthKeysPath()`, `GaterForHotReload()`, `Version()`, `StartTime()`, `PingProtocolID()`, `ConnectToPeer()`. The `serveRuntime` struct in `cmd/shurli/cmd_daemon.go` implements it.

**Consequences**: Clean dependency direction (daemon depends on interface, not concrete type). Easy to mock in tests (`mockRuntime`). Adding new runtime capabilities means adding methods to the interface - intentionally explicit.

**Reference**: `internal/daemon/server.go:23-32`, `cmd/shurli/cmd_daemon.go:23-28`

---

### ADR-F04: Hot-Reload `authorized_keys`

**Context**: Adding or removing peers via `shurli daemon auth add/remove` should take effect immediately without restarting the daemon.

**Alternatives considered**:
- **File watcher (fsnotify)** - Watch the file for changes. Rejected because it adds a dependency and doesn't help with API-triggered changes (where we already know when to reload).
- **Restart required** - Simpler but terrible UX. Rejected.

**Decision**: `GaterReloader` interface with `ReloadFromFile()` method. When the daemon API adds/removes a peer from the `authorized_keys` file, it immediately calls `ReloadFromFile()`, which re-reads the file and calls `gater.UpdateAuthorizedPeers()` with the new map. The gater uses `sync.RWMutex` for concurrent safety.

**Consequences**: Changes are atomic (read file, swap map under lock). No file watching needed. The gater's `authorizedPeers` map is replaced entirely - no incremental updates. This is fine because the authorized_keys file is small (typically <100 entries).

**Reference**: `cmd/shurli/cmd_daemon.go:37-51`, `internal/auth/gater.go:74-79`


---

# Batch G: Test Coverage

Coverage-instrumented Docker tests, relay binary, injectable exit, and post-phase audit protocol.

---

### ADR-G01: Coverage-Instrumented Docker Tests

**Context**: Docker integration tests verify real binaries in containers but didn't contribute to coverage metrics. Needed to merge Docker test coverage with unit test coverage.

**Alternatives considered**:
- **Separate coverage reports** - Track Docker and unit coverage independently. Rejected because it gives an incomplete picture.
- **Coverage at the Go test level only** - Skip Docker coverage. Rejected because the Docker tests exercise critical paths (relay, invite/join) that unit tests can't.

**Decision**: Build binaries with `-cover -covermode=atomic`, set `GOCOVERDIR` in containers, extract coverage data after tests, merge with unit test profiles using `go tool covdata`. Combined coverage reported in CI.

**Consequences**: Docker tests are slower (coverage instrumentation adds overhead), but we get accurate end-to-end coverage numbers. The merged profile reveals which code paths are only exercised by integration tests.

**Reference**: `test/docker/integration_test.go`, `.github/workflows/ci.yml`

---

### ADR-G02: Relay-Server Binary in Integration Tests

**Context**: Docker integration tests need to run the relay server. The relay server is built from `cmd/relay-server/`.

**Alternatives considered**:
- **Use a public relay** - Test against a real relay. Rejected because tests must be self-contained and reproducible.
- **Mock relay in-process** - Use libp2p relay transport directly. Rejected because we want to test the actual relay-server binary.

**Decision**: Build `relay-server` binary alongside `shurli` binary for Docker tests. The compose file starts a relay container, and node containers use it for circuit relay.

**Consequences**: Tests verify the actual deployment path (binary -> container -> relay -> circuit). Takes longer to build but catches real integration issues.

**Reference**: `test/docker/compose.yaml`, `test/docker/Dockerfile`

---

### ADR-G03: Injectable `osExit` for Testability

**Context**: Several commands call `os.Exit()` on error. This kills the test process, making those code paths untestable.

**Alternatives considered**:
- **Panic + recover** - Use `panic` instead of `os.Exit` and recover in tests. Rejected because panics have different semantics (stack traces, deferred functions).
- **Return error codes** - Refactor all commands to return errors. Considered for future, but too large a refactor for a testing improvement.

**Decision**: Package-level `var osExit = os.Exit` that tests override with a function that records the exit code instead of terminating. Applied to `cmd/shurli/` (the main binary) and `cmd/relay-server/`.

**Consequences**: Minimal code change (one variable + one test helper), enables testing of all exit paths. The variable is package-level, so tests must be careful about parallel execution (each test restores the original `osExit`).

**Reference**: `cmd/shurli/run.go`, `cmd/shurli/run_test.go`

---

### ADR-G04: Post-Phase Audit Protocol

**Context**: After completing each batch, need a systematic review to catch issues before moving to the next phase. Ad-hoc reviews miss things.

**Alternatives considered**:
- **Ad-hoc review** - Review when something feels wrong. Rejected because it's inconsistent and misses systematic issues.
- **External audit** - Hire security auditors. Planned for later stages, but too expensive for every batch.

**Decision**: Mandatory 6-category audit after every phase: source code audit, bad code scan, bug hunting, QA testing, security audit, and relay hardening review. Each category has specific checklists. Findings are compiled into a report, and fixes require explicit approval before implementation.

The Batch G audit found 10 issues (CVE in pion/dtls, TOCTOU on Unix socket, cookie ordering, body size limits, CI SHA pinning, etc.) - all fixed in commit `83d02d3`.

**Consequences**: Adds time between batches, but catches real issues. The audit that found the pion/dtls nonce-reuse CVE justified the entire protocol - that vulnerability could have compromised encrypted relay traffic.

**Reference**: Audit findings tracked in project memory, fixes in commit `83d02d3`


---

# Batch H: Observability

Prometheus metrics, nil-safe observability pattern, and auth decision callback.

---

### ADR-H01: Prometheus over OpenTelemetry

**Context**: Batch H adds metrics and audit logging. The original roadmap said "OpenTelemetry integration." Research revealed that libp2p v0.47.0 emits metrics natively via `prometheus/client_golang`, not OpenTelemetry.

**Alternatives considered**:
- **OpenTelemetry SDK** - Industry standard, supports traces + metrics + logs. Rejected because: +4MB binary size, 35% CPU overhead from span management on every stream, and libp2p already speaks Prometheus natively. Adding OTel would require a translation layer (Prometheus -> OTel) for zero benefit.
- **OpenTelemetry bridge only** (`go.opentelemetry.io/contrib/bridges/prometheus`) - Forward Prometheus metrics to OTel backends. Deferred to a future release when users request it. The bridge can be added later without changing any instrumentation code.
- **StatsD/Graphite** - Simpler push model. Rejected because Prometheus is already in our dependency tree as an indirect dep of libp2p.

**Decision**: Use `prometheus/client_golang` directly with an isolated `prometheus.Registry`. When metrics enabled, pass the registry to libp2p via `libp2p.PrometheusRegisterer(reg)` to get all built-in libp2p metrics for free. When disabled, call `libp2p.DisableMetrics()` for zero overhead.

**Consequences**: No distributed tracing (deferred). No OTLP export (can be added via bridge later). Binary size increase: ~1MB (28MB total). Any Prometheus-compatible tool (Grafana, Datadog, etc.) works out of the box.

**Reference**: `pkg/p2pnet/metrics.go`, `pkg/p2pnet/network.go`

---

### ADR-H02: Nil-Safe Observability Pattern

**Context**: Metrics and audit logging are opt-in. Every call site that records a metric or audit event needs to work correctly when observability is disabled.

**Alternatives considered**:
- **Feature flags with conditional compilation** - Build tags to exclude metrics code entirely. Rejected because it creates two binaries with different behavior, complicating testing.
- **No-op implementations** (interface-based) - Create `NullMetrics` / `NullAuditLogger` implementations. More idiomatic but adds interface overhead and boilerplate.
- **Global singleton with init check** - Single global metrics instance. Rejected to maintain testability (isolated registries per test).

**Decision**: Nil pointer checks at every call site. `*Metrics` and `*AuditLogger` are nil when disabled. All methods on `AuditLogger` check `if a == nil { return }`. Metrics call sites check `if metrics != nil` before recording. The `InstrumentHandler` middleware returns the handler unchanged when both are nil.

**Consequences**: Slightly verbose call sites (`if m != nil { m.Counter.Inc() }`). But: zero allocations when disabled, zero interface overhead, testable with isolated registries, and trivially verifiable (grep for nil checks).

**Reference**: `pkg/p2pnet/audit.go`, `internal/daemon/middleware.go`, `cmd/shurli/serve_common.go`

---

### ADR-H03: Auth Decision Callback (Avoiding Circular Imports)

**Context**: Auth decisions happen in `internal/auth/gater.go`. Metrics live in `pkg/p2pnet/metrics.go`. Go forbids circular imports: `internal/auth` cannot import `pkg/p2pnet`.

**Alternatives considered**:
- **Move gater to pkg/p2pnet** - Would work but breaks the `internal/` boundary. The gater is an internal implementation detail.
- **Shared interface package** - Create a `pkg/observe` package with metric recording interfaces. Adds complexity for a single callback.

**Decision**: Define `AuthDecisionFunc func(peerID, result string)` as a callback type in `internal/auth`. The gater calls this callback on every inbound decision. The wiring layer (`cmd/shurli/serve_common.go`) creates a closure that feeds both the Prometheus counter and the audit logger.

**Consequences**: Clean dependency graph. The auth package has zero knowledge of Prometheus or audit logging. The callback is nil-safe (checked before calling). Easy to add more observers later by extending the closure.

**Reference**: `internal/auth/gater.go:SetDecisionCallback`, `cmd/shurli/serve_common.go`


---

# Pre-Batch I Decisions

Makefile and build tooling, PAKE-secured invite/join, and private DHT namespace isolation.

---

### ADR-Ia01: Makefile over Task Runner

**Context**: Shurli's build command has non-trivial flags (`-ldflags="-s -w" -trimpath` with version/commit/date injection). Service installation differs by OS (systemd vs launchd). There was no single command to build, install, and manage the daemon.

**Alternatives considered**:
- **Shell script** (`build.sh`) - Portable, but no dependency tracking, no `.PHONY`, no `make -j` parallelism. Would duplicate what Make already provides.
- **Mage / Task** - Go-based task runners. Adds a dependency and learning curve for contributors. Overkill for ~120 lines of targets.
- **Just** - Modern command runner. Not installed by default anywhere. Make is ubiquitous.

**Decision**: GNU Make. Present on every Unix system. Targets map directly to the operations developers need: `build`, `test`, `install`, `check`, `push`. OS detection via `uname -s` routes `install-service` to the correct init system.

**Consequences**: Windows users need `make` installed (via WSL, MSYS2, or Chocolatey). Accepted because Shurli's primary targets are Linux servers and macOS desktops.

**Reference**: `Makefile`

---

### ADR-Ia02: Generic Checks Runner (Not Privacy-Specific)

**Context**: The project needs a pre-push verification step. The specific checks include scanning for private data leaks (IPs, hostnames, peer IDs). But encoding these checks in the Makefile would expose the exact values being checked for in a public repository.

**Alternatives considered**:
- **Hardcoded privacy checks** - Embed `git grep` commands in Makefile. Would publicly document the private values being searched for. Rejected.
- **Pre-push git hook** - Works but is per-clone, easily bypassed with `--no-verify`, and not part of the normal workflow.

**Decision**: `make check` reads commands from a `.checks` file (gitignored, user-created, one command per line). The Makefile target is entirely generic: it runs each line and fails if any return non-zero. No words about what is being checked or why. `make push` gates on `make check`, making it impossible to push without passing.

**Consequences**: Each developer creates their own `.checks` file. The mechanism is reusable for any project-specific validation. The Makefile reveals nothing about the nature of the checks.

**Reference**: `Makefile:check`, `.gitignore`

---

### ADR-Ic01: Protocol-Level DHT Namespace Isolation

**Context**: All Shurli nodes share a single DHT with protocol prefix `/shurli/kad/1.0.0`. While `authorized_keys` controls who can communicate, discovery is shared. A gaming group, family, or organization has no way to form a completely isolated peer network.

**Alternatives considered**:
- **Application-layer filtering** - Keep a shared DHT but filter results by namespace tag. Rejected because nodes still participate in routing for all namespaces, and filtering is a soft boundary (peers are discoverable, just ignored).
- **Rendezvous string per network** - Different rendezvous points but same DHT. Rejected for the same reason: the DHT is shared, so cross-namespace discovery leaks metadata about who is online.
- **Separate relay per namespace** - Run independent relay servers. Works but is operationally heavy. Not mutually exclusive with namespace isolation.

**Decision**: Derive the DHT protocol prefix from an optional namespace: `/shurli/<namespace>/kad/1.0.0`. Empty namespace preserves the existing `/shurli/kad/1.0.0` prefix. Nodes on different namespaces speak entirely different DHT protocols and cannot discover each other. This is protocol-level isolation, not a filter.

**Implementation**: `DHTProtocolPrefixForNamespace()` function in `pkg/p2pnet/network.go` replaces direct use of the `DHTProtocolPrefix` constant at all 4 DHT bootstrap call sites. Config field: `discovery.network` (optional, validated as DNS-label format). `shurli init --network` and `shurli status` expose the namespace in the CLI.

**Consequences**: Each private network needs its own relay (or a relay configured with the matching namespace). This is intentional: isolation means isolation. Multi-namespace relay support is deferred. Zero backward compatibility impact (empty namespace = global DHT).

**Reference**: `pkg/p2pnet/network.go:DHTProtocolPrefixForNamespace`, `internal/config/config.go:DiscoveryConfig.Network`, `internal/validate/network.go`

---

### ADR-Ib01: Ephemeral DH + Token-Bound AEAD over Formal PAKE

**Context**: The invite/join handshake transmits the invite token as cleartext hex over the stream. A malicious relay operator can observe the token and potentially replay it. The goal is to upgrade the handshake so the relay sees only opaque encrypted bytes.

**Alternatives considered**:
- **Formal PAKE (CPace/SPAKE2)** - `filippo.io/cpace` is a "weekend project", `github.com/bytemare/pake` hasn't been updated since 2020. No mature, maintained Go PAKE library exists. Implementing CPace from scratch would add complexity for marginal benefit given our high-entropy token (64-bit random).
- **SRP** - Well-specified but complex, designed for password authentication with stored verifiers. Overkill for single-use tokens with 10-minute TTL.
- **Pre-shared key TLS** - Would require a TLS layer on top of libp2p's existing Noise transport. Layering violation.

**Decision**: Ephemeral X25519 Diffie-Hellman key exchange with token-bound HKDF key derivation and XChaCha20-Poly1305 AEAD encryption. Zero new dependencies: `crypto/ecdh` (Go stdlib), `golang.org/x/crypto/hkdf` and `golang.org/x/crypto/chacha20poly1305` (already in dependency tree via libp2p).

Both sides generate ephemeral X25519 keypairs, exchange public keys, compute the shared secret, then derive the AEAD key via HKDF-SHA256 with the invite token mixed in. If tokens differ, HKDF produces different keys, AEAD decryption fails, and the inviter reports "invalid invite code" with no protocol details leaked.

**Security properties**:
- Passive relay: sees only ephemeral public keys + encrypted bytes. Cannot learn token or peer names.
- Active MITM: prevented by libp2p Noise handshake (invite code contains inviter's peer ID, verified by transport layer).
- Token brute force: 64-bit entropy = 2^64 attempts. Single-use + 10min TTL.
- Offline dictionary: attacker needs an ephemeral private key (destroyed after exchange) to compute the DH shared secret.

**Consequences**: Not formally a PAKE (a true PAKE protects even against private key compromise with low-entropy passwords). Since our token is 64-bit random and ephemeral keys are destroyed immediately, this distinction is academic. If formal PAKE is needed later, the wire format stays identical - just swap the DH for CPace.

**Reference**: `internal/invite/pake.go`, `internal/invite/pake_test.go`

---

### ADR-Ib02: Invite Code Versioning (v1/v2 Coexistence)

**Context**: The invite code format needs to evolve to include the DHT namespace and support the PAKE handshake. Existing v1 invite codes (generated by older shurli versions) should still work.

**Alternatives considered**:
- **Breaking change** - Only support v2 codes. Rejected because users may have old codes in scripts or documentation.
- **Content negotiation** - Single format with feature flags. Adds complexity without clear benefit.

**Decision**: Version byte in invite code (first byte of binary payload) determines format. Originally: v1 (0x01) = legacy cleartext, v2 (0x02) = PAKE-encrypted with namespace. Post-I-1 deleted the cleartext protocol and renumbered: v1 (0x01) = PAKE-encrypted invite, v2 (0x02) = relay pairing code. Future versions (0x03+) are rejected with a "please upgrade shurli" message.

On the wire, the stream handler reads the version byte: 0x01 triggers PAKE handshake, 0x02 triggers relay pairing protocol.

**Consequences**: v2 invite codes are slightly longer (1 extra byte for namespace length when global, more with a namespace). The inviter must handle both protocols in the stream handler, but the code paths are cleanly separated.

**Reference**: `internal/invite/code.go`, `cmd/shurli/cmd_invite.go`, `cmd/shurli/cmd_join.go`


---

# Batch I: Adaptive Multi-Interface Path Selection

Interface discovery, parallel dial racing, path quality tracking, network change monitoring, STUN hole-punching, and every-peer-is-a-relay.

---

### ADR-I01: Interface Discovery with IPv6/IPv4 Classification

**Context**: Shurli needs to know what network interfaces are available to make intelligent connection decisions. Without interface awareness, the system cannot distinguish between IPv4-only, IPv6-only, or dual-stack hosts.

**Alternatives considered**:
- **Rely on libp2p's address reporting** - libp2p reports listen addresses but doesn't classify them by interface or IP version. Insufficient for path ranking decisions.
- **Platform-specific APIs** (macOS SCDynamicStore, Linux netlink) - More detailed but requires platform-specific code for a cross-platform tool.

**Decision**: `DiscoverInterfaces()` in `pkg/p2pnet/interfaces.go` uses Go's `net.Interfaces()` to enumerate all interfaces and classify addresses as global IPv4, global IPv6, or loopback. Returns an `InterfaceSummary` with convenience flags (`HasGlobalIPv6`, `HasGlobalIPv4`). Called at startup and on every network change.

**Consequences**: Cross-platform (Go stdlib). Slightly less detailed than platform-native APIs but sufficient for path ranking. Prometheus `interface_count` gauge tracks interface availability.

**Reference**: `pkg/p2pnet/interfaces.go`, `pkg/p2pnet/interfaces_test.go`

---

### ADR-I02: Parallel Dial Racing (Replace Sequential Connect)

**Context**: The old `ConnectToPeer()` tried DHT discovery (15s timeout) then relay fallback (30s timeout) sequentially. Worst case: 45 seconds to connect. For a tool that needs to feel instant, this is unacceptable.

**Alternatives considered**:
- **Increase timeouts** - Makes the problem worse, not better.
- **Always use relay** - Fast but defeats the purpose of direct connections.
- **libp2p's built-in smart dialing only** - Handles address-level racing but doesn't race between discovery strategies (DHT vs relay).

**Decision**: `PathDialer.DialPeer()` races DHT discovery and relay connection in parallel goroutines. If the peer is already connected, returns immediately (fast path). First successful connection wins; the loser is cancelled. The winning path is classified as `DIRECT` or `RELAYED` based on multiaddr inspection. Old `ConnectToPeer()` preserved as fallback.

**Consequences**: Connection time drops from 45s worst-case to the faster of DHT or relay (typically 3-10s). Slightly more goroutines spawned per connection attempt, but context cancellation ensures clean cleanup.

**Reference**: `pkg/p2pnet/pathdialer.go`, `pkg/p2pnet/pathdialer_test.go`

---

### ADR-I03: Event-Driven Path Quality Tracking

**Context**: Once connected, Shurli needs to know the quality of each connection path (direct vs relayed, transport type, IP version) for monitoring and future path switching decisions.

**Alternatives considered**:
- **Periodic polling** - Poll connection state on a timer. Wasteful and misses transient changes.
- **Wrap every connection call** - Track state manually in every connect/disconnect code path. Error-prone and duplicative.

**Decision**: `PathTracker` subscribes to libp2p's event bus (`EvtPeerConnectednessChanged`) to receive connect/disconnect events passively. Maintains per-peer path info (type, transport, IP version, connected time, last RTT). Exposed via `GET /v1/paths` daemon API endpoint. Prometheus labels: `path_type`, `transport`, `ip_version`.

**Consequences**: Zero polling overhead. Event-driven means path info updates immediately on connection state changes. Adds a dependency on libp2p's event bus API stability.

**Reference**: `pkg/p2pnet/pathtracker.go`, `pkg/p2pnet/pathtracker_test.go`

---

### ADR-I04: Network Change Monitoring (Polling with Diff)

**Context**: When a user switches WiFi networks, gains/loses a cellular connection, or plugs in Ethernet, Shurli should detect the change and re-evaluate connection paths.

**Alternatives considered**:
- **macOS SCDynamicStore + Linux Netlink** - Platform-native, truly event-driven, zero polling. More code, platform-specific build tags, harder to test.
- **libp2p event bus only** - libp2p fires address change events but not for all interface changes (e.g., gaining an interface with no libp2p listener).

**Decision**: `NetworkMonitor` polls `DiscoverInterfaces()` at a configurable interval and diffs against the previous snapshot. On change, fires registered callbacks (interface re-scan, STUN re-probe, peer relay auto-detect update). Simple, cross-platform, testable.

**Consequences**: Polling introduces a detection delay (up to the poll interval). Acceptable because network changes are rare events and the poll interval is configurable. Platform-native event-driven detection can be added later as an optimization without changing the callback API.

**Reference**: `pkg/p2pnet/netmonitor.go`, `pkg/p2pnet/netmonitor_test.go`

---

### ADR-I05: Zero-Dependency STUN Client (RFC 5389)

**Context**: To classify NAT type and discover external addresses for hole-punching, Shurli needs STUN probing. Existing Go STUN libraries (pion/stun) would add a new dependency.

**Alternatives considered**:
- **pion/stun** - Mature, widely used. Rejected because it pulls in the entire pion dependency tree (already have pion/dtls as a transitive dep of libp2p, but adding pion/stun directly increases attack surface and binary size).
- **Skip STUN entirely** - Rely on AutoNAT v2 only. AutoNAT gives reachability but not NAT type classification (full-cone vs symmetric matters for hole-punch prediction).

**Decision**: Implement a minimal RFC 5389 STUN Binding Request client in `pkg/p2pnet/stunprober.go`. ~150 lines of code. Probes multiple STUN servers concurrently (Google, Cloudflare). Collects external addresses, classifies NAT type (none, full-cone, address-restricted, port-restricted, symmetric). `HolePunchable()` helper indicates DCUtR likelihood.

**Consequences**: Zero new dependencies. Binary size unchanged. The STUN client only implements Binding Request (the simplest STUN transaction). Does NOT implement TURN or ICE. Runs in background at startup (non-blocking) and re-probes on network change.

**Reference**: `pkg/p2pnet/stunprober.go`, `pkg/p2pnet/stunprober_test.go`

---

### ADR-I06: Every-Peer-Is-A-Relay (Auto-Enable with Public IP)

**Context**: The relay VPS is a single point of failure. If every peer with a public IP could relay for its authorized peers, the VPS becomes redundant.

**Alternatives considered**:
- **Manual relay enable** - User explicitly enables relay in config. Safe but friction prevents adoption.
- **Always enable relay** - Even on NATted nodes. Wasteful because NATted relays can't accept inbound connections.
- **DHT-based relay advertisement** - Peers discover relays via DHT. Deferred to Post-I because it requires the PeerManager/AddrMan infrastructure.

**Decision**: Any peer with a detected global IP (from `DiscoverInterfaces()`) auto-enables circuit relay v2 with conservative resource limits (4 reservations, 16 circuits, 128KB/direction, 10min sessions). Uses the existing `ConnectionGater` for authorization (no new ACL needed). Auto-detects on startup and network changes. Disables when public IP is lost.

**Consequences**: Peers behind NAT never become relays (correct). Peers with public IPs silently become relays for their authorized peers. The conservative limits prevent resource exhaustion on home machines. DHT-based relay discovery (so peers can find each other's relays) is deferred to Post-I.

**Reference**: `pkg/p2pnet/peerrelay.go`, `pkg/p2pnet/peerrelay_test.go`


---

# Post-I-2: Trust and Delivery

Peer introduction delivery, HMAC group commitment, relay admin socket, SAS verification, reachability grades, and sovereign peer interaction history.

---

### ADR-J01: Peer Introduction Delivery Protocol

**Context**: After relay pairing completes, the first peer to join a group gets authorized by the relay. But when a second peer joins later, the first peer's daemon doesn't know about the newcomer. In live testing, the home-node denied the client-node 20+ times because it had no record of the client's peer ID. The relay knows about both peers but had no mechanism to tell existing peers about newcomers.

**Alternatives considered**:
- **Polling-based discovery** - Daemons periodically ask the relay "who's in my group?" Wasteful and adds latency.
- **GossipSub** - Premature for current scale and adds protocol complexity.
- **Direct peer-to-peer sync** - Requires both peers to be online simultaneously, which is exactly the problem we're solving.

**Decision**: `/shurli/peer-notify/1.0.0` stream protocol. The relay acts as a post office: it knows who joined together and delivers introductions when peers connect. Wire format: version byte + group ID (32 bytes) + group size (byte) + per-peer entries (peer ID length + peer ID + name length + name + 32-byte HMAC proof). Two triggers: post-pairing notification (immediate, after new peer joins) and reconnect notification (on `EvtPeerIdentificationCompleted`, for peers that were offline during pairing). The receiving daemon validates group membership, enforces group size limits, verifies HMAC proofs, and adds authorized peers via hot-reload.

**Consequences**: Solves the first-joiner gap completely. Relay is the introduction medium, not the trust authority (HMAC proofs provide cryptographic verification). Wire format is generic enough for future introduction sources (peer-to-peer, multi-relay mesh). Adds a stream protocol dependency between relay and daemon.

**Reference**: `internal/relay/notify.go`, `internal/relay/notify_test.go`

---

### ADR-J02: HMAC Group Commitment Proof

**Context**: Peer-notify delivers introductions from the relay. A compromised relay could inject arbitrary peer IDs into introduction messages. Need cryptographic proof that each introduced peer actually held a valid pairing token.

**Alternatives considered**:
- **Trust the relay** - Works for private relays but violates the sovereignty principle. A relay should be a transport, not a trust root.
- **Public-key signatures per peer** - Each peer signs their own introduction. More complex and requires key exchange before the introduction itself.
- **Token-based challenge** - Relay challenges peers to prove token possession during pairing. Simpler but requires live interaction.

**Decision**: `HMAC-SHA256(token, groupID)` computed during pairing while the raw token is still in memory. The HMAC proof is stored in the TokenStore alongside the peer's entry. When peer-notify delivers introductions, each peer entry includes its 32-byte HMAC proof. The receiving daemon stores the proof in authorized_keys as `hmac_proof=<hex>`. Raw tokens are never stored - only SHA-256 hashes. HMAC proofs are the only derivative that persists.

**Consequences**: A compromised relay cannot forge valid HMAC proofs without access to the original pairing tokens (which are hashed immediately). Proofs can be verified by any peer that holds the same group's token. Adds 32 bytes per peer in the wire format, which is negligible.

**Reference**: `internal/relay/pairing.go:103-106`, `internal/relay/tokens.go:202-213`

---

### ADR-J03: Relay Admin Socket (Unix Socket + Cookie Auth)

**Context**: The first live test of relay pairing failed. `relay pair` created a fresh ephemeral TokenStore, generated tokens, and exited. The running `relay serve` process had a separate TokenStore with no knowledge of the generated tokens. Pairing was dead on arrival.

**Alternatives considered**:
- **Shared file-based token store** - Both processes read/write a shared token file. Race conditions, no atomic operations, no TTL enforcement.
- **Embed token generation in relay serve** - Makes relay serve interactive, breaking the systemd service model.
- **gRPC or custom protocol** - Over-engineered for what amounts to 3 HTTP endpoints.

**Decision**: Unix domain socket with cookie auth, following the exact pattern proven by the daemon API. Three endpoints: `POST /v1/pair` (create pairing group), `GET /v1/pair` (list groups), `DELETE /v1/pair/{id}` (revoke group). Rewrite `relay pair` as a fire-and-forget HTTP client that talks to the running relay's admin socket. Cookie is 32-byte random hex, `0600` permissions, rotated per restart. Socket path: `<config-dir>/relay-admin.sock`.

**Consequences**: Token generation happens inside the relay serve process where the TokenStore lives. No shared state, no file races. The admin socket pattern is now battle-tested in two places (daemon + relay). Adds ~300 lines but reuses the same auth pattern. Security: constant-time cookie comparison (`subtle.ConstantTimeCompare`), `MaxBytesReader` on request body, upper bound on pairing count.

**Reference**: `internal/relay/admin.go`, `internal/relay/admin_client.go`, `internal/relay/admin_test.go`

---

### ADR-J04: SAS Verification (OMEMO-style Emoji Fingerprint)

**Context**: After relay pairing, peers are authorized but unverified. The relay mediated the exchange - if the relay were compromised, it could have substituted peer IDs (MITM). Need out-of-band identity confirmation that users can perform over a phone call or in person.

**Alternatives considered**:
- **Full public key display** - Too long for humans to compare reliably.
- **Numeric fingerprint only** - Works but low memorability. Users won't do it unless it's engaging.
- **QR code scan** - Requires physical proximity and camera access. Too restrictive.

**Decision**: OMEMO-style SAS (Short Authentication String). Sorted SHA-256 hash of both peer IDs (sorted so both sides compute the same fingerprint). First 8 bytes encode to 4 emoji (2 bytes per index, mod 256 into a table of 256 universally recognizable emoji). Also generates a 6-digit numeric code as fallback. `[UNVERIFIED]` badge persists in `auth list` output until verification, then `verified=sha256:<prefix>` is written to authorized_keys. The 256-emoji table covers animals, nature, weather, food, objects, musical instruments, transport, and sports - categories that work across cultures.

**Consequences**: Verification is optional but visible. The persistent `[UNVERIFIED]` badge serves as a constant reminder without blocking functionality. 4-emoji comparison is quick and engaging. Entropy: 4 emoji from 256 = 2^32 possibilities, sufficient for active MITM detection in the private-network threat model.

**Reference**: `pkg/p2pnet/verify.go`, `pkg/p2pnet/verify_test.go`, `cmd/shurli/cmd_verify.go`

---

### ADR-J05: Reachability Grade Computation

**Context**: Users need to understand why connections go through relay instead of direct. STUN reporting "hole-punchable" is meaningless when CGNAT sits above the inner NAT and drops unsolicited inbound packets. Need a simple, honest grade that captures real-world reachability.

**Alternatives considered**:
- **Binary "reachable or not"** - Too coarse. A peer behind port-restricted NAT is meaningfully different from one behind CGNAT.
- **Raw STUN output** - Too technical for most users. "Address-restricted cone NAT" means nothing to someone who just wants SSH to work.
- **Auto-detect and hide** - Don't show the grade at all, just silently pick paths. But hiding information violates the principle of honest communication.

**Decision**: A-F grade computed from interface discovery + STUN results, displayed in daemon status output and exposed via API. Grade A: public IPv6 detected. Grade B: public IPv4 or hole-punchable NAT (full-cone, address-restricted). Grade C: port-restricted NAT. Grade D: symmetric NAT or CGNAT. Grade F: no connectivity detected. Critical design choice: CGNAT detection (`stun.BehindCGNAT`) caps the grade at D regardless of inner NAT type. This overrides STUN's false optimism. Grade updates on network change events.

**Consequences**: Users get honest, actionable information. "Grade D - CGNAT detected, hole-punch unlikely" is more useful than "hole-punchable: yes" when CGNAT will actually block it. The grade informs path selection decisions in future phases (Phase 5-L PeerManager). CGNAT detection is limited to RFC 6598 (100.64.0.0/10) on local interfaces - mobile CGNAT using RFC 1918 addresses (172.20.x.x) cannot be distinguished from home networks.

**Reference**: `pkg/p2pnet/reachability.go`, `pkg/p2pnet/reachability_test.go`

---

### ADR-J06: Sovereign Peer Interaction History

**Context**: Future trust algorithms (EigenTrust, Community Notes bridging, reputation scoring) need interaction data as input. If we wait until those algorithms ship to start collecting data, we'll have zero history to work with. Building the data layer now means months of interaction data will be ready when the algorithms arrive.

**Alternatives considered**:
- **Centralized reputation service** - Defeats sovereignty. A central server that knows who interacts with whom is a privacy nightmare.
- **Gossip-based reputation** - Peers share reputation scores with each other. Complex, game-able, and premature.
- **No collection, implement later** - Loses months of valuable interaction data that can never be recovered.

**Decision**: Per-peer JSON file (`peer_history.json`), stored locally and never shared. Tracks: `first_seen`, `last_seen`, `connection_count`, `avg_latency_ms` (Welford's online algorithm for running average), `path_types` (map of "direct":N, "relay":M), `introduced_by`, `intro_method` ("relay-pairing", "invite", "manual"). Thread-safe with `sync.RWMutex`. Atomic file writes (temp + rename) for crash safety. Best-effort load on startup (missing file is not an error).

**Consequences**: Zero external dependencies. Data stays sovereign - each peer controls its own history file. The schema is intentionally minimal but extensible. Future trust algorithms can consume this data without schema migrations. Storage growth is bounded by peer count (not connection count), since records are per-peer aggregates.

**Reference**: `internal/reputation/history.go`, `internal/reputation/history_test.go`


---

# Pre-Phase 5 Hardening

Bug fixes from cross-network testing, CGNAT detection improvements, and service deployment across three nodes.

---

### ADR-K01: Startup Race Condition Fix and CGNAT Detection

**Context**: Cross-network testing on 4 different networks (satellite WiFi, terrestrial WiFi, wired Ethernet, 5G cellular) exposed two bugs in the same area: network awareness.

Bug 1: The relay fired `peer-notify` introductions before the daemon finished registering its `/shurli/peer-notify/1.0.0` handler. First 1-2 delivery attempts failed with "protocols not supported." Root cause: `Bootstrap()` was called before `SetupPeerNotify()`, so the relay saw the peer connect before its stream handlers were ready.

Bug 2: RFC 6598 CGNAT addresses (100.64.0.0/10) on local interfaces were not being detected. The STUN prober reported "hole-punchable" for mobile hotspot connections even when a carrier NAT sat above the inner NAT.

**Decision**: Move `SetupPeerNotify()` before `Bootstrap()` in the startup sequence. All stream handlers must be registered before any peer discovery begins. For CGNAT: detect RFC 6598 addresses on local interfaces, set `BehindCGNAT` flag in STUN results, and cap the reachability grade at D. Two new unit tests cover the CGNAT detection path.

**Consequences**: Clean cold starts on all tested networks. Startup race eliminated. CGNAT detection works for RFC 6598 addresses. Limitation: mobile carriers that use RFC 1918 addresses (172.20.x.x, 10.x.x.x) for CGNAT cannot be distinguished from home networks. This is a fundamental limitation, not a bug - the address ranges overlap.

**Reference**: `cmd/shurli/serve_common.go`, `pkg/p2pnet/stunprober.go`, `pkg/p2pnet/reachability.go`

---

### ADR-K02: Stale Address Detection and Diagnostics

**Context**: After switching from WiFi to 5G cellular, the daemon's address list still contained the old WiFi address. The network monitor detected the interface removal correctly, but the stale address persisted in libp2p's address list. When switching back to WiFi, the daemon stayed on relay because it didn't re-evaluate direct paths.

**Alternatives considered**:
- **Full address lifecycle management** - Remove stale addresses from libp2p, trigger re-discovery. This is the correct long-term fix but requires the PeerManager that Phase 5-L will build.
- **Force restart on network change** - Crude but effective. Violates the "no disruption" principle.

**Decision**: Display-only fix for now. Cross-check `h.Addrs()` against `net.InterfaceAddrs()` and label stale addresses as `[local,stale?]` in status output. Delayed diagnostic log fires 10 seconds after a network change event, giving interfaces time to stabilize. Full address lifecycle management is deferred to Phase 5-L PeerManager.

**Consequences**: Users can see stale addresses in status output and understand why connections stay on relay. The diagnostic log helps debugging. Does not fix the underlying problem (PeerManager will). Avoids premature complexity that would need to be rewritten in Phase 5-L anyway.

**Reference**: `pkg/p2pnet/netmonitor.go`, `cmd/shurli/serve_common.go`

---

### ADR-K03: systemd and launchd Service Deployment

**Context**: All three nodes were running via `nohup` in the background. This meant: no automatic restart on crash, no watchdog monitoring, no boot-time startup, and log management via terminal scrollback. For infrastructure that's meant to be always-on, this is unacceptable.

**Decision**: Deploy proper service management on all three nodes:

- **Relay VPS (Linux)**: systemd unit `shurli-relay.service` with `Type=notify`, `WatchdogSec=90`, `Restart=on-failure`, `RestartSec=5`. Binary at `/usr/local/bin/shurli` via symlink to repo. Enabled at boot.
- **Home-node (Linux)**: systemd unit `shurli-daemon.service` with identical watchdog and restart configuration. Created fresh (no pre-existing service). Enabled at boot.
- **Client-node (macOS)**: launchd plist `com.shurli.daemon.plist` with `RunAtLoad=true`, `KeepAlive=true`. Binary at `/usr/local/bin/shurli` via symlink.

All three use the pure-Go `sd_notify` implementation in `internal/watchdog/` (no CGo, no-op on non-systemd). The watchdog checks health every 30s, sends `WATCHDOG=1` to systemd on success. `WatchdogSec=90` (3x interval) triggers restart if health checks stop.

**Consequences**: All nodes survive crashes, reboots, and power failures. Watchdog detects hung processes. Logs go to journald (Linux) or system log (macOS). `nohup` processes were killed and replaced. Dev workflow: rebuild binary in repo, restart service, symlink means no file copy needed.

**Reference**: `deploy/shurli-daemon.service`, `deploy/com.shurli.daemon.plist`, `relay-server/relay-server.service`, `internal/watchdog/watchdog.go`


---

# Dev Tooling Decisions

Build system, documentation pipeline, and developer workflow tooling.

---

### ADR-DT01: Go Replaces Bash for Doc Sync Pipeline

**Context**: `website/sync-docs.sh` transformed `docs/*.md` into Hugo-ready website content with front matter injection, cross-reference link rewriting, and image path mapping. Over 3 iterations, the sed-based link rewriting broke in different ways: parenthesis stripping from markdown links (bash `\(\)` grouping conflict), anchored links like `(FAQ.md#section)` not matched by the plain `(FAQ.md)` pattern, and `.md` extensions not stripped for Hugo directory-style URLs. Each fix introduced new fragility because sed regex is context-free and markdown links are not.

**Alternatives considered**:
- **Fix the sed chains again** - The third round of fixes proved the pattern: every sed fix risks breaking an adjacent match. The bash script had grown to 377 lines with 20+ sed commands, many interacting.
- **Python/Node script** - Would work but adds a runtime dependency to a pure-Go project. Every contributor would need Python/Node installed.
- **Add to shurli binary as a subcommand** - Keeps it in Go but bloats the shipped binary with dev-only code. Users never run doc sync.

**Decision**: Standalone Go program in `tools/sync-docs/` (4 files, ~1033 lines). Every text transform is a pure `func(string) string` with zero I/O. Table-driven tests cover every transform independently. Run via `go run ./tools/sync-docs` or `make sync-docs`. Not compiled into the shurli binary. CI runs it during website builds.

Architecture:
- `config.go` - ordered slices of doc entries and journal entries (replaces bash associative arrays)
- `transforms.go` - 11 pure transform functions using `strings.ReplaceAll` and one compiled regex
- `transforms_test.go` - 12 test functions + integration test that creates a project in `t.TempDir()`
- `main.go` - flag parsing, file I/O orchestration, `--dry-run` support

**Consequences**: Link rewriting bugs become compiler errors or test failures instead of runtime surprises. Adding a new doc file means adding one struct literal to `config.go`. The `website` Makefile target auto-syncs before starting Hugo. Zero new dependencies.

**Reference**: [`tools/sync-docs/`](https://github.com/shurlinet/shurli/blob/main/tools/sync-docs/)

---

### ADR-DT02: Relay Setup as Go Subcommand (Replace Bash Section 6.5)

**Context**: `relay-server/setup.sh` section 6.5 generated relay node configuration (192 lines of bash). It duplicated config YAML that already existed in Go's `config_template.go`, creating a maintenance burden where changes had to be made in two places.

**Alternatives considered**:
- **Keep bash, import template** - Bash cannot import Go templates. Would require generating a shared file format.
- **Separate Go binary in tools/** - Like sync-docs, but relay setup is user-facing (operators run it), not dev-only. Belongs in the main binary.

**Decision**: `shurli relay setup` subcommand with TimeMachine-style backup/restore. Reuses existing `config_template.go` for config generation. Setup.sh section 6.5 reduced from 192 lines to 3 lines (`shurli relay setup`). 18 new tests cover backup rotation, restore, config generation, and edge cases.

**Consequences**: Single source of truth for relay config. Operators get `--backup` and `--restore` flags for free. The bash script still handles system-level tasks (apt, systemd, firewall) where bash is the right tool.

**Reference**: [`cmd/shurli/cmd_relay.go`](https://github.com/shurlinet/shurli/blob/main/cmd/shurli/cmd_relay.go), [`relay-server/setup.sh`](https://github.com/shurlinet/shurli/blob/main/relay-server/setup.sh)


---

# Shurli Relay Server Setup

Complete guide to deploying the relay server on a fresh VPS (Ubuntu 22.04 / 24.04).

## 1. Initial VPS Setup

SSH into your fresh VPS and install git:

```bash
ssh root@YOUR_VPS_IP
apt update && apt upgrade -y
apt install -y git ufw

# Enable firewall with SSH access
ufw allow OpenSSH
ufw default deny incoming
ufw default allow outgoing
ufw enable
```

### Clone the repo

```bash
git clone https://github.com/shurlinet/shurli.git
cd Shurli/relay-server
```

### Run the setup script

The script detects that you're root and walks you through creating a secure service user:

```bash
bash setup.sh
```

It will:
1. Ask you to **select an existing user** or **create a new one**
2. **New user**: creates the account, sets a password, copies your SSH keys, locks down home directory (700), and offers to harden SSH (disable password auth + root login)
3. **Existing user**: audits sudo group, password, SSH keys, directory permissions, and SSH daemon config, and offers to fix any issues
4. Continues with the full setup: builds binary, installs systemd service (running as the selected user), configures firewall, tunes QUIC buffers, and transfers file ownership

**If creating a new user**, test SSH access in a separate terminal before closing the root session:

```bash
ssh newuser@YOUR_VPS_IP
sudo whoami    # should print: root
```

### Configure (as the service user)

```bash
ssh shurli@YOUR_VPS_IP
cd Shurli/relay-server

# Create config from sample
cp ../configs/relay-server.sample.yaml relay-server.yaml

# Edit if needed (defaults are good - port 7777, gating enabled)
nano relay-server.yaml
```

Then restart the service to pick up config changes:

```bash
sudo systemctl restart shurli-relay
```

### Add peers to the relay

**Option A: Pairing codes (recommended)**

Generate pairing codes and share them with your peers:

```bash
# Generate 3 pairing codes (one per person)
./shurli relay pair --count 3

# Each person joins with one command on their machine:
shurli join <pairing-code> --name laptop
```

Pairing codes handle authorization automatically. Everyone who joins with a code from the same relay is mutually authorized and can verify each other with `shurli verify <name>`.

**Option B: Manual authorization**

If you already know the peer IDs, add them directly:

```bash
# Using the CLI
./shurli relay authorize <peer-id> --comment "home-node"
./shurli relay authorize <peer-id> --comment "client-node"

# Or edit the file directly (one peer ID per line)
nano relay_authorized_keys
```

```
12D3KooWARqzAAN9es44ACsL7W82tfbpiMVPfSi1M5czHHYPk5fY  # home-node
12D3KooWNq8c1fNjXwhRoWxSXT419bumWQFoTbowCwHEa96RJRg6  # client-node
```

Restart the service after manual changes:

```bash
sudo systemctl restart shurli-relay
```

---

## 2. Verify

Run the health check anytime:

```bash
cd ~/Shurli/relay-server
bash setup.sh --check
```

You should see all `[OK]` items:

```
Binary:
  [OK]   shurli binary exists
  [OK]   shurli is executable

Configuration:
  [OK]   relay-server.yaml exists
  [OK]   Connection gating is ENABLED
  ...

Service:
  [OK]   shurli-relay service is enabled (starts on boot)
  [OK]   shurli-relay service is running
  [OK]   Service runs as non-root user: shurli
  ...

=== Summary: 25 passed, 0 warnings, 0 failures ===
Everything looks great!
```

---

## 3. Uninstall

To remove the systemd service, firewall rules, and system tuning:

```bash
cd ~/Shurli/relay-server
bash setup.sh --uninstall
```

This removes:
- The systemd service (stopped, disabled, file deleted)
- Firewall rules for port 7777
- QUIC buffer tuning from `/etc/sysctl.conf`
- Journald log rotation settings

It does **not** delete your binary, config, keys, or source code. To fully clean up:

```bash
rm -rf ~/Shurli  # Only if you want to remove everything
```

---

## 4. Useful Commands

```bash
# Service management
sudo systemctl status shurli-relay
sudo systemctl restart shurli-relay
sudo systemctl stop shurli-relay

# Follow logs
sudo journalctl -u shurli-relay -f

# Recent logs (last 50 lines)
sudo journalctl -u shurli-relay -n 50

# Check log disk usage
sudo journalctl --disk-usage

# Update relay server (after code changes)
cd ~/Shurli
git pull
go build -ldflags="-s -w" -trimpath -o relay-server/shurli ./cmd/shurli
sudo systemctl restart shurli-relay
```

---

## 5. Security Checklist

| Item | How to verify |
|------|--------------|
| Non-root user | `whoami` (should NOT be root) |
| SSH key-only login | `grep PasswordAuthentication /etc/ssh/sshd_config` → `no` |
| Root login disabled | `grep PermitRootLogin /etc/ssh/sshd_config` → `no` |
| Firewall active | `sudo ufw status` → active, default deny incoming |
| Only needed ports open | `sudo ufw status` → 22/tcp (SSH) + 7777/tcp+udp |
| Connection gating on | `grep enable_connection_gating relay-server.yaml` → `true` |
| Key file permissions | `ls -la relay_node.key` → `-rw-------` (600) |
| Log rotation | `grep SystemMaxUse /etc/systemd/journald.conf` → `500M` |
| System updates | `sudo apt update && sudo apt upgrade` |

Or just run: `bash setup.sh --check`

---

## File Layout

After setup, your relay-server directory looks like:

```
~/Shurli/relay-server/
├── shurli                    # Binary (built from cmd/shurli, gitignored)
├── relay-server.yaml         # Config (gitignored)
├── relay-server.service      # Template service file (in git)
├── relay_node.key            # Identity key (auto-generated, gitignored)
├── relay_authorized_keys     # Allowed peer IDs (gitignored)
└── setup.sh                  # Setup + health check script
```

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Service fails to start | `sudo journalctl -u shurli-relay -n 30` for error logs |
| "Permission denied" on key file | `chmod 600 relay_node.key` |
| Peers can't connect | Use `shurli relay pair` to generate codes, or check `relay_authorized_keys` has their peer IDs |
| Random peers connecting | Verify `enable_connection_gating: true` in config |
| High log disk usage | `sudo journalctl --vacuum-size=200M` to trim now |
| Port not reachable | `sudo ufw status` and check VPS provider firewall/security group |
| Service runs as root | `bash setup.sh --uninstall` then re-run `bash setup.sh` (as root it will guide you through user setup) |


---

